<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>Tutorial Data Science Archivi - Data Trading</title>
	<atom:link href="https://datatrading.info/category/tutorial-data-science/feed/" rel="self" type="application/rss+xml" />
	<link></link>
	<description>Tecnologie Digitali applicate al Trading</description>
	<lastBuildDate>Sat, 22 Oct 2022 07:07:54 +0000</lastBuildDate>
	<language>it-IT</language>
	<sy:updatePeriod>
	hourly	</sy:updatePeriod>
	<sy:updateFrequency>
	1	</sy:updateFrequency>
	

<image>
	<url>https://datatrading.info/wp-content/uploads/2019/04/favico.-300x300.jpg</url>
	<title>Tutorial Data Science Archivi - Data Trading</title>
	<link></link>
	<width>32</width>
	<height>32</height>
</image> 
	<item>
		<title>Modelli di regressione lineare bayesiana con PyMC3</title>
		<link>https://datatrading.info/modelli-di-regressione-lineare-bayesiana-con-pymc3/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Mon, 18 Sep 2017 14:41:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Statistica Bayesiana]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4413</guid>

					<description><![CDATA[<p>Nei precedenti articoli abbiamo introdotto la statistica bayesiana,&#160;ricavato analiticamente una proporzione binomiale con priori coniugati&#160;e abbiamo&#160;descritto le basi della Markov Chain Monte Carlo&#160;tramite l&#8217;algoritmo Metropolis.&#160;In questo articolo introduciamo la modellazione di regressione nel framework bayesiano ed eseguiamo l&#8217;inferenza utilizzando la libreria MCMC di PyMC3&#160;. Iniziamo riassumendo l&#8217;approccio classico, o frequentista, alla regressione lineare multipla. Quindi &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/modelli-di-regressione-lineare-bayesiana-con-pymc3/"> <span class="screen-reader-text">Modelli di regressione lineare bayesiana con PyMC3</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/modelli-di-regressione-lineare-bayesiana-con-pymc3/">Modelli di regressione lineare bayesiana con PyMC3</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4413" class="elementor elementor-4413">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-5029469 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="5029469" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-180c9e9" data-id="180c9e9" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-9d8ddfa elementor-widget elementor-widget-text-editor" data-id="9d8ddfa" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>Nei precedenti articoli abbiamo </span><a href="https://datatrading.info/introduzione-alla-statistica-bayesiana/"><span>introdotto la statistica bayesiana</span></a><span>,&nbsp;</span><a href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/"><span>ricavato analiticamente una proporzione binomiale con priori coniugati</span></a><span>&nbsp;e abbiamo&nbsp;</span><a href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/"><span>descritto le basi della Markov Chain Monte Carlo</span></a><span>&nbsp;tramite l&#8217;algoritmo Metropolis.&nbsp;In questo articolo introduciamo la modellazione di regressione nel framework bayesiano ed eseguiamo l&#8217;inferenza utilizzando la </span><a href="https://github.com/pymc-devs/pymc3"><span>libreria MCMC di PyMC3</span></a><span>&nbsp;.</span></p>
<p><span>Iniziamo riassumendo l&#8217;approccio classico, o frequentista, alla regressione lineare multipla. Quindi descriviamo l&#8217;approccio di un bayesiano alla regressione lineare. Infine descriviamo brevemente il concetto di </span><a href="https://en.wikipedia.org/wiki/Generalized_linear_model"><span>modello lineare generalizzato</span></a><span> (GLM), necessario per comprendere la sintassi delle descrizioni dei modelli in PyMC3.</span></p>
<p><span>Successivamente alla descrizione di questi modelli simuliamo alcuni dati lineari con rumore e quindi usiamo PyMC3 per produrre distribuzioni a posteriori per i parametri del modello.&nbsp;Questa è la stessa procedura che abbiamo eseguito quando abbiamo discusso di modelli di serie temporali come </span><a href="https://www.quantstart.com/articles/Autoregressive-Moving-Average-ARMA-p-q-Models-for-Time-Series-Analysis-Part-3"><span>ARMA</span></a><span>&nbsp;e&nbsp;</span><a href="https://www.quantstart.com/articles/Generalised-Autoregressive-Conditional-Heteroskedasticity-GARCH-p-q-Models-for-Time-Series-Analysis"><span>GARCH</span></a><span>.&nbsp;Questo processo di &#8220;simula e adatta&#8221; non solo ci aiuta a capire il modello, ma controlla anche che lo stiamo adattando correttamente quando conosciamo i &#8220;veri&#8221; valori dei parametri.</span></p>
<p><span>Rivolgiamo ora la nostra attenzione all&#8217;approccio frequentista alla regressione lineare.</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-18c6feb elementor-widget elementor-widget-text-editor" data-id="18c6feb" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Regressione lineare frequentista</h2><p>L&#8217;approccio frequentista, o classico, alla regressione lineare multipla assume un modello della forma (Hastie et al [2]):</p><p style="text-align: center;">\(\begin{eqnarray}f \left( \mathbf{X} \right) = \beta_0 + \sum_{j=1}^p \mathbf{X}_j \beta_j + \epsilon = \beta^T \mathbf{X} + \epsilon\end{eqnarray}\)</p><p>Dove \(\beta^T\) è la trasposta del vettore coefficiente \(\beta\) e \(\epsilon \sim \mathcal{N}(0,\sigma^2)\) è l&#8217;errore della misurazione, distribuito normalmente con media zero e deviazione standard \(\sigma\).</p><p>Cioè, il nostro modello \(f(\mathbf{X})\) è <em>lineare</em> per i predittori,\(\mathbf{X}\), associato a qualche errore di misurazione.</p><p>Se abbiamo una serie di dati di allenamento \((x_1, y_1), \ldots, (x_N, y_N)\) l&#8217;obiettivo è quindi stimare i coefficienti \(\beta\), che forniscono il miglior adattamento lineare ai dati. Geometricamente, questo significa che dobbiamo trovare l&#8217;orientamento dell&#8217;iperpiano che meglio caratterizza linearmente i dati.</p><p>&#8220;Migliore&#8221; in questo caso significa ridurre al minimo una qualche forma di funzione di errore. Il metodo più popolare per farlo è tramite i <em>minimi quadrati ordinari (OLS)</em>. Se consideriamo la <em>somma residua dei quadrati (RSS)</em>, che è la somma delle differenze al quadrato tra gli output e le stime di regressione lineare, come segue:</p><p style="text-align: center;">\(\begin{eqnarray}\text{RSS}(\beta) &amp;=&amp; \sum_{i=1}^{N} (y_i &#8211; f(x_i))^2 \\<br />&amp;=&amp; \sum_{i=1}^{N} (y_i &#8211; \beta^T x_i)^2\end{eqnarray}\)</p><p>Quindi l&#8217;obiettivo di OLS è ridurre al minimo l&#8217;RSS, tramite l&#8217;adeguamento dei coefficienti \(\beta\). La <em>stima di massima verosimiglianza</em> di \(\beta\), che minimizza l&#8217;RSS, è data da (vedi Hastie et al [2] per i dettagli):</p><p style="text-align: center;">\(\begin{eqnarray}\hat{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}\end{eqnarray}\)</p><p>Per ricavare la previsione successiva \(y_{N+1}\), considerando i  nuovi dati \(x_{N+1}\), è sufficiente moltiplicare i componenti di \(x_{N+1}\) con i relativi coefficienti \(\beta\) e ottenere \(y_{N+1}\).</p><p>E&#8217; importante evidenziare che \(\hat{\beta}\) è una <em>stima puntuale</em>, cioè è un valore singolo di \(\mathbb{R}^{p+1}\). Nella formulazione bayesiana questa interpretazione differisce sostanzialmente.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-117a558 elementor-widget elementor-widget-text-editor" data-id="117a558" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Regressione lineare bayesiana</h2><p>In un framework bayesiano, la regressione lineare è espressa in modo probabilistico. Cioè, riformuliamo il modello di regressione lineare di cui sopra per utilizzare le distribuzioni di probabilità. La sintassi di una regressione lineare in un framework bayesiano è simile alla seguente:</p><p style="text-align: center;">\(\begin{eqnarray}\mathbf{y} \sim \mathcal{N} \left(\beta^T \mathbf{X}, \sigma^2 \mathbf{I} \right)\end{eqnarray}\)</p><p>In parole, i nostri datapoint di risposta \(\mathbf{y}\) sono campionati da una distribuzione normale multivariata che ha una media uguale al prodotto tra i coefficienti \(\beta\) e predittori \(\mathbf{X}\), e una varianza di \(\sigma^2\). Inoltre \(\mathbf{I}\) si riferisce alla matrice identità, necessaria  affinché la distribuzione sia multivariata.</p><p>Questa è una formulazione molto diversa dall&#8217;approccio frequentista. Nell&#8217;impostazione frequentista non si fa menzione delle distribuzioni di probabilità per qualcosa di diverso dall&#8217;errore di misurazione. Nella formulazione bayesiana l&#8217;intero problema è riformulato in modo tale che i valori \(y_i\) sono campioni di una distribuzione normale.</p><p>Una domanda comune in questa fase è &#8220;Qual è il vantaggio di farlo?&#8221;. Cosa otteniamo da questa riformulazione? Ci sono due ragioni principali per farlo ( Wiecki [6]):</p><ul><li><strong>Distribuzioni a priori</strong>: se abbiamo una conoscenza preliminare dei parametri, possiamo scegliere distribuzioni a priori che riflettano questi parametri. In caso contrario, possiamo comunque scegliere <a href="https://en.wikipedia.org/wiki/Prior_probability#Uninformative_priors">priori non informativi</a>.</li><li><strong>Distribuzioni  a posteriori</strong>: come menzionato in precedenza, il valore MLE frequentista per i nostri coefficienti di regressione, \(\hat{\beta}\), è solo una stima puntuale. Nella formulazione bayesiana riceviamo un&#8217;intera distribuzione di probabilità che caratterizza la nostra incertezza per i differenti coefficienti \(\beta\). Il vantaggio immediato è, dopo aver preso in considerazione tutti i dati, la possibilità di quantificare l&#8217;incertezza nei parametri \(\hat{\beta}\) tramite la varianza della distribuzione a posteriori. Una varianza maggiore indica una maggiore incertezza.</li></ul><p>Sebbene la formula dell&#8217;approccio bayesiano possa sembrare concisa, in realtà non ci dà molti indizi su come specificare un modello e campionarlo usando Markov Chain Monte Carlo. Nei prossimi paragrafi vediamo come usare <a href="https://github.com/pymc-devs/pymc3">PyMC3</a> per formulare e utilizzare un modello di regressione lineare bayesiana.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-671f0bd elementor-widget elementor-widget-text-editor" data-id="671f0bd" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Regressione lineare bayesiana con PyMC3</h2><p>In questo paragrafo vediamo un approccio consolidato con esempi statistici, cioè simuliamo alcuni dati con proprietà che già conosciamo, e quindi adattiamo un modello per verificare la presenza di queste proprietà. Abbiamo usato questa tecnica molte negli articoli precedenti, principalmente negli articoli sull&#8217;<a href="https://datatrading.info/tutorial/data-science/">analisi delle serie temporali</a>.</p><p>Nonostante seguire una tale procedura possa sembrare complicato, ci sono in realtà due principali vantaggi. Il primo vantaggio è la possibilità di capire esattamente come funziona il fitting del modello. Per farlo, dobbiamo prima capirlo, quindi ci aiuta a intuire come funziona il modello. Il secondo vantaggio è la possibilità di vedere come si comporta il modello (cioè i valori e l&#8217;incertezza che restituisce) in una situazione in cui conosciamo effettivamente i veri valori che stiamo cercano di stimare.</p><p>In questo approccio con Python usiamo le librerie <a href="http://www.numpy.org/">numpy</a> e <a href="http://pandas.pydata.org/">pandas</a> per simulare i dati, usiamo <a href="https://stanford.edu/~mwaskom/software/seaborn/">seaborn</a> per visualizzare i grafici e infine il modulo <a href="https://web.archive.org/web/20181117040421/https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function">Generalized Linear Models</a> (GLM) di PyMC3 per effettuare una regressione lineare bayesiana e campionarla, sul nostro set di dati simulati.</p><p>La seguente analisi si basa principalmente su una raccolta di post di blog scritti da Thomas Wiecki [6] e Jonathan Sedar [5], insieme a basi bayesiane più teoriche di Gelman et al [1] .</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-200ce29 elementor-widget elementor-widget-text-editor" data-id="200ce29" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h4>Cosa sono i modelli lineari generalizzati?</h4><p>Prima di iniziare a descrivere la regressione lineare bayesiana, è opportuno introdurre brevemente il concetto di modello lineare generalizzato (GLM), poiché è utile per formulare il nostro modello in PyMC3.</p><p>Un modello lineare generalizzato è un meccanismo flessibile per estendere la regressione lineare ordinaria a forme di regressione più generali, inclusa la regressione logistica (classificazione) e la regressione di Poisson (usata per i dati discreti), nonché la regressione lineare stessa.</p><p>I GLM permetto alle variabili endogene (le variabili di risposta) di avere distribuzioni di errore diverse dalla distribuzione normale (vedi \(\epsilon\) sopra, nel paragrafo della regressione frequentista). Nel modello lineare generalizzato ciascun valore dalla variabile dipendente \(\mathbf{y}\) si assume venga generato da una particolare variabile casuale della famiglia di<a href="https://en.wikipedia.org/wiki/Exponential_family"> distribuzioni esponenziali</a>. Questa famiglia di distribuzioni comprende molte distribuzioni comuni tra cui normale, gamma, beta, chi quadrato, Bernoulli, Poisson e altri.</p><p>La media \(\mathbf{\mu}\) di questa distribuzione, dipende da \(\mathbf{X}\) tramite la seguente relazione:</p><p style="text-align: center;">\(\begin{eqnarray}\mathbb{E}(\mathbf{y}) = \mu = g^{-1}(\mathbf{X}\beta)\end{eqnarray}\)</p><p>Dove \(g\) è la funzione di collegamento. In questo ambito, la varianza è tipicamente una funzione \(V\) della media:</p><p style="text-align: center;">\(\begin{eqnarray}\text{Var}(\mathbf{y}) = V(\mathbb{E}(\mathbf{y})) = V(g^{-1}(\mathbf{X}\beta))\end{eqnarray}\)</p><p>Nell&#8217;impostazione frequentista, come con la normale regressione lineare sopra, i coefficienti ignoti \(\)\beta\(\) sono stimati attraverso un approccio di <a href="https://en.wikipedia.org/wiki/Maximum_likelihood">massima verosimiglianza</a>.</p><p>Non descriviamo in modo approfondito i GLM perchè non sono oggetto di questo articolo. Li abbiamo introdotti perché dobbiamo usare il modulo <code>glm</code> di PyMC3, che è stato scritto da Thomas Wiecki e altri[6], per specificare facilmente la nostra regressione lineare bayesiana.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-229c065 elementor-widget elementor-widget-text-editor" data-id="229c065" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h4>Simulazione dei dati e adattamento del modello con PyMC3</h4>
Prima di utilizzare PyMC3 per specificare e campionare un modello bayesiano, dobbiamo simulare alcuni dati lineari rumorosi. Il seguente codice esegue queste operazioni (è una modifica ed estensione del codice presente nel post di Jonathan Sedar [5]):					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-b14e4d1 elementor-widget elementor-widget-code-highlight" data-id="b14e4d1" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns


sns.set(style="darkgrid", palette="muted")


def simulate_linear_data(
    start, stop, N, beta_0, beta_1, eps_mean, eps_sigma_sq
):
    """
    Simulate a random dataset using a noisy
    linear process.

    Parameters
    ----------
    N: `int`
        Number of data points to simulate
    beta_0: `float`
        Intercept
    beta_1: `float`
        Slope of univariate predictor, X

    Returns
    -------
    df: `pd.DataFrame`
        A DataFrame containing the x and y values.
    """
    # Create a pandas DataFrame with column 'x' containing
    # N uniformly sampled values between 0.0 and 1.0
    df = pd.DataFrame(
        {"x": 
            np.linspace(start, stop, num=N)
        }
    )
    # Use a linear model (y ~ beta_0 + beta_1*x + epsilon) to 
    # generate a column 'y' of responses based on 'x'
    df["y"] = beta_0 + beta_1*df["x"] + np.random.RandomState(s).normal(
        eps_mean, eps_sigma_sq, N
    )
    return df


def plot_simulated_data(df):
    """
    Plot the simulated data with sns.lmplot()

    Parameters
    ----------
    df: `pd.DataFrame`
        A DataFrame containing the x and y values.
    """
    # Plot the data, and a frequentist linear regression fit
    # using the seaborn package
    sns.lmplot(x="x", y="y", data=df, height=10)
    plt.xlim(0.0, 1.0)
    plt.show()


if __name__ == "__main__":
    # These are our "true" parameters
    beta_0 = 1.0  # Intercept
    beta_1 = 2.0  # Slope

    # Simulate 100 data points between 0 and 1, with a variance of 0.5
    start = 0
    stop = 1
    N = 100
    eps_mean = 0.0
    eps_sigma_sq = 0.5

    # Fix Random Seed
    s = 42

    # Simulate the "linear" data using the above parameters
    df = simulate_linear_data(
        start, stop, N, beta_0, beta_1, eps_mean, eps_sigma_sq
    )
    # Plot the simulated data
    plot_simulated_data(df)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-9c724e9 elementor-widget elementor-widget-text-editor" data-id="9c724e9" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				L&#8217;output è dato nella figura seguente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-e13c87c elementor-widget elementor-widget-image" data-id="e13c87c" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="768" height="696" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-sim-768x696.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-bayes-linear-model-sim" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-sim-768x696.png 768w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-sim-300x272.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-sim-160x145.png 160w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-sim.png 800w" sizes="(max-width: 768px) 100vw, 768px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-b65c4af elementor-widget elementor-widget-text-editor" data-id="b65c4af" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Abbiamo simulato 100 punti dati, con un&#8217;intercetta \(\beta_0=1\) e una pendenza \(\beta_1=2\). I valori epsilon sono normalmente distribuiti con media zero e varianza \(\sigma^2=\frac{1}{2}\). I dati sono stati tracciati utilizzando la funzione <code>sns.lmplot</code>. Inoltre, il metodo utilizza un approccio MLE frequentista per adattare una linea di regressione lineare ai dati.</p><p>Dopo aver eseguito la simulazione, possiamo adattare una regressione lineare bayesiana ai dati. È qui che il modulo <code>glm</code> entra in gioco. Insieme alla libreria <a href="https://bambinos.github.io/bambi/main/index.html">Bambi</a>, come descritto nel <a href="https://www.pymc.io/projects/docs/en/latest/learn/core_notebooks/GLM_linear.html?highlight=glm%20patsy#glm-linear-regression">tutorial PyMC</a>, il modulo utilizza una sintassi di specifica del modello. La libreria bambi considera un identificatore della formula del modello lineare da cui crea una matrice di progettazione, quindi aggiunge variabili casuali per ciascuno dei coefficienti e un&#8217;opportuna verosimiglianza al modello. Se bambi non è installato, si può installarlo con il comando &lt;code&gt;pip install bambi&lt;/code&gt;.</p><p>Infine, usiamo il No-U-Turn Sampler (NUTS) per eseguire l&#8217;inferenza effettiva e quindi tracciare il grafico del modello. Consideriamo 5000 campioni dalla distribuzione a posteriori e usiamo i primi 500 campioni per mettere a punto il modello, quindi scartarli come &#8220;burn in&#8221;. PyMC consente di campionare più catene in base al numero di CPU presenti nella macchina. Per impostazione predefinita è impostato sul numero di core nel sistema, in questo caso lo abbiamo impostato esplicitamente ad 1.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-057d05d elementor-widget elementor-widget-code-highlight" data-id="057d05d" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># NEW
import bambi as bmb
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
# NEW
import pymc as pm
import seaborn as sns

...


def glm_mcmc_inference(df, iterations=5000):
    """
    Calculates the Markov Chain Monte Carlo trace of
    a Generalised Linear Model Bayesian linear regression 
    model on supplied data.

    Parameters
    ----------
    df: `pd.DataFrame`
        DataFrame containing the data
    iterations: `int`
        Number of iterations to carry out MCMC for
    """
    # Create the glm using the Bambi model syntax
    model = bmb.Model("y ~ x", df)

    # Fit the model using a NUTS (No-U-Turn Sampler) 
    trace = model.fit(
        draws=5000,
        tune=500,
        discard_tuned_samples=True,
        chains=1, 
        progressbar=True)
    return trace


def plot_glm_model(trace):
    """
    Plot the trace generated from fitting the model. 

    Parameters
    ----------
    trace: `tracepymc.backends.base.MultiTrace`
        A MultiTrace or ArviZ InferenceData object that contains the samples.
    """
    pm.plot_trace(trace)
    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    # These are our "true" parameters
    beta_0 = 1.0  # Intercept
    beta_1 = 2.0  # Slope

    # Simulate 100 data points between 0 and 1, with a variance of 0.5
    start = 0
    stop = 1
    N = 100
    eps_mean = 0.0
    eps_sigma_sq = 0.5

    # Fix Random Seed
    s = 42

    # Simulate the "linear" data using the above parameters
    df = simulate_linear_data(
        start, stop, N, beta_0, beta_1, eps_mean, eps_sigma_sq
    )
    # Plot the simulated data
    plot_simulated_data(df)

    # NEW
    # Fit the GLM 
    trace = glm_mcmc_inference(df, iterations=5000)
    # Plot the GLM
    plot_glm_model(trace))</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-2ab63c1 elementor-widget elementor-widget-text-editor" data-id="2ab63c1" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				L&#8217;output dello script è il seguente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-af5e21a elementor-widget elementor-widget-code-highlight" data-id="af5e21a" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard ">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (2 chains in 2 jobs)
NUTS: [Intercept, x, y_sigma]
[-----------------]Sampling 1 chains for 500 tune and 5_000 draw iterations (500 + 5_000 draws total) took 12 seconds.</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-b50db73 elementor-widget elementor-widget-text-editor" data-id="b50db73" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Il traceplot è riportato nella figura seguente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-be2b6ce elementor-widget elementor-widget-image" data-id="be2b6ce" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="768" height="384" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-traceplot-768x384.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-bayes-linear-model-traceplot" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-traceplot-768x384.png 768w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-traceplot-300x150.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-traceplot-160x80.png 160w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-traceplot.png 800w" sizes="(max-width: 768px) 100vw, 768px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6680f6f elementor-widget elementor-widget-text-editor" data-id="6680f6f" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Nel precedente articolo sull&#8217;<a href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/">algoritmo Metropolis MCMC</a> abbiamo descritto le basi dei traceplot. Ricordiamo che i modelli bayesiani forniscono una completa distribuzione di probabilità a posteriori per ciascuno dei parametri del modello, al contrario di una stima frequentista puntiforme.</p><p>Sul lato sinistro del pannello possiamo vedere le <em>distribuzioni marginali</em> per ogni parametro di interesse. Si noti che la distribuzione dell&#8217;intercetta \(\beta_0\) ha la sua stima modale/massima a posteriori quasi a 1, vicino al valore reale del parametro \(\beta_0=1\). La stima per il parametro \(\beta_1\) della pendenza(x) ha una moda a circa 2.1, vicino al valore reale del parametro \(\beta_1=2\). Il parametro di errore y_sigma associato al rumore di misurazione del modello ha una moda di circa 0.46, che è leggermente fuori rispetto al valore reale \(\epsilon=0.5\).</p><p>In tutti i casi c&#8217;è una  ragionevole varianza associata a ciascun marginale a posteriori, che significa un certo grado di incertezza in ciascuno dei valori. Se possiamo simulare più dati ed eseguire più campioni, probabilmente  questa varianza diminuirebbe.</p><p>Il punto chiave da sottolineare è l&#8217;assenza di una stima puntuale per una retta di regressione, cioè &#8220;una retta di migliore adattamento&#8221;, come avviene nel caso frequentista. In questo caso otteniamo una <em>distribuzione</em> di probabili linee di regressione.</p><p>Possiamo tracciare queste linee insieme alla vera linea di regressione e ai dati simulati che abbiamo creato in precedenza. Iniziamo creando la nostra figura in cui il nostro asse x è un numero in virgola mobile spaziato linearmente tra 0 e 1 con 100 intervalli. Quindi tracciamo i nostri dati simulati usando la funzione <code>plt.scatter</code> di Matplotlib e il DataFrame originale, df.</p><p>Possiamo accedere ai dati numerici per le distribuzioni posteriori dall&#8217;oggetto trace creato dal modello. I dati di inferenza della traccia hanno i seguenti gruppi: <em>posterior, log_likelihood, sample_stats, observed_data</em>. Il gruppo <em>posterior</em> contiene i valori dei 5000 campioni delle distribuzioni a posteriori dell&#8217;intercetta e della pendenza(x). Questi sono array nella forma(1, 5000). Il primo valore rappresenta il numero di catene, il secondo è il numero di estrazioni/campioni. Questo set di dati presenta anche un metodo <code>to_numpy()</code> che converte i dati in un array numpy. Quindi, per inserire i valori delle intercette in un array da cui possiamo campionare e tracciare, dobbiamo usare il comando <code>trace.posterior.Intercept.to_numpy()[0]</code>. Per ottenere i dati sulle pendenze eseguiamo un comando simile, sostituendo &#8216;Intercepts&#8217; con &#8216;x&#8217;, cioè <code>trace.posterior.x.to_numpy()[0]</code>. Ora abbiamo due matrici con i valori numerici delle distribuzioni a posteriori per le nostre pendenze e intercette.</p><p>Per tracciare un campione di 100 linee, dobbiamo prelevare 100 di questi valori dai nostri array. Per prima cosa creiamo un elenco di indici di esempio usando un elenco di numeri interi casuali con il metodo <code>random.randint()</code> di numpy. Quindi eseguiamo il ciclo attraverso l&#8217;array con il valore dell&#8217;indice e creiamo una linea utilizzando l&#8217;intercetta e la pendenza, quindi tracciando la linea sul nostro grafico. Infine tracciamo la nostra vera linea di regressione usando le variabili beta_0 e beta_1 dei dati simulati. Il frammento di codice seguente produce questo grafico:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a3c4ea9 elementor-widget elementor-widget-code-highlight" data-id="a3c4ea9" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>def plot_regression_lines(trace, df, N):
    """
    Plot the simulated data with true and estimated regression lines.

    Parameters
    ----------
    trace: `tracepymc.backends.base.MultiTrace`
        A MultiTrace or ArviZ InferenceData object that contains the samples.
    df: `pd.DataFrame`
        DataFrame containing the data
    N: `int`
        Number of data points to simulate
    """
    fig, ax = plt.subplots(figsize=(7, 7))
    # define x axis ticks
    x = np.linspace(0, 1, N)
    # plot simulated data observations
    ax.scatter(df['x'], df['y'])
    # extract slope and intercept draws from PyMC trace
    intercepts = trace.posterior.Intercept.to_numpy()[0]
    slopes = trace.posterior.x.to_numpy()[0]
    # plot 100 random samples from the slope and intercept draws
    sample_indexes = np.random.randint(len(intercepts), size=100)
    for i in sample_indexes:
        y_line = intercepts[i] + slopes[i] * x
        ax.plot(x, y_line, c='black', alpha=0.07)
    # plot true regression line
    y = beta_0 + beta_1*x
    ax.plot(x, y, label="True Regression Line", lw=3., c="green")
    ax.legend(loc=0)
    ax.set_xlim(0.0, 1.0)
    ax.set_ylim(0.0, 4.0)
    plt.show()


if __name__ == "__main__":
    # These are our "true" parameters
    beta_0 = 1.0  # Intercept
    beta_1 = 2.0  # Slope

    # Simulate 100 data points between 0 and 1, with a variance of 0.5
    start = 0
    stop = 1
    N = 100
    eps_mean = 0.0
    eps_sigma_sq = 0.5

    # Fix Random Seed
    s = 42

    # Simulate the "linear" data using the above parameters
    df = simulate_linear_data(
        start, stop, N, beta_0, beta_1, eps_mean, eps_sigma_sq
    )
    # Plot the simulated data
    plot_simulated_data(df)

    # Fit the GLM 
    trace = glm_mcmc_inference(df, iterations=5000)
    # Plot the GLM
    plot_glm_model(trace)
    # NEW
    plot_regression_lines(trace, df, N)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-6f055a4 elementor-widget elementor-widget-text-editor" data-id="6f055a4" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Nella seguente figura possiamo vedere l&#8217;intervallo campionato delle linee di regressione a posteriori:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-5c06035 elementor-widget elementor-widget-image" data-id="5c06035" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="768" height="636" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-post-samples-768x636.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-bayes-linear-model-post-samples" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-post-samples-768x636.png 768w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-post-samples-300x248.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-post-samples-160x132.png 160w, https://datatrading.info/wp-content/uploads/trading-quantitativo-bayes-linear-model-post-samples.png 800w" sizes="(max-width: 768px) 100vw, 768px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-06c1425 elementor-widget elementor-widget-text-editor" data-id="06c1425" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Da quanto sopra possiamo notare la presenza di incertezza nella posizione della retta di regressione campionata dal modello bayesiano. Tuttavia, si può vedere che l&#8217;intervallo è relativamente ristretto e che l&#8217;insieme dei campioni non è troppo dissimile dalla &#8220;vera&#8221; linea di regressione stessa.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2906c80 elementor-widget elementor-widget-text-editor" data-id="2906c80" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Prossimi passi</h2><p>Nell&#8217;<a href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/">articolo precedente</a> abbiamo esaminato un  semplice metodo MCMC chiamato algoritmo Metropolis. In quell&#8217;articolo  abbiamo espresso la volontà di descrivere come funziona i vari &#8220;algoritmi&#8221; di MCMC &#8220;sotto il cofano&#8221;. Nei prossimi articoli prendiamo in considerazione gli algoritmi di Gibbs Sampler, Hamiltonian Sampler e No-U-Turn Sampler, tutti utilizzati nei principali pacchetti software bayesiani.</p><p>Alla fine vediamo l&#8217;approccio di una robusta regressione e modelli lineari gerarchici, una potente tecnica di modellazione resa possibile dalle rapide implementazioni MCMC. Da un punto di vista della finanza quantitativa descriviamo anche un modello di volatilità stocastica che utilizza PyMC e vediamo come possiamo utilizzare questo modello per formare algoritmi di trading.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-1120937 elementor-widget elementor-widget-text-editor" data-id="1120937" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Nota bibliografica</h2><p>Un&#8217;introduzione alla regressione lineare frequentista può essere trovata in James et al (2013)[4] . Una panoramica più tecnica, compresi i metodi di selezione dei sottoinsiemi, può essere trovata in Hastie et al (2009)[2]. Gelman et al (2013)[1] discutono in modo approfondito i modelli lineari bayesiani a un livello ragionevolmente tecnico.</p><p>Questo articolo è fortemente influenzato dai precedenti post sul blog di Thomas Wiecki[6], inclusa la sua discussione sui GLM bayesiani, nonché dai post di Jonathan Sedar[5].</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-97407bc elementor-widget elementor-widget-text-editor" data-id="97407bc" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Riferimenti</h2>
<ul>
 	<li><a href="http://amzn.to/1S5sSyT" name="ref-bda3">Gelman, A. et al (2013) <em>Bayesian Data Analysis, 3rd Edition</em>, Chapman and Hall/CRC</a></li>
 	<li><a href="http://statweb.stanford.edu/~tibs/ElemStatLearn/" name="ref-esl">Hastie, T., Tibshirani, R., Friedman, J. (2009) <em>The Elements of Statistical Learning</em>, Springer</a></li>
 	<li><a href="http://arxiv.org/pdf/1111.4246v1.pdf" name="ref-hoffman-gelman">Hoffman, M.D., and Gelman, A. (2011) &#8220;The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo, <em>arXiv:1111.4246 [stat.CO]</em></a></li>
 	<li><a href="http://www-bcf.usc.edu/~gareth/ISL/" name="ref-isl">James, G., Witten, D., Hastie, T., Tibshirani, R. (2013) <em>An Introduction to Statistical Learning</em>, Springer</a></li>
 	<li><a href="https://sedar.co/" name="ref-sedar">Sedar, J. (2016) <em>Bayesian Inference with PyMC3 &#8211; Part 1</em></a></li>
 	<li><a href="https://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/" name="ref-wiecki">Wiecki, T. (2015) <em>The Inference Button: Bayesian GLMs made easy with PyMC3</em>, https://twiecki.github.io/blog/2013/08/12/bayesian-glms-1/</a></li>
</ul>					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/modelli-di-regressione-lineare-bayesiana-con-pymc3/">Modelli di regressione lineare bayesiana con PyMC3</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Markov Chain Monte Carlo per l&#8217;inferenza bayesiana &#8211; L&#8217;algoritmo Metropolis</title>
		<link>https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Wed, 13 Sep 2017 19:03:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Statistica Bayesiana]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4386</guid>

					<description><![CDATA[<p>Nei precedenti articoli sull&#8217;inferenza bayesiana abbiamo introdotto la statistica bayesiana e considerato come dedurre una proporzione binomiale usando il concetto di distribuzione a priori coniugata. Abbiamo descritto come non tutti i modelli possono utilizzare la distribuzione a priori coniugata, quindi il calcolo della distribuzione a posteriori dovrebbe essere approssimato numericamente. In questo articolo introduciamo la principale famiglia &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/"> <span class="screen-reader-text">Markov Chain Monte Carlo per l&#8217;inferenza bayesiana &#8211; L&#8217;algoritmo Metropolis</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/">Markov Chain Monte Carlo per l&#8217;inferenza bayesiana &#8211; L&#8217;algoritmo Metropolis</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4386" class="elementor elementor-4386">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-474fc0c elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="474fc0c" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-d1deb60" data-id="d1deb60" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-22937f6 elementor-widget elementor-widget-text-editor" data-id="22937f6" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Nei precedenti articoli sull&#8217;inferenza bayesiana abbiamo <a href="https://datatrading.info/introduzione-alla-statistica-bayesiana/">introdotto la statistica bayesiana</a> e considerato come <a href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/">dedurre una proporzione binomiale</a> usando il concetto di <a href="https://en.wikipedia.org/wiki/Conjugate_prior" target="_blank" rel="noopener">distribuzione a priori coniugata</a>. Abbiamo descritto come non tutti i modelli possono utilizzare la distribuzione a priori coniugata, quindi il calcolo della distribuzione a posteriori dovrebbe essere approssimato numericamente.</p><p>In questo articolo introduciamo la principale famiglia di algoritmi, noti collettivamente come Markov Chain Monte Carlo (MCMC), che ci consentono di approssimare la distribuzione a posteriori calcolata dal teorema di Bayes. In particolare, consideriamo l&#8217;algoritmo Metropolis, che è facilmente enunciabile e relativamente semplice da capire. Serve come un utile punto di partenza per conoscere MCMC prima di approfondire algoritmi più sofisticati come Metropolis-Hastings, Gibbs Samplers e Hamiltonian Monte Carlo.</p><p>Dopo aver descritto il funzionamento MCMC, descriviamo come utilizzare la libreria open-source <a href="https://github.com/pymc-devs/pymc3" target="_blank" rel="noopener">PyMC3</a>, che si occupa di  implementare molti dettagli sottostanti, permettendoci di concentrarci sulla modellazione bayesiana.</p><p>Se non hai ancora guardato i precedenti articoli sulle statistiche bayesiane, ti suggerisco di leggere quanto segue prima di procedere:</p><ul><li><a href="https://datatrading.info/introduzione-alla-statistica-bayesiana/">Introduzione alla Statistica Bayesiana</a></li><li><a href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/">Inferenza bayesiana di una proporzione binomiale &#8211; L&#8217;approccio analitico</a></li></ul><h2>Obiettivi dell&#8217;inferenza bayesiana</h2><div> </div><p>Il nostro obiettivo nel campo della statistica bayesiana è <em>produrre strategie di trading quantitative basate su modelli bayesiani</em>. Tuttavia, per raggiungere tale obiettivo, dobbiamo  introdurre una ragionevole  quantità di teoria sulla statistica bayesiana. Finora abbiamo descritto:</p><ul><li>La filosofia della statistica bayesiana, utilizzando il teorema di Bayes per aggiornare le nostre precedenti convinzioni sulle probabilità dei risultati sulla base di nuovi dati</li><li>Come utilizzare la distribuzione a priori coniugata come mezzo per semplificare il calcolo della distribuzione a posteriori nel caso di inferenza su una proporzione binomiale</li></ul><p>In questo articolo descriviamo di MCMC come mezzo per calcolare la distribuzione a posteriori quando la distribuzione a priori coniugata non è applicabile.</p><p>Dopo la descrizione del MCMC, <span style="background-color: var(--ast-global-color-5); font-size: 1rem; font-weight: inherit;">in questo articolo</span><span style="font-style: inherit; font-weight: inherit; background-color: var(--ast-global-color-5);"> consideriamo campionatori più sofisticati </span><span style="background-color: var(--ast-global-color-5); font-size: 1rem; font-weight: inherit;">utilizzando PyMC3 </span><span style="background-color: var(--ast-global-color-5); font-style: inherit; font-weight: inherit;">e quindi li applichiamo a modelli più complessi. In definitiva, vediamo come i nostri modelli sono abbastanza utili da fornire informazioni dettagliate sulla previsione dei rendimenti degli asset. A quel punto siamo in grado di iniziare a costruire un modello di trading a partire dall&#8217;analisi bayesiana.</span></p><h2>Perché il Markov Chain Monte Carlo?</h2><div> </div><p>Nell&#8217;articolo precedente abbiamo considerato la distribuzione a priori coniugata, che ha fornito una significativa &#8220;scorciatoia&#8221; matematica per calcolare la distribuzione a posteriori presente nella regola di Bayes. Una domanda perfettamente legittima a questo punto consiste nel capire perché abbiamo bisogno di MCMC se possiamo semplicemente usare la distribuzione a priori coniugata.</p><p>Dobbiamo prevedere il Markov Chain Monte Carlo perchè non tutti i modelli possono essere sinteticamente enunciati in termini di priori coniugati. In particolare, molte situazioni di modellazione più complicate, in particolare quelle relative a <a href="https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling" target="_blank" rel="noopener">modelli gerarchici</a> con centinaia di parametri, sono completamente intrattabili con metodi analitici.</p><p>Se ricordiamo la regola di Bayes:</p><p style="text-align: center;">\(\begin{eqnarray}P(\theta | D) = \frac{P(D | \theta) P(\theta)}{P(D)}\end{eqnarray}\)</p><p>E&#8217; evidente che dobbiamo calcolare le <em>prove</em> \(P(D)\). Per ottenere ciò dobbiamo valutare il seguente integrale, che integra su tutti i possibili valori di \(\theta\):</p><p style="text-align: center;">\(\begin{eqnarray}P(D) = \int_{\Theta} P(D, \theta) \text{d}\theta\end{eqnarray}\)</p><p>Il problema fondamentale consiste nella difficoltà di valutare analiticamente questo integrale e quindi è necessario ricorrere un metodo di approssimazione numerica.</p><p>Un ulteriore problema è l&#8217;elevato numero dei parametri che i nostri modelli potrebbero richiedere. Ciò significa che le nostre precedenti distribuzioni potrebbero potenzialmente avere un numero elevato di dimensioni, con la conseguenza che anche le distribuzioni a posteriori saranno di dimensioni elevate. In conclusione, si tratta di dover valutare numericamente un integrale in uno spazio dimensionale potenzialmente molto grande.</p><p>Questo scenario è spesso spesso descritto come la <a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank" rel="noopener">Maledizione della Dimensionalità</a>. Informalmente, significa che il volume di uno spazio ad alta dimensione è così vasto che tutti i dati disponibili diventano estremamente scarsi all&#8217;interno di quello spazio e quindi portano a problemi di rilevanza statistica. In pratica, per acquisire una qualsiasi significatività statistica, il volume dei dati necessari deve crescere in modo esponenziale con il numero delle dimensioni.</p><p>Tali problemi sono spesso estremamente difficili da affrontare se non vengono affrontati in modo intelligente. In questa situazione è la motivazione alla base dei metodi Markov Chain Monte Carlo, che  permettono una ricerca intelligente all&#8217;interno di uno spazio dimensionale elevato, e quindi i modelli bayesiani di dimensioni elevate diventano trattabili.</p><p>L&#8217;idea di base è campionare dalla distribuzione a posteriori combinando una &#8220;ricerca casuale&#8221; (l&#8217;aspetto Monte Carlo) con un meccanismo per &#8220;saltare&#8221; in modo intelligente, ma in un modo che alla fine non dipende da dove siamo partiti (una <a href="https://datatrading.info/le-proprieta-di-markov-e-martingale/">proprietà della Catena di Markov</a>). Quindi i metodi Markov Chain Monte Carlo sono ricerche senza memoria eseguite con salti intelligenti.</p><p><em>Per completezza, il MCMC non è utilizzato solo per eseguire statistiche bayesiane. È anche ampiamente utilizzato nella fisica computazionale e nella biologia computazionale in quanto può essere applicato generalmente all&#8217;approssimazione di qualsiasi integrale di alta dimensione.</em></p><h4>Algoritmi di Monte Carlo della catena di Markov</h4><div> </div><p>Il Markov Chain Monte Carlo è una famiglia di algoritmi, piuttosto che un metodo particolare. In questo articolo ci concentriamo su uno specifico metodo noto come l&#8217;algoritmo Metropolis. Nei prossimi articoli prenderemo in considerazione Metropolis-Hastings, Gibbs Sampler, Hamiltonian MCMC e No-U-Turn Sampler (NUTS). Quest&#8217;ultimo è effettivamente incorporato in PyMC3, il software che  usiamo per dedurre numericamente la nostra proporzione binomiale in questo articolo.</p><h2>L&#8217;algoritmo Metropolis</h2><div> </div><p>Il primo algoritmo MCMC considerato in questa serie di articoli è dovuto a <a href="http://bayes.wustl.edu/Manual/EquationOfState.pdf" target="_blank" rel="noopener">Metropolis (1953)</a>. Come puoi vedere, è un metodo piuttosto vecchio! Sebbene da allora siano stati apportati miglioramenti sostanziali agli algoritmi di campionamento MCMC, questo metodo è sufficiente per questo articolo. La descrizione di questo  semplice metodo ci aiuterà a comprendere i campionatori più complessi descritti negli articoli successivi.</p><p>La maggior parte degli algoritmi MCMC seguono il seguente schema (vedi <a href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/#ref-davidson-pilon">Metodi bayesiani per hacker</a> per maggiori dettagli):</p><ol><li style="list-style-type: none;"><ol><li>Iniziare l&#8217;algoritmo nella posizione corrente nello spazio dei parametri (\(\theta_{\text{current}}\)).</li><li>Proporre un &#8220;salto&#8221; in una nuova posizione nello spazio dei parametri (\(\theta_{\text{new}}\)).</li><li>Accettare o rifiutare il salto in modo probabilistico, utilizzando le informazioni precedenti e i dati disponibili.</li><li>Se il salto viene accettato, spostarsi nella nuova posizione e tornare al passaggio 1.</li><li>Se il salto viene rifiutato, restare nell&#8217;attuale posizione e tornare al passaggio 1.</li><li>Dopo che si è verificato un determinato numero di salti, restituire tutte le posizioni accettate</li></ol></li></ol><p><span style="font-style: inherit; font-weight: inherit; background-color: var(--ast-global-color-5);">La principale differenza tra gli algoritmi MCMC risiede nel </span><em style="font-weight: inherit; background-color: var(--ast-global-color-5);">modo in cui si salta</em><span style="font-style: inherit; font-weight: inherit; background-color: var(--ast-global-color-5);"> e nel </span><em style="font-weight: inherit; background-color: var(--ast-global-color-5);">modo in cui decide se saltare</em><span style="font-style: inherit; font-weight: inherit; background-color: var(--ast-global-color-5);">.</span></p><p>L&#8217;algoritmo Metropolis utilizza una distribuzione normale per proporre un salto. Questa distribuzione normale ha un valore medio <span style="background-color: var(--ast-global-color-5); font-size: 1rem; font-weight: inherit;"><strong>μ</strong> </span><span style="font-style: inherit; font-weight: inherit; background-color: var(--ast-global-color-5);">che è uguale alla posizione corrente e accetta una &#8220;larghezza della proposta&#8221; per la sua deviazione standard </span><strong><span style="background-color: var(--ast-global-color-5); font-size: 1rem;">σ</span></strong>.</p><p>La larghezza della proposta è un parametro dell&#8217;algoritmo Metropolis e ha un impatto significativo sulla convergenza. Una maggiore larghezza della proposta salterà ulteriormente e coprirà più spazio nella distribuzione a posteriori, ma potrebbe inizialmente perdere una regione di maggiore probabilità. Tuttavia, una larghezza della proposta più piccola non coprirà rapidamente molto spazio e quindi potrebbe richiedere più tempo per convergere.</p><p>Una distribuzione normale è una buona scelta per tale distribuzione proposta (per parametri continui) poiché, per definizione, è più probabile che si selezioni punti più vicini alla posizione attuale che più lontani. Tuttavia, occasionalmente si sceglierà punti più lontani, consentendo di esplorare tutto lo spazio.</p><p>Una volta che il salto è stato proposto, dobbiamo decidere (in maniera probabilistica) se è una buona mossa saltare alla nuova posizione. Come effettuare questa verifica? Calcoliamo il rapporto tra la distribuzione proposta della <em>nuova</em> posizione e la distribuzione proposta della posizione <em>attuale</em> per determinare la probabilità di spostamento, <strong>p</strong>:</p><p style="text-align: center;">\(\begin{eqnarray}p = P(\theta_{\text{new}})/P(\theta_{\text{current}})\end{eqnarray}\)</p><p>Quindi generiamo un numero casuale uniforme sull&#8217;intervallo [0, 1]. Se questo numero è contenuto nell&#8217;intervallo [0, p] allora <span style="font-size: 16px;">accettiamo il movimento, altrimenti lo rifiutiamo.</span></p><p>Sebbene questo sia un algoritmo relativamente semplice, non è immediatamente chiaro come ci possa aiutare ad evitare il problema del calcolo di un integrale con  dimensione elevata dell&#8217;evidenza, \(P(D)\).</p><p>Come <a href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/" target="_blank" rel="noopener">sottolinea Thomas Wiecki nel suo articolo sul campionamento MCMC</a>, in realtà stiamo dividendo il posteriore del parametro proposto per il posteriore del parametro corrente. Utilizzando la regola di Bayes si elimina le prove, \(P(D)\), dal<span style="font-size: 16px;"> rapporto:</span></p><p style="text-align: center;">\(\begin{eqnarray}\frac{P(\theta_{\text{new}}|D)}{P(\theta_{\text{current}}|D)} = \frac{\frac{P(D|\theta_{\text{new}})P(\theta_{\text{new}})}{P(D)}}{\frac{P(D|\theta_{\text{current}})P(\theta_{\text{current}})}{P(D)}} = \frac{P(D|\theta_{\text{new}})P(\theta_{\text{new}})}{P(D|\theta_{\text{current}})P(\theta_{\text{current}})}\end{eqnarray}\)</p><p>Il lato destro di quest&#8217;ultima uguaglianza contiene solo le verosimiglianze e i priori, entrambi facilmente calcolabili. Quindi, dividendo il posteriore in una posizione per il posteriore in un&#8217;altra, il più delle volte campionamo regioni con probabilità a posteriori più elevate, in un modo che riflette pienamente la probabilità dei dati.</p><h2>Introduzione a PyMC3</h2><div> </div><p><a href="https://github.com/pymc-devs/pymc3" target="_blank" rel="noopener">PyMC3</a> è una libreria Python che effettua la &#8220;Programmazione Probabilistica&#8221;. Cioè, possiamo definire un modello probabilistico e quindi eseguire l&#8217;inferenza bayesiana sul modello, utilizzando vari tipi di  metodi Markov Chain Monte Carlo. In questo senso è simile ai pacchetti <a href="http://mcmc-jags.sourceforge.net/" target="_blank" rel="noopener">JAGS</a> e <a href="http://mc-stan.org/" target="_blank" rel="noopener">Stan</a>. PyMC3 ha una <a href="https://github.com/pymc-devs/pymc3/graphs/contributors" target="_blank" rel="noopener">lunga lista di collaboratori</a> ed è sempre in fase sviluppo  ed aggiornamento.</p><p>PyMC3 è stato progettato con una sintassi pulita che consente la specifica del modello estremamente semplice, con un codice &#8220;boilerplate&#8221; minimale. Esistono classi per tutte le principali distribuzioni di probabilità ed è facile aggiungere altre distribuzioni specialistiche. Ha una suite diversificata e potente di algoritmi di campionamento MCMC, incluso l&#8217;algoritmo Metropolis di cui abbiamo discusso sopra, nonché <a href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/#ref-hoffman-gelman">No-U-Turn Sampler (NUTS)</a> . Questo ci permette di definire modelli complessi con molte migliaia di parametri.</p><p>PyMC3 utilizza inoltre la libreria <a href="https://theano-pymc.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Theano</a> di Python, spesso utilizzata per applicazioni di <a href="https://en.wikipedia.org/wiki/Deep_learning" target="_blank" rel="noopener">Deep Learning</a> ad alta intensità di CPU/GPU , al fine di massimizzare l&#8217;efficienza nella velocità di esecuzione.</p><p><em>Descrivamo Theano negli articoli che trattano del <a href="https://datatrading.info/deep-learning-con-theano-regressione-logistica/">Deep Learning applicato al trading quantitativo</a>.</em></p><p>In questo articolo utilizziamo PyMC3 per eseguire un semplice esempio di deduzione di una proporzione binomiale, sufficiente per descrivere le principali idee, senza impantanarsi nelle specifiche di implementazione di MCMC. Negli articoli successivi esploriamo più funzionalità di PyMC3 quando dobbiamo eseguire l&#8217;inferenza su modelli più sofisticati.</p><h2>Dedurre una proporzione binomiale con Markov Chain Monte Carlo</h2><div> </div><p>Come descritto nell&#8217;articolo <a href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/">sull&#8217;inferenza di una proporzione binomiale usando la distribuzione a priori coniugata</a> , il nostro obiettivo è stimare l&#8217;equità di una moneta, eseguendo una sequenza di lanci di monete.</p><p>L&#8217;equità della moneta è data da un parametro \(\theta \in [0,1]\)</p><p><math xmlns="http://www.w3.org/1998/Math/MathML"><mo stretchy="false">, </mo></math></p><p>dove \(\theta=0.5\) significa una moneta con la stessa probabilità di avere testa o croce.</p><p>Abbiamo descritto la possibilità di utilizzare una distribuzione di probabilità relativamente flessibile, la <a href="https://en.wikipedia.org/wiki/Beta_distribution" target="_blank" rel="noopener">distribuzione beta</a>, per modellare la nostra precedente convinzione  sull&#8217;equità della moneta. Abbiamo anche appreso che, utilizzando una funzione di probabilità di <a href="https://en.wikipedia.org/wiki/Bernoulli_trial" target="_blank" rel="noopener">Bernoulli</a> per simulare i lanci di monete virtuali con una particolare equità, la nostra convinzione a posteriori avrebbe anche in questo caso la forma di una distribuzione beta. Questo è un esempio di una  <em>distribuzione a priori coniugata</em>.</p><p>Per essere chiari, questo significa <em>che non è necessario utilizzare MCMC per stimare il posteriore in questo caso particolare</em> poiché esiste già una soluzione analitica in forma chiusa. Tuttavia, la maggior parte dei modelli di inferenza bayesiana non ammette una soluzione in forma chiusa per il posteriore, e quindi è necessario utilizzare MCMC.</p><p>Applichiamo MCMC a un caso di cui &#8220;conosciamo già la risposta&#8221;, in modo da poter confrontare i risultati di una soluzione in forma chiusa e quella calcolata per approssimazione numerica.</p><h4>Riepilogo dell&#8217;inferenza di una proporzione binomiale con la distribuzione a priori coniugata</h4><div> </div><p>Nell&#8217;articolo <a href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/">precedente</a> abbiamo assunto una particolare convinzione  a priori, cioè la moneta fosse probabilmente equa, ma non ne siamo pienamente certi. Questo si traduce nel considerare \(\theta\) con una media \(\mu=0.5\) e una deviazione standard \(\sigma=0.1\).</p><p>Una distribuzione beta ha due parametri, \(\alpha\) e \(\beta\), che caratterizzano la &#8220;forma&#8221; delle nostre convinzioni. Una media \(\mu=0.5\) <span style="font-size: 16px;">e una deviazione standard \(\sigma=0.1\) </span>si traduce in \(\alpha=12\) e \(\beta=12\) <span style="font-size: 16px;">(vedi l&#8217;</span><a style="font-size: 16px; background-color: #ffffff;" href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/">articolo precedente</a><span style="font-size: 16px;"> per i dettagli su questa trasformazione).</span></p><p>Abbiamo quindi effettuato 50 lanci e osservato 10 teste. Quando lo abbiamo inserito nella nostra soluzione in forma chiusa per la distribuzione beta a posteriori, abbiamo ricevuto un posteriore con \(\alpha=22\) e \(\beta=52\). Di seguito la figura che mostra le due distribuzioni:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2283122 elementor-widget elementor-widget-image" data-id="2283122" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="403" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-qs-beta-distribution-prior-posterior.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-qs-beta-distribution-prior-posterior" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-qs-beta-distribution-prior-posterior.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-qs-beta-distribution-prior-posterior-300x173.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-qs-beta-distribution-prior-posterior-160x92.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-f9fb253 elementor-widget elementor-widget-text-editor" data-id="f9fb253" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Possiamo intuitivamente vedere come la massa della probabilità si è spostata drasticamente a più vicino a 0,2, che è l&#8217;equità del campione dai nostri lanci. Da notare anche che il picco è diventato più stretto poiché siamo ora abbastanza fiduciosi nei nostri risultati, dopo aver effettuato 50 lanci.</p><h4>Dedurre una proporzione binomiale con PyMC3</h4><div> </div><p>Ora eseguiremo la stessa analisi utilizzando invece il metodo numerico Markov Chain Monte Carlo. <span style="font-style: inherit; font-weight: inherit; background-color: var(--ast-global-color-5);">Innanzitutto, dobbiamo installare PyMC3:</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-0a9a063 elementor-widget elementor-widget-code-highlight" data-id="0a9a063" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>pip install --process-dependency-links git+https://github.com/pymc-devs/pymc3</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-e908bd4 elementor-widget elementor-widget-text-editor" data-id="e908bd4" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Una volta installato, dobbiamo successivamente  importare le librerie necessarie, che includono Matplotlib, Numpy, Scipy e lo stesso PyMC3. Abbiamo anche impostato lo stile grafico dell&#8217;output di Matplotlib in modo che sia simile alla libreria grafica <a href="https://ggplot2.tidyverse.org/" target="_blank" rel="noopener">ggplot2</a>:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-79f0ce4 elementor-widget elementor-widget-code-highlight" data-id="79f0ce4" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import matplotlib.pyplot as plt
import numpy as np
import pymc3
import scipy.stats as stats

plt.style.use("ggplot")</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-e1fa766 elementor-widget elementor-widget-text-editor" data-id="e1fa766" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Il prossimo passo è impostare i nostri parametri precedenti, così come il numero di prove di lancio di monete effettuate e le teste restituite. Specifichiamo anche, per completezza, i parametri della distribuzione beta calcolata analiticamente a posteriori, che utilizzeremo per il confronto con il nostro approccio MCMC. Inoltre precisiamo di voler effettuare 100.000 iterazioni dell&#8217;algoritmo Metropolis:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a29c60c elementor-widget elementor-widget-code-highlight" data-id="a29c60c" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Parameter values for prior and analytic posterior
n = 50
z = 10
alpha = 12
beta = 12
alpha_post = 22
beta_post = 52

# How many iterations of the Metropolis 
# algorithm to carry out for MCMC
iterations = 100000</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-22cb017 elementor-widget elementor-widget-text-editor" data-id="22cb017" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Ora definiamo la nostra distribuzione beta a priori e il modello di probabilità di Bernoulli. PyMC3 ha un&#8217;API molto pulita per eseguire questa operazione. Si usa un context <code>with</code> di Python per assegnare tutti i parametri, le dimensioni dei passaggi e i valori iniziali a un&#8217;istanza <code>pymc3.Model</code> (che denominiamo <code>basic_model</code>, come dal <a href="https://www.pymc.io/projects/docs/en/stable/learn.html" target="_blank" rel="noopener">tutorial PyMC3</a>).</p><p>In primo luogo, specifichiamo il parametro <code>theta</code> come distribuzione beta, prendendo i valori a priori <code>alpha</code> e <code>beta</code> come parametri. Ricordiamo che i specifici valori di \(\alpha=12\) e \(\beta=12\) <span style="font-size: 16px;">implicano una media a priori \(\mu=0.5\) e una deviazione standard a priori \(\sigma=0.1\)</span><span style="font-size: 16px;">.</span></p><p>Definiamo quindi la funzione di verosimiglianza di Bernoulli, specificando il parametro di equità <code>p=theta</code>, il numero di prove <code>n=n</code>e le teste osservate <code>observed=z</code>, il tutto ricavato dai parametri sopra specificati.</p><p>A questo punto possiamo trovare un valore di partenza ottimale per l&#8217;algoritmo Metropolis utilizzando l&#8217;ottimizzazione PyMC3 Maximum A Posteriori (MAP) (ne parliamo nel dettagli negli articoli successivi). Infine specifichiamo il campionatore  <code>Metropolis</code> da utilizzare e quindi i risultati effettivi <code>sample(..)</code>. Questi risultati sono memorizzati nella variabile <code>trace</code>:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-b0df5ea elementor-widget elementor-widget-code-highlight" data-id="b0df5ea" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Use PyMC3 to construct a model context
basic_model = pymc3.Model()
with basic_model:
    # Define our prior belief about the fairness
    # of the coin using a Beta distribution
    theta = pymc3.Beta("theta", alpha=alpha, beta=beta)

    # Define the Bernoulli likelihood function
    y = pymc3.Binomial("y", n=n, p=theta, observed=z)

    # Carry out the MCMC analysis using the Metropolis algorithm
    # Use Maximum A Posteriori (MAP) optimisation as initial value for MCMC
    start = pymc3.find_MAP() 

    # Use the Metropolis algorithm (as opposed to NUTS or HMC, etc.)
    step = pymc3.Metropolis()

    # Calculate the trace
    trace = pymc3.sample(iterations, step, start, random_seed=1, progressbar=True)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-a047fdf elementor-widget elementor-widget-text-editor" data-id="a047fdf" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>Si noti come la specifica del modello tramite l&#8217;API PyMC3 sia quasi simile alla specifica matematica effettiva del modello, con un codice &#8220;boilerplate&#8221; minimale. Dimostreremo la potenza di questa API negli articoli successivi quando arriveremo a specificare alcuni modelli più complessi.</span></p><p><span>Ora che il modello è stato specificato e campionato, desideriamo tracciare i risultati. Creiamo un istogramma dalla </span><strong><span>traccia</span></strong><span> (l&#8217;elenco di tutti i campioni accettati) del campionamento MCMC utilizzando 50 bin. Tracciamo quindi le distribuzioni beta analitiche precedenti e posteriori utilizzando il metodo </span><code>stats.beta.pdf(..)</code><span> di SciPy. Infine, aggiungiamo alcune etichette al grafico e lo visualizziamo:</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-94c9305 elementor-widget elementor-widget-code-highlight" data-id="94c9305" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Plot the posterior histogram from MCMC analysis
bins=50
plt.hist(
    trace["theta"], bins, 
    histtype="step", normed=True, 
    label="Posterior (MCMC)", color="red"
)

# Plot the analytic prior and posterior beta distributions
x = np.linspace(0, 1, 100)
plt.plot(
    x, stats.beta.pdf(x, alpha, beta), 
    "--", label="Prior", color="blue"
)
plt.plot(
    x, stats.beta.pdf(x, alpha_post, beta_post), 
    label='Posterior (Analytic)', color="green"
)

# Update the graph labels
plt.legend(title="Parameters", loc="best")
plt.xlabel("$\\theta$, Fairness")
plt.ylabel("Density")
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-0a602a4 elementor-widget elementor-widget-text-editor" data-id="0a602a4" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Quando il codice viene eseguito si ottiene il seguente output:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-4990ebd elementor-widget elementor-widget-code-highlight" data-id="4990ebd" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>Applied logodds-transform to theta and added transformed theta_logodds to model.
[-----            14%                  ] 14288 of 100000 complete in 0.5 sec
[----------       28%                  ] 28857 of 100000 complete in 1.0 sec
[---------------- 43%                  ] 43444 of 100000 complete in 1.5 sec
[-----------------58%--                ] 58052 of 100000 complete in 2.0 sec
[-----------------72%-------           ] 72651 of 100000 complete in 2.5 sec
[-----------------87%-------------     ] 87226 of 100000 complete in 3.0 sec
[-----------------100%-----------------] 100000 of 100000 complete in 3.4 sec</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-8413e02 elementor-widget elementor-widget-text-editor" data-id="8413e02" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>Chiaramente, il tempo di campionamento dipenderà dalla velocità del tuo computer. </span><span>L&#8217;output grafico dell&#8217;analisi è riportato nella seguente immagine:</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-03f8a9e elementor-widget elementor-widget-image" data-id="03f8a9e" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="406" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-beta-distribution-mcmc" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc-300x174.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc-160x93.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-d870d97 elementor-widget elementor-widget-text-editor" data-id="d870d97" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>In questo particolare caso di modello a parametro singolo, con 100.000 campioni, la convergenza dell&#8217;algoritmo Metropolis è estremamente buona. L&#8217;istogramma segue da vicino la distribuzione calcolata analiticamente a posteriore, come da previsione. In un modello relativamente semplice come questo non abbiamo bisogno di calcolare 100.000 campioni ma ne sarebbero sufficienti molti meno. Tuttavia, sottolinea la convergenza dell&#8217;algoritmo Metropolis.</p><p>Possiamo anche considerare un concetto noto come <strong>traccia</strong>, cioè il vettore dei campioni prodotti dalla procedura di campionamento MCMC. Possiamo usare il metodo  <code>traceplot</code> per tracciare sia una <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="noopener">stima della densità del kernel</a> (KDE) dell&#8217;istogramma visualizzato sopra, sia la traccia.</p><p>Il trace plot è estremamente utile per valutare la convergenza di un algoritmo MCMC e se è necessario escludere un periodo di campioni iniziali (noto come <strong>burn-in</strong>). Descriviamo la traccia, il burn-in e altri problemi di convergenza in altri articoli dove studiamo campionatori più sofisticati. Per produrre la traccia chiamiamo semplicemente <code>traceplot</code> con la variabile <code>trace</code>:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-263327b elementor-widget elementor-widget-code-highlight" data-id="263327b" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Show the trace plot
pymc3.traceplot(trace)
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-dde10a9 elementor-widget elementor-widget-text-editor" data-id="dde10a9" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Ecco il tracciato completo:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-9d5b11f elementor-widget elementor-widget-image" data-id="9d5b11f" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="456" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc-traceplot.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-beta-distribution-mcmc-traceplot" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc-traceplot.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc-traceplot-300x195.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-beta-distribution-mcmc-traceplot-160x104.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2d505bf elementor-widget elementor-widget-text-editor" data-id="2d505bf" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Da notare come la stima di KDE della convinzione a posteriori nell&#8217;equità riflette sia la convinzione a priori di <span style="font-size: 16px;">\(\sigma=0.5\) </span>che i nostri dati con una correttezza campionaria di <span style="font-size: 16px;">\(\sigma=0.2\)</span>. Inoltre possiamo vedere che la procedura di campionamento MCMC è &#8220;convergente alla distribuzione&#8221; poiché la serie di campionamento sembra stazionaria.</p><p>In casi più complicati, che esaminiamo in altri articoli,  dobbiamo considerare un periodo di &#8220;burn in&#8221; così come i &#8220;sottili&#8221; risultati per rimuovere l&#8217;autocorrelazione, entrambi i quali migliorano la convergenza.</p><p>Per completezza, di seguito l&#8217;elenco completo:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-fdb66ef elementor-widget elementor-widget-code-highlight" data-id="fdb66ef" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import matplotlib.pyplot as plt
import numpy as np
import pymc3
import scipy.stats as stats

plt.style.use("ggplot")

# Parameter values for prior and analytic posterior
n = 50
z = 10
alpha = 12
beta = 12
alpha_post = 22
beta_post = 52

# How many iterations of the Metropolis 
# algorithm to carry out for MCMC
iterations = 100000

# Use PyMC3 to construct a model context
basic_model = pymc3.Model()
with basic_model:
    # Define our prior belief about the fairness
    # of the coin using a Beta distribution
    theta = pymc3.Beta("theta", alpha=alpha, beta=beta)

    # Define the Bernoulli likelihood function
    y = pymc3.Binomial("y", n=n, p=theta, observed=z)

    # Carry out the MCMC analysis using the Metropolis algorithm
    # Use Maximum A Posteriori (MAP) optimisation as initial value for MCMC
    start = pymc3.find_MAP() 

    # Use the Metropolis algorithm (as opposed to NUTS or HMC, etc.)
    step = pymc3.Metropolis()

    # Calculate the trace
    trace = pymc3.sample(iterations, step, start, random_seed=1, progressbar=True)

# Plot the posterior histogram from MCMC analysis
bins=50
plt.hist(
    trace["theta"], bins, 
    histtype="step", normed=True, 
    label="Posterior (MCMC)", color="red"
)

# Plot the analytic prior and posterior beta distributions
x = np.linspace(0, 1, 100)
plt.plot(
    x, stats.beta.pdf(x, alpha, beta), 
    "--", label="Prior", color="blue"
)
plt.plot(
    x, stats.beta.pdf(x, alpha_post, beta_post), 
    label='Posterior (Analytic)', color="green"
)

# Update the graph labels
plt.legend(title="Parameters", loc="best")
plt.xlabel("$\\theta$, Fairness")
plt.ylabel("Density")
plt.show()

# Show the trace plot
pymc3.traceplot(trace)
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-ac503df elementor-widget elementor-widget-text-editor" data-id="ac503df" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2><span>Prossimi passi</span></h2><div><span><br></span></div>
<p><span>In questo articolo abbiamo descritto le basi di MCMC, nonché un metodo specifico noto come algoritmo Metropolis, applicato per dedurre una proporzione binomiale.</span></p>
<p><span>Tuttavia, come discusso in precedenza, PyMC3 utilizza un campionatore MCMC molto più sofisticato noto come No-U-Turn Sampler (NUTS).&nbsp;Per capire la logica di questo campionatore, alla fine dobbiamo considerare ulteriori tecniche di campionamento come Metropolis-Hastings, Gibbs Sampling e Hamiltonian Monte Carlo (su cui si basa NUTS).</span></p>
<p><span>Vogliamo anche iniziare ad applicare le tecniche di Programmazione Probabilistica a modelli più complessi, come i modelli gerarchici.&nbsp;Questo a sua volta ci aiuterà a produrre sofisticate strategie di trading quantitativo.</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-db6927d elementor-widget elementor-widget-text-editor" data-id="db6927d" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Riferimenti</h2><ul><li><a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/" name="ref-davidson-pilon">Davidson-Pilon, C. et al (2016) &#8220;Probabilistic Programming &amp; Bayesian Methods for Hackers&#8221;, <em>http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/</em></a></li><li><a href="http://www.sciencedirect.com/science/article/pii/037026938791197X" name="ref-duane-et-al">Duane, S. et al (1987) &#8220;Hybrid Monte Carlo&#8221;, <em>Physics Letters B 195 (2): 216–222</em></a></li><li><a href="https://www.ma.imperial.ac.uk/~das01/MyWeb/SCBI/Papers/GelfandSmith.pdf" name="ref-gelfand-smith">Gelfand, A.E. and Smith, A.F.M. (1990) &#8220;Sampling-based approaches to calculating marginal densities&#8221;, <em>J. Amer. Statist. Assoc.</em>, 85, 140, 398-409.</a></li><li><a href="http://www.stat.cmu.edu/~acthomas/724/Geman.pdf" name="ref-geman-geman">Geman, S. and Geman, D. (1984) &#8220;Stochastic relaxation, Gibbs distributions and the Bayesian restoration of images.&#8221;, <em>IEEE Trans. Pattern Anal. Mach. Intell.</em>, 6, 721-741.</a></li><li><a href="http://amzn.to/1S5sSyT" name="ref-bda3">Gelman, A. et al (2013) <em>Bayesian Data Analysis, 3rd Edition</em>, Chapman and Hall/CRC</a></li><li><a href="https://www.jstor.org/stable/2334940" name="ref-hastings">Hastings, W. (1970) &#8220;Monte Carlo sampling methods using Markov chains and their application&#8221;, <em>Biometrika</em>, 57, 97-109.</a></li><li><a href="http://arxiv.org/pdf/1111.4246v1.pdf" name="ref-hoffman-gelman">Hoffman, M.D., and Gelman, A. (2011) &#8220;The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo, <em>arXiv:1111.4246 [stat.CO]</em></a></li><li><a href="http://amzn.to/25lETKe" name="ref-kruschke">Kruschke, J. (2014) <em>Doing Bayesian Data Analysis, Second Edition: A Tutorial with R, JAGS, and Stan</em>, Academic Press</a></li><li><a href="http://bayes.wustl.edu/Manual/EquationOfState.pdf" name="ref-metropolis">Metropolis, N. et al (1953) &#8220;Equations of state calculations by fast computing machines&#8221;, <em>J. Chem. Phys.</em>, 21, 1087-1092.</a></li><li><a href="http://www.stat.ufl.edu/archived/casella/Papers/MCMCHistory.pdf" name="ref-robert-casella">Robert, C. and Casella, G. (2011) &#8220;A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data&#8221;, <em>Statistical Science</em> 0, 00, 1-14.</a></li><li><a href="http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/" name="ref-wiecki">Wiecki, T. (2015) &#8220;MCMC sampling for dummies&#8221;, <em>http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/</em></a></li></ul>					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/">Markov Chain Monte Carlo per l&#8217;inferenza bayesiana &#8211; L&#8217;algoritmo Metropolis</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Inferenza Bayesiana di una proporzione Binomiale &#8211; L&#8217;approccio Analitico</title>
		<link>https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Sat, 09 Sep 2017 08:58:49 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Statistica Bayesiana]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=991</guid>

					<description><![CDATA[<p>Nel precedente articolo relativo alle statistiche bayesiane è stata esaminata la regola di Bayes ed evidenziato come permette di aggiornare razionalmente le convinzioni sull&#8217;incertezza dopo aver avuto evidenza di nuove prove. E&#8217; stato inoltre brevemente accennato che tali tecniche stanno diventando estremamente importanti nei campi della data science e della finanza quantitativa. In questo articolo &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/"> <span class="screen-reader-text">Inferenza Bayesiana di una proporzione Binomiale &#8211; L&#8217;approccio Analitico</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/">Inferenza Bayesiana di una proporzione Binomiale &#8211; L&#8217;approccio Analitico</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="991" class="elementor elementor-991">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-f31b7e9 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="f31b7e9" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-7f676fb" data-id="7f676fb" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-7d885be elementor-widget elementor-widget-text-editor" data-id="7d885be" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Nel <a href="http://datatrading.info/introduzione-alla-statistica-bayesiana/">precedente articolo</a> relativo alle statistiche bayesiane è stata esaminata la <a href="http://en.wikipedia.org/wiki/Bayes%27_rule">regola di Bayes</a> ed evidenziato come permette di aggiornare razionalmente le convinzioni sull&#8217;incertezza dopo aver avuto evidenza di nuove prove. E&#8217; stato inoltre brevemente accennato che tali tecniche stanno diventando estremamente importanti nei campi della <a href="http://en.wikipedia.org/wiki/Data_science">data science</a> e della finanza quantitativa.</p><p>In questo articolo si vuole ampliare l&#8217;esempio del lancio di una moneta, descritto nel precedente articolo, approfondendo le nozioni relativa alle <a href="http://en.wikipedia.org/wiki/Bernoulli_trial">prove di Bernoulli</a>, alla <a href="http://en.wikipedia.org/wiki/Beta_distribution">distribuzione beta</a> e alla <a href="http://en.wikipedia.org/wiki/Conjugate_prior">distribuzione a priori coniugata</a>.</p><p>L&#8217;obiettivo in questo articolo consiste nel descrivere come eseguire quello che è noto come &#8220;inferenza su una proporzione binomiale&#8221;, cioè studiare le situazioni probabilistiche con due risultati (ad esempio il lancio di una moneta) e tentare di stimare la proporzione di un insieme ripetuto di eventi che si presentano un risultato &#8220;testa&#8221; o &#8220;croce&#8221;.</p><p><strong>L&#8217;obiettivo è stimare quanto sia equa una moneta</strong>. Si usa questa stima per fare <em>previsioni</em> su quante volte uscirà &#8220;testa&#8221; nei futuri lanci della moneta.<br />Anche se questo può sembrare un esempio piuttosto accademico, in realtà è sostanzialmente applicabile alle applicazioni del mondo reale più di quanto possa sembrare. Si considera i seguenti scenari:</p><ul><li><strong>Ingegneria</strong>: stima della proporzione delle pale delle turbine degli aeromobili che presentano un difetto strutturale dopo la fabbricazione;</li><li><strong>Scienze sociali</strong>: stima della percentuale di individui che rispondono &#8220;sì&#8221; a una domanda di censimento;</li><li><strong>Scienza medica</strong>: stima della percentuale dei pazienti che recuperano completamente dopo aver assunto un farmaco sperimentale per curare una malattia;</li><li><strong>Finanza aziendale</strong>: stima della percentuale di transazioni errate durante lo svolgimento di audit finanziari</li><li><strong>Data Science</strong>: stima della percentuale di individui che fanno clic su un annuncio quando visitano un sito Web.</li></ul><p> </p><p>Come si può vedere, l&#8217;inferenza su una proporzione binomiale è una tecnica statistica estremamente importante e costituisce la base di molti degli aspetti avanzati delle statistiche bayesiane.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-7c657e5 elementor-widget elementor-widget-heading" data-id="7c657e5" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">L'approccio Bayesiano</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-4667109 elementor-widget elementor-widget-text-editor" data-id="4667109" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Dopo aver motivato il concetto di statistiche bayesiane nel precedente articolo, è necessario delineare come procedere l&#8217;analisi. Questo motiva le seguenti sezioni (matematicamente piuttosto pesanti) e offre una panoramica su cosa sia un approccio bayesiano.</p><p>Come detto sopra, l&#8217;obiettivo consiste nel stimare l&#8217;equità di una moneta. Una volta ottenuta la stima per l&#8217;equità della moneta, si può usarla per prevedere il numero di volte che uscirà &#8220;testa&#8221; nei futuri lanci della moneta.</p><p>Si approfondisce specifiche tecniche specifiche mentre si descrive i seguenti passaggi:</p><ul><li><strong>Presupposti</strong> &#8211; Si ipotizza che la moneta abbia due esiti (cioè non atterrerà mai su di un lato), i lanci sono completamente casuali ed indipendenti l&#8217;uno dall&#8217;altro. Inoltre la correttezza della moneta è stazionaria, cioè non cambia nel tempo. Si indica l&#8217;equità con il parametro θ.</li><li><strong>Credenze precedenti</strong> &#8211; Per effettuare un&#8217;analisi bayesiana, si deve  quantificare le precedenti convinzioni sull&#8217;equità della moneta. Questo si riduce a specificare una distribuzione di probabilità sulle convinzioni di questa equità. Si utilizza una distribuzione di probabilità relativamente flessibile chiamata distribuzione beta per modellare le convinzioni.</li><li><strong>Dati sperimentali</strong> &#8211; Si effettuano alcuni lanci di monete (virtuali) al fine di avere alcuni dati concreti. Si conteggia il numero di teste <em>z</em> che appaiono in <em>N</em> lanci della moneta. Si ha anche bisogno di un modo per determinare la probabilità di tali risultati, data una particolare correttezza, θ, della moneta. Per questo si ha bisogno di discutere le <strong>funzioni di verosimiglianza</strong>, e in particolare la funzione di <strong>verosimiglianza di Bernoulli</strong>.</li><li><strong>Credenze a posteriori</strong> &#8211; Una volta ottenuta una credenza precedente e una funzione di verosimiglianza, si può usare la regola di Bayes per calcolare una <em>credenza a posteriori</em> sull&#8217;equità della moneta. Si associa le precedenti credenze con i dati osservati e si aggiorna di conseguenza le convinzioni. Fortunatamente, se si usa una distribuzione beta per le credenze precedenti e una probabilità di Bernoulli, si ottene una distribuzione beta anche a posteriori. Questi sono noti come <em>conjugate priors</em>.</li><li><strong>Inferenza</strong> &#8211; Dopo aver ottenuto le credenze a posteriori, si può stimare l&#8217;equità della moneta θ, prevedere la probabilità delle teste al prossimo lancio o persino vedere come i risultati dipendono da diverse scelte delle credenze precedenti. Quest&#8217;ultimo è noto come <em>confronto tra modelli</em>.</li></ul><p><br />Ad ogni fase del processo, si realizza rappresentazioni grafiche per ciascuna di queste funzioni e distribuzioni, utilizzando il <a href="http://stanford.edu/~mwaskom/software/seaborn/">pacchetto di grafica Seaborn</a> di Python. Seaborn si basa sulla libreria <a href="http://matplotlib.org/">Matplotlib</a>, ed è ottimizzata per la visualizzazione di dati statistici. Si può dare un&#8217;occhiata a questa <a href="http://stanford.edu/~mwaskom/software/seaborn/examples/index.html">gallery</a> per avere un&#8217;idea sul funzionamento e potenzialità di Seaborn.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-328ec4f elementor-widget elementor-widget-heading" data-id="328ec4f" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">
Presupposti dell'approccio</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-f762a20 elementor-widget elementor-widget-text-editor" data-id="f762a20" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Come per tutti i modelli, si devono fare alcune ipotesi sullo scenario di applicazione.</p><ul><li>Si può ipotizzare la moneta possa avere solo due risultati, cioè può atterrare solo sulla sua testa o coda e mai sul suo lato.</li><li>Ogni lancio della moneta è completamente indipendente dagli altri, cioè si hanno <a href="http://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables">lanci indipendenti e identicamente distribuiti</a>.</li><li>La correttezza della moneta non cambia nel tempo, cioè è <a href="http://en.wikipedia.org/wiki/Stationary_process">stazionaria</a>.</li></ul><p><br />Tenendo presente questi presupposti, si può iniziare a discutere la procedura bayesiana.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-abd818d elementor-widget elementor-widget-heading" data-id="abd818d" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">La Regola di Bayes</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-2c37a6e elementor-widget elementor-widget-text-editor" data-id="2c37a6e" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Nel precedente articolo è stata delineata la regola di Bayes come segue:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-83403a7 elementor-widget elementor-widget-heading" data-id="83403a7" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">La Regola di Bayes applicata all'Inferenza Bayesiana</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-6197fb3 elementor-widget elementor-widget-text-editor" data-id="6197fb3" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p style="text-align: center;">\(
\begin{eqnarray}
P(\theta|D) = P(D|\theta) \; P(\theta) \; / \; P(D)
\end{eqnarray}
\)</p>
dove:
<ul>
 	<li>P(θ) è la probabilità <strong>a priori</strong>, o probabilità marginale, di θ. Questa è la forza nella nostra convinzione su θ senza considerare le prove D. La nostra precedente opinione sulla probabilità che la moneta sia equa e non truccata.</li>
 	<li>P(θ|D) è la probabilità <strong>a posteriori</strong>, o probabilità condizionata. Questa è la forza (raffinata) della nostra convinzione di θ una volta che l&#8217;evidenza di D è stata presa in considerazione. Dopo aver visto uscire 4 teste su 8 lanci della moneta, si può dire che questa è la nostra visione aggiornata sulla correttezza della moneta.</li>
 	<li>P(D|θ) è la probabilità condizionata di D. Questa è la probabilità di vedere i dati D come generati da un modello con parametro θ. Se si potesse sapere che la moneta era equa e non truccata, la probabilità condizionata di D esprime la probabilità di avere un determinato numero di risultati &#8220;testa&#8221; per un particolare numero di lanci.</li>
 	<li>P(D) è la<strong> prova</strong>, cioè la probabilità a priori di D, e funge da costante di normalizzazione. Questa è la probabilità dei dati D, determinata come somma (o integrazione) di tutti i possibili valori di θ, ponderati con un indice del grado di confidenza con cui si crede alla validità di quei particolari valori di θ.</li>
</ul>
Da notare che per calcolare la probabilità a posteriori si devono specificare tre distinti componenti. Sono la <em>verosimiglianza</em>, la <em>probabilità a priori</em> e l&#8217;<em>evidenza</em>. Nelle sezioni seguenti si descrive esattamente come specificare ciascuno di questi componenti per un particolare caso di inferenza su una proporzione binomiale.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-aee6927 elementor-widget elementor-widget-heading" data-id="aee6927" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">La funzione di Verosimiglianza</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-d56415b elementor-widget elementor-widget-heading" data-id="d56415b" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">La Distribuzione di Bernoulli</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-82b759e elementor-widget elementor-widget-text-editor" data-id="82b759e" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Si consideri una sequenza di lanci di una moneta. Si è interessati alla probabilità che esca una &#8220;​​testa&#8221;. In particolare, si vuole la probabilità che esca una &#8220;testa&#8221; in funzione del relativo parametro di equità θ.

Questo avrà una forma funzionale, f. Se si indica con k la variabile casuale che descrive il risultato del lancio della moneta, appartenente all&#8217;insieme {1,0}, dove k = 1 rappresenta una testa e k = 0 rappresenta una croce, allora la probabilità di avere una testa, con una certa equità della moneta, è data da:
<p style="text-align: center;">\(
\begin{eqnarray}
P(k = 1 | \theta) = f(\theta)
\end{eqnarray}
\)</p>
Si può scegliere una forma particolarmente succinta per f(θ) semplicemente affermando che la probabilità è data dallo stesso θ, cioè \(f(\theta) = \theta\). Da questo si deduce che la probabilità di avere una &#8220;testa&#8221; dal lancio di una moneta è pari a:
<p style="text-align: center;">
\(\begin{eqnarray}
P(k = 1 | \theta) = \theta
\end{eqnarray}
\)</p>
E la probabilità di avere una &#8220;croce&#8221; è pari a:
<p style="text-align: center;">\(
\begin{eqnarray}
P(k = 0 | \theta) = 1-\theta
\end{eqnarray}
\)</p>
che può anche essere descritto come:
<p style="text-align: center;">\(\begin{eqnarray}
P(k | \theta) = \theta^k (1 &#8211; \theta)^{1-k}
\end{eqnarray}
\)</p>
dove \(k \in \{1, 0\}\) e \(\theta \in [0,1]\).

Questa è nota come <strong>Distribuzione di Bernoulli</strong>. Fornisce la probabilità su due valori discreti separati di k per un parametro di equità fisso θ.

In sostanza, quantifica la probabilità che esca &#8220;testa&#8221; o &#8220;croce&#8221; a seconda di quanto sia equa la moneta.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-ab51ffd elementor-widget elementor-widget-heading" data-id="ab51ffd" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">La funzione di Verosimiglianza di Bernoulli</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-8320912 elementor-widget elementor-widget-text-editor" data-id="8320912" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Si può considerare un altro modo per osservare la funzione di cui sopra. Se si considera un&#8217;osservazione fissa, cioè un risultato noto, k, del lancio di una moneta e il parametro di equità θ come variabile continua, allora:
<p style="text-align: center;">\(
\begin{eqnarray}
P(k | \theta) = \theta^k (1 &#8211; \theta)^{1-k}
\end{eqnarray}
\)</p>
descrive la probabilità di un risultato fisso k dato un particolare valore di θ. Mentre si modifica θ (ad esempio cambiando l&#8217;equità della moneta), si inizia a vedere diverse probabilità per k.

Questo è noto come la <strong>funzione di verosimiglianza</strong> di θ. È una funzione di un θ <em>continuo</em> e si distingue dalla distribuzione di Bernoulli perché quest&#8217;ultima è in realtà una distribuzione di probabilità discreta su due potenziali risultati k del lancio della moneta.

Da notare che la funzione di verosimiglianza non è in realtà una distribuzione di probabilità poiché l&#8217;integrazione di tutti i valori del parametro di equità θ in realtà non è uguale a 1, come richiesto per una distribuzione di probabilità.

Si definisce \(P(k | \theta) = \theta^k (1 &#8211; \theta)^{1-k}\) come la <strong>funzione di verosimiglianza di Bernoulli</strong> per θ.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-cc40b05 elementor-widget elementor-widget-heading" data-id="cc40b05" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">Lanci Multipli della moneta</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-a8f77d3 elementor-widget elementor-widget-text-editor" data-id="a8f77d3" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				La funzione di verosimiglianza di Bernoulli può essere usata per determinare la probabilità di vedere una particolare sequenza di N lanci, data dall&#8217;insieme \(\{k_1, &#8230;, k_N\}\).

Poiché ciascuno di questi lanci è indipendente da qualsiasi altro, la probabilità che la sequenza si verifichi è semplicemente il prodotto della probabilità di ogni lancio.

Se si considera uno specifico parametro di equità θ, allora la probabilità di vedere questo particolare insieme di lanci è data da:
<p style="text-align: center;">\(
\begin{eqnarray}
P(\{k_1, &#8230;, k_N\} | \theta) &amp;=&amp; \prod_{i} P(k_i | \theta) \\
&amp;=&amp; \prod_{i} \theta^{k_i} (1 &#8211; \theta)^{1-{k_i}}
\end{eqnarray}
\)</p>
Per esempio, se si è interessati al numero di &#8220;teste&#8221; uscite in N lanciChe dire se siamo interessati al numero di teste, dato <em>z</em> il numero di teste uscite, allora la formula sopra diventa:
<p style="text-align: center;">\(
\begin{eqnarray}
P(z, N | \theta) = \theta^z (1 &#8211; \theta)^{N-z}
\end{eqnarray}
\)</p>
Cioè, la probabilità di avere z &#8220;teste&#8221; in N lanci, assumendo un parametro di equità θ. Si userà questa formula per determinare la distribuzione delle credenze posteriori, come descritto più avanti in questo articolo.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-0a4be6f elementor-widget elementor-widget-heading" data-id="0a4be6f" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">Quantificare le Credenze a Priori</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-b2fb3d7 elementor-widget elementor-widget-text-editor" data-id="b2fb3d7" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Un passo estremamente importante nell&#8217;approccio bayesiano è determinare le probabilità a priori delle credenze di partenza e quindi trovare un modo per quantificarle.

Nell&#8217;approccio bayesiano bisogna determinare le credenze precedenti sui parametri e quindi trovare una distribuzione di probabilità che quantifichi queste convinzioni.
In questo caso si è interessati alle precedenti convinzioni sulla correttezza della moneta, cioè si desidera quantificare l&#8217;incertezza relativa alla possibilità che la moneta sia truccata.

Per fare ciò bisogna capire l&#8217;intervallo di valori che θ può assumere e la probabilità (ipotetica) del verificarsi di ognuno di questi valori.

\(\theta=0\) indica una moneta per cui esce sempre corce, mentre \(\theta=1\) implica una moneta per cui esce sempre testa. Una moneta equa è denotata da \(\theta=0,5\). Quindi \(\theta \in [0,1]\). Questo implica che la nostra distribuzione di probabilità deve esistere nell&#8217;intervallo [0,1].

La domanda diventa: quale distribuzione di probabilità usare per quantificare le credenze a priori sulla moneta?					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-5fb84de elementor-widget elementor-widget-heading" data-id="5fb84de" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">Distribuzione Beta</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-9cc2c5d elementor-widget elementor-widget-text-editor" data-id="9cc2c5d" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				In questo caso si sceglie la <strong>distribuzione beta</strong>. La funzione di densità di probabilità della distribuzione beta corrisponde a:
<p style="text-align: center;">\(
\begin{eqnarray}
P(\theta | \alpha, \beta) = \theta^{\alpha &#8211; 1} (1 &#8211; \theta)^{\beta &#8211; 1} / B(\alpha, \beta)
\end{eqnarray}
\)</p>
Dove il termine nel denominatore, \(B(\alpha, \beta)\), agisce come una costante normalizzante in modo che l&#8217;area sotto il PDF abbia in realtà somma pari a 1.

Nel grafico seguente si rappresentano diverse distribuzione beta a seconda dei parametri α e β:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8a17eb0 elementor-widget elementor-widget-image" data-id="8a17eb0" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
									<figure class="wp-caption">
										<img width="768" height="583" src="https://datatrading.info/wp-content/uploads/2017/10/beta-distributione-trading-algoritmico-768x583.png" class="attachment-medium_large size-medium_large" alt="" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/2017/10/beta-distributione-trading-algoritmico-768x583.png 768w, https://datatrading.info/wp-content/uploads/2017/10/beta-distributione-trading-algoritmico-160x120.png 160w, https://datatrading.info/wp-content/uploads/2017/10/beta-distributione-trading-algoritmico.png 1023w" sizes="(max-width: 768px) 100vw, 768px" />											<figcaption class="widget-image-caption wp-caption-text">Fig -  Rappresentazione di diverse della distribuzione beta per vari parametri α e β.</figcaption>
										</figure>
								</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-dddfb9b elementor-widget elementor-widget-text-editor" data-id="dddfb9b" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Il codice Python per produrre questo grafico è il seguente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6c1b60c elementor-widget elementor-widget-bdt-source-code" data-id="6c1b60c" data-element_type="widget" data-widget_type="bdt-source-code.default">
				<div class="elementor-widget-container">
			
        <div class="bdt-source-code prism-okaidia">

                            <button class="bdt-copy-button">Copy                </button>
            
            <pre class="language-py">
            <code>import numpy as np
from scipy.stats import beta
import matplotlib.pyplot as plt
import seaborn as sns


if __name__ == &quot;__main__&quot;:
    sns.set_palette(&quot;deep&quot;, desat=.6)
    sns.set_context(rc={&quot;figure.figsize&quot;: (8, 4)})
    x = np.linspace(0, 1, 100)
    params = [
        (0.5, 0.5),
        (1, 1),
        (4, 3),
        (2, 5),
        (6, 6)
    ]
    for p in params:
        y = beta.pdf(x, p[0], p[1])
        plt.plot(x, y, label=&quot;$\\alpha=%s$, $\\beta=%s$&quot; % p)
    plt.xlabel(&quot;$\\theta$, Fairness&quot;)
    plt.ylabel(&quot;Density&quot;)
    plt.legend(title=&quot;Parameters&quot;)
    plt.show()</code>
        </pre>

        </div>
		</div>
				</div>
				<div class="elementor-element elementor-element-74fc7fe elementor-widget elementor-widget-text-editor" data-id="74fc7fe" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Essenzialmente, quando α diventa più grande, la maggior parte della distribuzione di probabilità si muove verso destra (una moneta truccata per avere più &#8220;teste&#8221;), mentre un aumento di β sposta la distribuzione verso sinistra (una moneta truccata per avere più &#8220;croci&#8221;) .</p><p>Tuttavia, se si aumenta sia α che β, la distribuzione inizia a restringersi. Se α e β aumentano ugualmente, allora la distribuzione raggiungerà un picco per θ = 0,5, cioè quando la moneta è equa.</p><p>Perché si è scelto la funzione beta come probabilità a priori? Ci sono un paio di motivi:</p><ul><li><strong>Supporto</strong> &#8211; La funzione è definito nell&#8217;intervallo [0,1], lo stesso intervallo di esistenza di θ.</li><li><strong>Flessibilità</strong> &#8211; Possiede due parametri di forma conosciuti come α e β, che gli conferiscono una notevole flessibilità. Questa flessibilità offre una vasta scelta delle modalità per modelliamo le credenze.</li></ul><p>Tuttavia, la ragione più importante per scegliere una distribuzione beta è dovuta al fatto che questa è una <strong>distribuzione a priori congiunta</strong> (conjugate prior) per la distribuzione di Bernoulli.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c4d3e28 elementor-widget elementor-widget-heading" data-id="c4d3e28" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">Distribuzione a Priori Coniugata</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-740ab2f elementor-widget elementor-widget-text-editor" data-id="740ab2f" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Nella regola di Bayes, riportata sopra, è evidente che la distribuzione a posteriori è proporzionale al prodotto della distribuzione precedente e alla funzione di verosimiglianza:
<p style="text-align: center;">\(
\begin{eqnarray}
P(\theta | D) \propto P(D|\theta) P(\theta)
\end{eqnarray}
\)</p>
Una distribuzione a priori coniugata è una possibile scelta per descrivere la probabilità a priori perché, quando accoppiata con uno specifico tipo di funzione di verosimiglianza, fornisce una distribuzione a posteriori appartenente alla stessa famiglia della distribuzione a priori.

Sia a priori che sia a posteriori hanno la stessa famiglia di distribuzione di probabilità, ma con parametri diversi.

I conjugate prior sono estremamente convenienti dal punto di vista del calcolo poiché forniscono espressioni di forma chiusa per la distribuzione a posteriori, annullando in tal modo qualsiasi complessa integrazione numerica.

Per il nostro esempio, se si usa una funzione di verosimiglianza di Bernoulli e una distribuzione beta come descrivere la probabilità a priori si ottiene immediatamente che la probabilità a posteriori sarà anch&#8217;essa una distribuzione beta.

L&#8217;utilizzo di una distribuzione beta per la probabilità a priori implica la possibilità di effettuare lanci di monete sperimentali e perfezionare direttamente le convinzioni. La probabilità a posteriori diventerà la nuova probabilità a priori ed è possibile usare la regola di Bayes in successione quando vengono generati nuovi lanci di monete.

Se la precedente credenza è specificata da una distribuzione beta e si ha una funzione di verosimiglianza di Bernoulli, allora anche la probabilità a posteriori sarà anche una distribuzione beta.
Da notare tuttavia che un priore è solo coniugato rispetto a una particolare funzione di probabilità.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-aae6027 elementor-widget elementor-widget-heading" data-id="aae6027" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">Perché coniugare una Distribuzione Beta con una funzione di Verosimiglianza di Bernoulli?</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-a6a281b elementor-widget elementor-widget-text-editor" data-id="a6a281b" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Si può utilizzare un semplice calcolo per dimostrare che scegliere la distribuzione beta per la probabilità a priori, con una verosimiglianza di Bernoulli, fornisce una distribuzione beta per la probabilità a posteriori.

Come accennato in precedenza, la funzione di densità di probabilità di una distribuzione beta, per uno specifico parametro θ, è data da:
<p style="text-align: center;">\(
\begin{eqnarray}
P(\theta | \alpha, \beta) = \theta^{\alpha &#8211; 1} (1 &#8211; \theta)^{\beta &#8211; 1} / B(\alpha, \beta)
\end{eqnarray}
\)</p>
Da notare che la forma della distribuzione beta è simile alla forma di una verosimiglianza di Bernoulli. Infatti, se si moltiplicano le due forme (come nella regola di Bayes), si ottiene:
<p style="text-align: center;">\(
\begin{eqnarray}
\theta^{\alpha &#8211; 1} (1 &#8211; \theta)^{\beta &#8211; 1} / B(\alpha, \beta) \times \theta^{k} (1 &#8211; \theta)^{1-k} \propto \theta^{\alpha + k &#8211; 1} (1 &#8211; \theta)^{\beta + k}
\end{eqnarray}
\)</p>
Il termine sul lato destro del segno di proporzionalità ha la stessa forma del prior (a meno di una costante normalizzante).					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-67983b0 elementor-widget elementor-widget-heading" data-id="67983b0" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">Differenti modi per specificare una Distribuzione Beta</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-8398031 elementor-widget elementor-widget-text-editor" data-id="8398031" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Finora è stato descritto come utilizzare una distribuzione beta per specificare le convinzioni precedenti relative alla correttezza della moneta. Tuttavia, si hanno solo due parametri con cui giocare, vale a dire α e β.

In che modo questi due parametri corrispondono alla significato più intuitivo di &#8220;verosimiglianza&#8221; e &#8220;incertezza&#8221; nella corretta della moneta?

Bene, questi due concetti corrispondono perfettamente alla <em>media</em> e alla <em>varianza</em> della distribuzione beta. Quindi, se si trova una relazione tra questi due valori e i parametri α e β, si può più facilmente specificare le nostre convinzioni.

Si scopre (vedi l&#8217;articolo di <a href="http://en.wikipedia.org/wiki/Beta_distribution">Wikipedia sulla distribuzione beta</a>) che la media μ è data da:
<p style="text-align: center;">\(
\begin{eqnarray}
\mu = \frac{\alpha}{\alpha + \beta}
\end{eqnarray}
\)</p>

Mentre la deviazione standard σ è data da:
<p style="text-align: center;">\(
\begin{eqnarray}
\sigma = \sqrt{\frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)}}
\end{eqnarray}
\)</p>

Quindi, riorganizzando queste formule per fornire α e β in termini di μ e σ. α si ottiene:
<p style="text-align: center;">\(
\begin{eqnarray}
\alpha = \left( \frac{1-\mu}{\sigma^2} &#8211; \frac{1}{\mu} \right) \mu^2
\end{eqnarray}
\)</p>

Mentre β è dato da:
<p style="text-align: center;">\(
\begin{eqnarray}
\beta = \alpha \left( \frac{1}{\mu} &#8211; 1 \right)
\end{eqnarray}
\)</p>

In questo passaggio è necessario prestare molta attenzione perché non è possibile specificare un σ > 0,289, dato che questa è la deviazione standard di una densità uniforme (che implica nessuna credenza precedente su ogni specifica equità della moneta).

Ad esempio, se si ipotizza una equità della moneta intorno allo 0,5, ma senza l&#8217;assoluta certezza. Si può specificare una deviazione standard di circa 0,1. Quale distribuzione beta sarà prodotta come risultato?

Inserendo i numeri nelle formule si ottine α = 12 e β = 12 e la distribuzione beta è simile alla seguente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-4a8ee29 elementor-widget elementor-widget-image" data-id="4a8ee29" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
									<figure class="wp-caption">
										<img width="768" height="464" src="https://datatrading.info/wp-content/uploads/2017/10/beta-distribution-12-12-inferenza-bayesiana-trading-algoritmico-768x464.png" class="attachment-medium_large size-medium_large" alt="" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/2017/10/beta-distribution-12-12-inferenza-bayesiana-trading-algoritmico-768x464.png 768w, https://datatrading.info/wp-content/uploads/2017/10/beta-distribution-12-12-inferenza-bayesiana-trading-algoritmico.png 1313w" sizes="(max-width: 768px) 100vw, 768px" />											<figcaption class="widget-image-caption wp-caption-text">Fig - Una distribuzione Beta con α=12 e β=12.</figcaption>
										</figure>
								</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-ae10834 elementor-widget elementor-widget-text-editor" data-id="ae10834" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Da notare come, nonostante il picco sia centrato attorno allo 0,5, è presente una significativa incertezza in questa credenza che è rappresentata dalla larghezza della curva.					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/inferenza-bayesiana-di-una-proporzione-binomiale-lapproccio-analitico/">Inferenza Bayesiana di una proporzione Binomiale &#8211; L&#8217;approccio Analitico</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Introduzione alla Statistica Bayesiana</title>
		<link>https://datatrading.info/introduzione-alla-statistica-bayesiana/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Mon, 04 Sep 2017 11:05:18 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Statistica Bayesiana]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=969</guid>

					<description><![CDATA[<p>Negli ultimi mesi abbiamo descritto su DataTrading molti aspetti del trading quantitativo quali i modelli di machine learning, l&#8217;analisi delle serie temporali e l&#8217;implementazione di un&#8217;ambiente di backtesting. A questo punto è utile introdurre le moderne tecniche matematiche che sono alla base non solo della finanza quantitativa e del trading algoritmico, ma anche dei campi &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/introduzione-alla-statistica-bayesiana/"> <span class="screen-reader-text">Introduzione alla Statistica Bayesiana</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/introduzione-alla-statistica-bayesiana/">Introduzione alla Statistica Bayesiana</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="969" class="elementor elementor-969">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-37cc890 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="37cc890" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-fcb73a4" data-id="fcb73a4" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-f9cbf66 elementor-widget elementor-widget-text-editor" data-id="f9cbf66" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Negli ultimi mesi abbiamo descritto su DataTrading molti aspetti del trading quantitativo quali i modelli di machine learning, l&#8217;analisi delle serie temporali e l&#8217;implementazione di un&#8217;ambiente di backtesting. A questo punto è utile introdurre le moderne tecniche matematiche che sono alla base non solo della finanza quantitativa e del trading algoritmico, ma anche dei campi emergenti della <strong>scienza dei dati</strong> e dell&#8217;<strong>apprendimento statistico delle macchine</strong>.</p><p>Le competenze quantitative sono ora molto richieste non solo nel settore finanziario, ma anche nelle startup tecnologiche, e per l&#8217;analisi dati delle grandi aziende.</p><p>Prima di iniziare a discutere le moderne tecniche di &#8220;bleeding edge&#8221;, è necessario avere una solida comprensione delle sottostanti teorie matematiche e statistiche, che sono alla base di questi modelli. Una tecnica fondamentale è quella delle <strong>statistiche bayesiane</strong>. Fino ad oggi in questo sito non è stato pubblicato nessun articolo relativo ai metodi bayesiani. Questo articolo è stato scritto per aiutare a capire la &#8220;filosofia&#8221; dell&#8217;approccio bayesiano, come si confronta con l&#8217;approccio tradizionale / classico alle statistiche e le potenziali applicazioni nella finanza quantitativa e scienza dei dati.</p><p>In particole in questo articolo si:</p><ul><li>Definisce le statistiche bayesiane (o inferenza bayesiana)</li><li>Confronta le statistiche classiche con le statistiche bayesiane</li><li>Descrive la famosa regola di Bayes, uno strumento essenziale per l&#8217;inferenza bayesiana</li><li>Interpreta e applica la regola di Bayes per eseguire l&#8217;inferenza bayesiana</li><li>Descrive un concreto esempio di coin-flip probabilistico di inferenza bayesiana</li></ul>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-0db61d0 elementor-widget elementor-widget-heading" data-id="0db61d0" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">Che cos'è la Statistica Bayesiana?</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-03cb706 elementor-widget elementor-widget-text-editor" data-id="03cb706" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Le statistiche bayesiane rappresentano un particolare <strong>approccio all&#8217;applicazione della probabilità ai problemi statistici</strong>. Fornisce gli strumenti matematici per <em>avere evidenza su stati o eventi casuali in termini di gradi di credibilità</em> o più specificamente di probabilità bayesiana</p><p>In particolare<em> l&#8217;inferenza bayesiana</em> interpreta la probabilità come una misura della credibilità o della fiducia che un individuo può avere sul reale verificarsi di un particolare evento.</p><p>Si può avere una precedente convinzione su un evento, ma è probabile che le convinzioni cambino quando nuove prove vengono portate alla luce. Le statistiche bayesiane forniscono un solido mezzo matematico per incorporare le precedenti credenze e prove per produrre nuove credenze a posteriori.</p><p>Le statistiche bayesiane ci forniscono strumenti matematici per aggiornare razionalmente le nostre convinzioni soggettive alla luce di nuovi dati o prove.</p><p>Questo è in contrasto con un&#8217;altra forma di inferenza statistica, nota come statistica <em>classica</em> o <em>frequentista</em>, che presuppone che le probabilità siano la frequenza di particolari eventi casuali che si verificano in<em> una lunga serie di prove ripetute</em>.</p><p>Per esempio, mentre si lancia ripetutamente un dado equo a sei facce (cioè non pesato), si ha che che ogni numero sul dado tende ad uscire per circa un 1/6 volte.</p><p>La statistica frequentista presuppone che le probabilità siano la frequenza di lungo periodo di eventi casuali in prove ripetute.</p><p>Quando si effettuano inferenze statistiche, cioè inferendo informazioni statistiche da sistemi probabilistici, i due approcci &#8211; frequentista e bayesiano &#8211; hanno filosofie molto diverse.</p><p>La statistica frequentista cerca di eliminare l&#8217;incertezza fornendo stime. La statistica bayesiana cerca di preservare e perfezionare l&#8217;incertezza adeguando le credenze individuali alla luce di nuove prove.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-3707096 elementor-widget elementor-widget-heading" data-id="3707096" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">Frequentista (o Empirica) vs Bayesiana</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-5fe223b elementor-widget elementor-widget-text-editor" data-id="5fe223b" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Per chiarire la distinzione tra le due diverse filosofie statistiche, si considerano due esempi di sistemi probabilistici:</p><ul><li><strong>Coin flip</strong> &#8211; Qual è la probabilità che esca testa per una moneta truccata?</li><li><strong>Elezione di un particolare candidato per il Primo Ministro</strong> &#8211; Qual è la probabilità di vedere vincere un singolo candidato, che non è mai stato candidato in precedenza?</li></ul><p> </p><p>La seguente tabella descrive le filosofie alternative dell&#8217;approccio frequentista e bayesiano:</p><table><tbody><tr><td><strong>Esempio</strong></td><td><strong>Approccio Frequentista</strong></td><td><strong>Approccio Bayesiano</strong></td></tr><tr><td><strong>Lancio di una moneta truccata</strong></td><td>La probabilità di vedere il risultato &#8220;testa&#8221; nei lanci di una moneta truccata corrisponde alla frequenza dei risultati &#8220;testa&#8221; dopo un elevato numero di lanci indipendenti della moneta. Cioè, mentre si esegue più lanci di monete il numero di teste ottenute, come proporzione dei lanci totali, tende alla probabilità &#8220;vera&#8221; o &#8220;fisica&#8221; che il lancio della della moneta produca una testa. In particolare, l&#8217;individuo che esegue l&#8217;esperimento non incorpora le proprie convinzioni sulla correttezza delle altre monete.</td><td>Prima di ogni lancio della moneta, un individuo può considerare che la moneta non sia truccata. Dopo pochi lanci la moneta esce continuamente teste. Quindi la precedente convinzione sulla correttezza della moneta viene modificata per tenere conto del fatto che sono uscite tre teste di fila e quindi la moneta potrebbe essere truccata. Dopo 500 lanci, con 400 teste, l&#8217;individuo crede che la moneta sia molto probabile truccata. La credenza a posteriori è fortemente modificata rispetto alle ipotesi di partenza.</td></tr><tr><td><strong>Elezioni del canditato</strong></td><td>Il candidato si candida per la prima volta per questa particolare elezione e quindi non si possono eseguire &#8220;prove ripetute&#8221;. In un contesto frequentista si costruisco prove &#8220;virtuali&#8221; del processo elettorale. La probabilità di vittoria del candidato è definita come la frequenza relativa di vittorie del candidato nelle prove &#8220;virtuali&#8221;, come frazione di tutte le prove.</td><td>Un individuo ha una precedente convinzione sulle possibilità di un candidato di vincere un&#8217;elezione e la sua fiducia può essere quantificata come probabilità. Tuttavia, un altro individuo potrebbe anche avere una credenza anteriore diversa e quindi una diversa sulla probabilità dello stesso candidato. Quando arrivano nuovi dati, entrambe le convinzioni sono (razionalmente) aggiornate dalla procedura bayesiana.</td></tr></tbody></table>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-1798440 elementor-widget elementor-widget-text-editor" data-id="1798440" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Quindi nell&#8217;interpretazione bayesiana la probabilità è <em>la sintesi dell&#8217;opinione di un individuo</em>. Un punto chiave è che diversi individui (intelligenti) possono avere opinioni diverse (e quindi diverse credenze), poiché hanno un diverso accesso ai dati e diverso modi di interpretarli. Tuttavia, poiché entrambi questi individui incontrano nuovi dati a cui entrambi hanno accesso, le loro credenze precedenti (potenzialmente diverse) porteranno a credenze a posteriori che cominceranno a convergere l&#8217;una verso l&#8217;altra, sotto la procedura di aggiornamento razionale dell&#8217;inferenza bayesiana.

Nel contesto bayesiano un individuo applica una probabilità pari a 0 quando non ha fiducia nel verificarsi di un evento, mentre applica una probabilità pari a 1 quando è assolutamente certo del verificarsi di un evento. Se assegnano una probabilità tra 0 e 1, si consente una ponderata confidenza con altri potenziali risultati.

Per eseguire l&#8217;<em>inferenza bayesiana</em>, si deve  utilizzare un famoso teorema di probabilità noto come <strong>regola di Bayes</strong> e interpretarlo nel modo corretto. Nel riquadro seguente, si deriva la regola di Bayes usando la definizione di probabilità condizionale. Tuttavia, non è essenziale seguire questo procedimento per usare i metodi bayesiani, quindi sentitevi liberi di saltare la prossima sezione se si vuol andare direttamente alla descrizione su come usare la regola di Bayes.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c0a0db1 elementor-widget elementor-widget-heading" data-id="c0a0db1" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">La Regola di Bayes</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-c8eda8a elementor-widget elementor-widget-text-editor" data-id="c8eda8a" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Si consideri la definizione di probabilità condizionale, che fornisce una regola per determinare la probabilità di un evento A, data l&#8217;esistenza di un altro evento B. Un esempio potrebbe essere &#8220;Qual è la probabilità che piova quando ci sono nuvole nel cielo &#8221;

La definizione matematica della probabilità condizionale è la seguente:
<p style="text-align: center;">\(
\begin{eqnarray}
P(A|B) = \frac{P(A \cap B)}{P(B)}
\end{eqnarray}
\)</p>
Questo formula afferma che la probabilità che si verifichi A, sapendo che si è verificato B, cioè esprime una &#8220;correzione&#8221; delle aspettative per A dettata dall&#8217;osservazione di B, corrisponde alla la probabilità congiunta dei due eventi, ovvero la probabilità che si verifichino entrambi, in rapporto alla probabilità di B.

Nell&#8217;esempio precedente significa la probabilità che piova dato che ci sono le nuvole è uguale alla probabilità che la pioggia e la presenza di nuvole si verificano insieme, in relazione alla probabilità di avere nuvole in cielo.

Se si moltiplica entrambi i fattori di questa equazione per P(B) si ottiene:
<p style="text-align: center;">\(
\begin{eqnarray}
P(B) P(A|B) = P(A \cap B)
\end{eqnarray}
\)</p>
Ma, si può semplicemente fare lo stesso ragionamento per P(B|A), che è simile a chiedere &#8220;Qual è la probabilità di vedere le nuvole, dato che sta piovendo?&#8221;:
<p style="text-align: center;">\(
\begin{eqnarray}
P(B|A) = \frac{P(B \cap A)}{P(A)}
\end{eqnarray}
\)</p>
Dato che P(A∩B)=P(B∩A), sostituendo il precedente e moltiplicando per P(A), si ottiene:
<p style="text-align: center;">\(
\begin{eqnarray}
P(A) P(B|A) = P(A \cap B)
\end{eqnarray}
\)</p>
Ora si può confrontare tra loro le due espressioni per P(A∩B):
<p style="text-align: center;">\(
\begin{eqnarray}
P(B) P(A|B) = P(A) P(B|A)
\end{eqnarray}
\)</p>
Se ora si divide entrambi i lati per P(B) si ottiene la famosa regola di Bayes:
<p style="text-align: center;">\(
\begin{eqnarray}
P(A|B) = \frac{P(B|A) P(A)}{P(B)}
\end{eqnarray}
\)</p>
Tuttavia, per il successivo utilizzo della regola di Bayes è utile scrivere il denominare, P(B) in termini di P(B|A). Si può considerare P(B) come:
<p style="text-align: center;">\(
\begin{eqnarray}
P(B) = \sum_{a \in A} P(B \cap A)
\end{eqnarray}
\)</p>
Questo è possibile perché gli eventi A sono una partizione sufficiente ampio dello spazio campione.

In questo modo sostituendo la definizione della probabilità condizionata si ottiene:
<p style="text-align: center;">\(
\begin{eqnarray}
P(B) = \sum_{a \in A} P(B \cap A) = \sum_{a \in A} P(B|A) P(A)
\end{eqnarray}
\)</p>
Infine, si può sostituire P(B) nella regola di Bayes e si ottiene una versione alternativa della stessa regola di Bayes, che è ampiamente usata nell&#8217;inferenza bayesiana:
<p style="text-align: center;">\(
\begin{eqnarray}
P(A|B) = \frac{P(B|A) P(A)}{\sum_{a \in A} P(B|A) P(A)}
\end{eqnarray}
\)</p>
Ora che si è definita la regola di Bayes, si è in grado di applicarla all&#8217;inferenza statistica.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-16cd678 elementor-widget elementor-widget-heading" data-id="16cd678" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">Applicazione della Regola di Bayes per l'Inferenza Bayesiana</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-95f0a88 elementor-widget elementor-widget-text-editor" data-id="95f0a88" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Come affermato all&#8217;inizio di questo articolo, l&#8217;idea di base dell&#8217;<strong>inferenza bayesiana</strong> consiste nell&#8217;aggiornare continuamente le convinzioni precedenti sugli eventi quando vengono presentate nuove prove. Questo è un modo molto naturale per studiare gli eventi probabilistici. Man mano che si accumulano sempre più prove, le precedenti convinzioni sono costantemente &#8220;sbiadite&#8221; da qualsiasi nuovo dato.

Si consideri l&#8217;evento (piuttosto privo di senso) che la Luna sta andando a scontrarsi con la Terra. Per ogni notte che passa, l&#8217;applicazione dell&#8217;inferenza bayesiana tenderà a correggere la nostra precedente credenza in una credenza a posteriori secondo cui la Luna ha sempre meno probabilità di entrare in collisione con la Terra, poiché rimane in orbita.

Per dimostrare un concreto esempio numerico di inferenza bayesiana è necessario introdurre qualche nuova notazione.

In primo luogo, si deve considerare il concetto di parametri e modelli. Un parametro potrebbe essere l&#8217;osservazione della una moneta truccata, che si può etichettare come \(\theta\). Quindi \(\theta = P(H)\) descriverebbe la distribuzione di probabilità delle convinzioni che uscirà testa dopo il lancio della moneta. Il modello è il mezzo effettivo per codificare matematicamente questo lancio. In questo caso, il lancio della moneta può essere modellato come una prova di Bernoulli.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-0c356a7 elementor-widget elementor-widget-heading" data-id="0c356a7" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">La Prova di Bernoulli</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-3147de1 elementor-widget elementor-widget-text-editor" data-id="3147de1" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>La prova di Bernoulli è un esperimento casuale con solo due risultati, di solito etichettati come &#8220;successo&#8221; o &#8220;fallimento&#8221;, in cui la probabilità del successo è esattamente la stessa ogni volta che viene eseguita la sperimentazione. La probabilità del successo è data da \(\theta\), che è un numero compreso tra 0 e 1. Quindi \(\theta \in [0,1]\).</p><p>Nel corso dell&#8217;esperimento di alcuni esperimenti di lancio di una moneta (ripetute prove di Bernoulli) si generano alcuni dati, \(D\), relativamente ai risultati testa o croce.</p><p>Una naturale domanda da porsi è &#8220;Qual è la probabilità di vedere 3 teste in 8 lanci (8 prove di Bernoulli), data una moneta equa (\(\theta = 0,5\))?&#8221;.</p><p>Un modello ci aiuta ad accertare la probabilità di vedere questi dati, \(D\), dato un valore del parametro \(\theta\). La probabilità di vedere i dati \(D\) sotto un particolare valore di \(\theta\) è data dalla seguente notazione: \(P(D|\theta)\).</p><p>Tuttavia, si può essere interessati anche alla domanda alternativa: &#8220;Qual è la probabilità che la moneta sia non truccata, dato che si è verifica una particolare sequenza di teste e croci?&#8221;.</p><p>Quindi si è interessati alla distribuzione di probabilità che riflette la convinzione su diversi valori possibili di \(\theta\), dato che abbiamo osservato alcuni dati \(D\). Questo è denotato da \(P(\theta|D)\). Si noti che questo è l&#8217;opposto di \(P(D|\theta)\). Quindi, come è possibile ottenere un valore reale tra queste due probabilità? Si scopre che la regola di Bayes è il collegamento che consente di passare tra queste due situazioni.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-35ecd73 elementor-widget elementor-widget-heading" data-id="35ecd73" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h4 class="elementor-heading-title elementor-size-default">La Regola di Bayes applicata all'Inferenza Bayesiana</h4>		</div>
				</div>
				<div class="elementor-element elementor-element-1932ddb elementor-widget elementor-widget-text-editor" data-id="1932ddb" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p style="text-align: center;">\(<br />\begin{eqnarray}<br />P(\theta|D) = P(D|\theta) \; P(\theta) \; / \; P(D)<br />\end{eqnarray}<br />\)</p><p>dove:</p><ul><li>P(θ) è la probabilità <strong>a priori</strong>, o probabilità marginale, di θ. Questa è la forza nella nostra convinzione su θ senza considerare le prove D. La nostra precedente opinione sulla probabilità che la moneta sia equa e non truccata.</li><li>P(θ|D) è la probabilità <strong>a posteriori</strong>, o probabilità condizionata. Questa è la forza (raffinata) della nostra convinzione di θ una volta che l&#8217;evidenza di D è stata presa in considerazione. Dopo aver visto uscire 4 teste su 8 lanci della moneta, si può dire che questa è la nostra visione aggiornata sulla correttezza della moneta.</li><li>P(D|θ) è la probabilità condizionata di D. Questa è la probabilità di vedere i dati D come generati da un modello con parametro θ. Se si potesse sapere che la moneta era equa e non truccata, la probabilità condizionata di D esprime la probabilità di avere un determinato numero di risultati &#8220;testa&#8221; per un particolare numero di lanci.</li><li>P(D) è la<strong> prova</strong>, cioè la probabilità a priori di D, e funge da costante di normalizzazione. Questa è la probabilità dei dati D, determinata come somma (o integrazione) di tutti i possibili valori di θ, ponderati con un indice del grado di confidenza con cui si crede alla validità di quei particolari valori di θ.</li></ul><p>L&#8217;obiettivo dell&#8217;inferenza bayesiana è fornire una procedura razionale e matematicamente valida per incorporare le nostre credenze precedenti, con qualsiasi prova a portata di mano, al fine di produrre una nuova credenza a posteriori. Ciò che rende questa tecnica così importante è la possibilità che le credenze a posteriori siano usate come credenze precedenti nella generazione di nuovi dati. Quindi l&#8217;inferenza bayesiana ci consente di adeguare continuamente le nostre convinzioni con nuovi dati, applicando ripetutamente la regola di Bayes.</p><p>In questo articolo è stata introdotta molta teoria da comprendere, quindi è bene vedere un esempio concreto utilizzando l&#8217;antico strumento degli statistici: il lancio di una moneta.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-d16df9d elementor-widget elementor-widget-heading" data-id="d16df9d" data-element_type="widget" data-widget_type="heading.default">
				<div class="elementor-widget-container">
			<h2 class="elementor-heading-title elementor-size-default">Esempio del Lancio di una Moneta</h2>		</div>
				</div>
				<div class="elementor-element elementor-element-bbea876 elementor-widget elementor-widget-text-editor" data-id="bbea876" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>In questo esempio si considera molti lanci di una moneta equa, cioè non truccata. Si utilizza l&#8217;inferenza bayesiana per aggiornare le convinzioni sulla correttezza della moneta quando sono disponibili più dati (cioè più lanci di monete). La moneta sarà effettivamente equa, ma non lo si saprà fino a quando non verranno effettuate le prove. All&#8217;inizio non si ha una precedente convinzione sulla correttezza della moneta, cioè, si può dire che ogni livello di equità è altrettanto probabile.</p><p><span style="letter-spacing: 0px;">Nel linguaggio statistico si eseguono </span><strong style="letter-spacing: 0px;">N</strong><span style="letter-spacing: 0px;"> ripetuti test di Bernoulli con </span><strong style="letter-spacing: 0px;">θ = 0.5</strong><span style="letter-spacing: 0px;">. Si usa una </span><a style="background-color: #ffffff; letter-spacing: 0px;" href="http://en.wikipedia.org/wiki/Uniform_distribution_%28continuous%29">distribuzione uniforme</a><span style="letter-spacing: 0px;"> come mezzo per caratterizzare la precedente convinzione relativa ai dubbi sull&#8217;equità. Questo afferma che si considera ogni livello di equità (o ogni valore di θ) ugualmente probabile.</span></p><p><span style="letter-spacing: 0px;">Si usa poi una procedura di aggiornamento bayesiano per passare dalle credenze precedenti alle credenze posteriori osservando nuovi lanci di monete. Questo viene eseiguito utilizzando una particolare procedura matematica che si basa sulla </span><a style="background-color: #ffffff; letter-spacing: 0px;" href="https://en.wikipedia.org/wiki/Conjugate_prior"><em>Conjugate Prior</em></a><span style="letter-spacing: 0px;">, cioè la distribuzione a priori coniugata, che fornisce gli strumenti per spiegare come l&#8217;esempio del lancio della moneta viene eseguito nella pratica.</span></p><p><span style="letter-spacing: 0px;">La distribuzione uniforme è in realtà un caso più specifico di un&#8217;altra distribuzione di probabilità, nota come </span><a style="background-color: #ffffff; letter-spacing: 0px;" href="http://en.wikipedia.org/wiki/Beta_distribution">distribuzione Beta</a><span style="letter-spacing: 0px;">. Dato che, con il modello binomiale, se si usa una distribuzione Beta per le convinzioni precedenti si ottiene una distribuzione Beta anche per le convinzioni a posteriori. Questo è un risultato matematico estremamente utile, in quanto le distribuzioni Beta sono abbastanza flessibili nelle convinzioni di modellazione. Tuttavia, non voglio soffermarmi troppo su questi dettagli, dato che ne discuteremo nel prossimo articolo. In questa fase, queste distribuzioni consentono di creare facilmente alcune rappresentazioni grafiche che enfatizzano la procedura bayesiana!</span></p><p><span style="letter-spacing: 0px;">Nella seguente figura si possono vedere 6 casi particolari in cui sono stati effettuato un certo numero di prove di Bernoulli (lancio di moneta). Nella prima sotto-trama non sono state effettuato prove e quindi la funzione di densità di probabilità (in questo caso la densità precedente) è la distribuzione uniforme. Dichiara che si ha la stessa credenza in tutti i valori di θ che rappresentano l&#8217;equità della moneta.</span></p><p>Il pannello successivo mostra 2 prove effettuate ed per entrambe è uscito testa. La procedura bayesiana che utilizza le distribuzioni Beta consente ora di aggiornare a posteriori la densità. Da notare come in questo il peso della densità è ora spostato sul lato destro del grafico. Questo indica che la precedente credenza di eguale probabilità di equità della moneta, accoppiata con 2 nuovi punti di dati, porta a credere che la moneta abbia più probabilità di essere truccata (distorta verso le teste).</p><p>I successivi due grafici mostrano rispettivamente 10 e 20 prove. Da notare che nonostante aver ottenuto 2 croci in in 10 prove, si crede ancora che la moneta sia ingiusta e distorta nei confronti delle teste. Dopo 20 prove, si è ottenuto qualche croce in più. La densità della probabilità è ora spostata più vicino a <strong>θ = P(H) = 0,5</strong>. Quindi si sta iniziando a credere che probabilmente moneta sia equa.</p><p>Dopo 50 e 500 prove, ora stiamo iniziando a credere che l&#8217;equità della moneta sia molto probabilmente intorno a θ = 0,5. Ciò è indicato dalla larghezza restringente della densità di probabilità, che ora è raggruppata strettamente attorno a θ = 0.46 nel pannello finale. Se dovessimo effettuare altre 500 prove (dal momento che la moneta è effettivamente giusta) vedremmo questa densità di probabilità diventare ancora più stretta e centrata più vicino a θ = 0,5.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-cdbdf28 elementor-widget elementor-widget-image" data-id="cdbdf28" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="768" height="629" src="https://datatrading.info/wp-content/uploads/2017/10/bayes-bernoulli-trading-algoritmico-768x629.png" class="attachment-medium_large size-medium_large" alt="" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/2017/10/bayes-bernoulli-trading-algoritmico-768x629.png 768w, https://datatrading.info/wp-content/uploads/2017/10/bayes-bernoulli-trading-algoritmico.png 1035w" sizes="(max-width: 768px) 100vw, 768px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-f9b3c21 elementor-widget elementor-widget-text-editor" data-id="f9b3c21" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Quindi si può vedere che l&#8217;inferenza bayesiana fornisce una procedura razionale per passare da una situazione incerta con informazioni limitate a una situazione più certa con una quantità significativa di dati. Nel prossimo articolo si descrive la nozione della distribuzione a priori coniugata in modo più approfondito, il che semplifica fortemente la matematica per eseguire l&#8217;inferenza bayesiana in questo esempio.

Per completezza, si fornisce il codice Python (fortemente commentato) per la produzione di questa trama. Fa uso del modello statistico di SciPy, in particolare, la distribuzione Beta:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-e380934 elementor-widget elementor-widget-bdt-source-code" data-id="e380934" data-element_type="widget" data-widget_type="bdt-source-code.default">
				<div class="elementor-widget-container">
			
        <div class="bdt-source-code prism-okaidia">

                            <button class="bdt-copy-button">Copy                </button>
            
            <pre class="language-py">
            <code>
if __name__ == &quot;__main__&quot;:
    # Create a list of the number of coin tosses (&quot;Bernoulli trials&quot;)
    number_of_trials = [0, 2, 10, 20, 50, 500]

    # Conduct 500 coin tosses and output into a list of 0s and 1s
    # where 0 represents a tail and 1 represents a head
    data = stats.bernoulli.rvs(0.5, size=number_of_trials[-1])
    
    # Discretise the x-axis into 100 separate plotting points
    x = np.linspace(0, 1, 100)
    
    # Loops over the number_of_trials list to continually add
    # more coin toss data. For each new set of data, we update
    # our (current) prior belief to be a new posterior. This is
    # carried out using what is known as the Beta-Binomial model.
    # For the time being, we won&#039;t worry about this too much. It 
    # will be the subject of a later article!
    for i, N in enumerate(number_of_trials):
        # Accumulate the total number of heads for this 
        # particular Bayesian update
        heads = data[:N].sum()

        # Create an axes subplot for each update 
        ax = plt.subplot(len(number_of_trials) / 2, 2, i + 1)
        ax.set_title(&quot;%s trials, %s heads&quot; % (N, heads))

        # Add labels to both axes and hide labels on y-axis
        plt.xlabel(&quot;$P(H)$, Probability of Heads&quot;)
        plt.ylabel(&quot;Density&quot;)
        if i == 0:
            plt.ylim([0.0, 2.0])
        plt.setp(ax.get_yticklabels(), visible=False)
                
        # Create and plot a  Beta distribution to represent the 
        # posterior belief in fairness of the coin.
        y = stats.beta.pdf(x, 1 + heads, 1 + N - heads)
        plt.plot(x, y, label=&quot;observe %d tosses,\n %d heads&quot; % (N, heads))
        plt.fill_between(x, 0, y, color=&quot;#aaaadd&quot;, alpha=0.5)

    # Expand plot to cover full width/height and show it
    plt.tight_layout()
    plt.show()</code>
        </pre>

        </div>
		</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/introduzione-alla-statistica-bayesiana/">Introduzione alla Statistica Bayesiana</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Modelli di Markov nascosti per determinare il Regime di Mercato</title>
		<link>https://datatrading.info/modelli-di-markov-nascosti-per-determinare-il-regime-di-mercato/</link>
					<comments>https://datatrading.info/modelli-di-markov-nascosti-per-determinare-il-regime-di-mercato/#respond</comments>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Wed, 30 Aug 2017 15:14:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Time series analysis]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4795</guid>

					<description><![CDATA[<p>Nell&#8217;articolo precedente della serie abbiamo introdotto i Hidden Markov Models. Sono stati descritti nel contesto della più ampia classe dei modelli di Markov. Questi modelli sono usati dai trader quantitativi per determinare i regimi di mercato in modo da adattare la gestione delle loro strategie quantitative. In particolare è stato menzionato che &#8220;la variazione dei &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/modelli-di-markov-nascosti-per-determinare-il-regime-di-mercato/"> <span class="screen-reader-text">Modelli di Markov nascosti per determinare il Regime di Mercato</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/modelli-di-markov-nascosti-per-determinare-il-regime-di-mercato/">Modelli di Markov nascosti per determinare il Regime di Mercato</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4795" class="elementor elementor-4795">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-29c84a0 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="29c84a0" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-c24133f" data-id="c24133f" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-b8b3c2a elementor-widget elementor-widget-text-editor" data-id="b8b3c2a" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Nell&#8217;<a href="https://datatrading.info/introduzione-ai-modelli-di-markov-nascosti/">articolo precedente</a> della serie abbiamo introdotto i Hidden Markov Models. Sono stati descritti nel contesto della più ampia classe dei modelli di Markov. Questi modelli sono usati dai trader quantitativi per determinare i regimi di mercato in modo da adattare la gestione delle loro strategie quantitative.</p><p>In particolare è stato menzionato che &#8220;<em>la variazione dei regimi  causano aggiustamenti dei ritorni degli asset tramite la variazione delle loro medie, varianze/volatilità, correlazioni seriali e covarianze, influenzando l&#8217;efficacia dei metodi di analisi delle serie temporali che si basano sulla stazionarietà</em>&#8220;. Questo ha un significativo impatto nel modo in cui le strategie di trading subiscono variazioni durante il loro ciclo di vita.</p><p>In questo articolo descriviamo come implementare i modelli di Markov Nascosti tramite il linguaggio di programmazione Python. L&#8217;interfaccia Python più semplice per i modelli markov nascosti è la libreiria <a href="https://hmmlearn.readthedocs.io/">hmmlearn</a>. Descriviamo come usare i HMM per analizzare  i vari regimi in cui si trovano i mercati azionari statunitensi. Negli articoli successivi descriviamo come usare queste analisi di regime in una sottoclasse del modulo <code>RiskManager</code> di <a href="https://github.com/datatrading-info/DataTrader">DataTrader</a> al fine di regolare i segnali di trading per alcune strategie.</p><p>All&#8217;interno dell&#8217;articolo effettuiamo una simulazione di una serie di rendimenti del mercato in due diversi regimi &#8211; &#8220;rialzista&#8221; e &#8220;ribassista&#8221;. Quindi applichiamo un modello di Markov alla serie dei ritorni per identificare la probabilità di trovarsi in un particolare regime di mercato.</p><p>Dopo aver delineato la procedura sui dati simulati, applichiamo il modello Hidden Markov ai dati azionari statunitensi al fine di determinare i regimi a due stati sottostanti.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2525bd6 elementor-widget elementor-widget-text-editor" data-id="2525bd6" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Regimi di mercato</h2><p>Applicare i Hidden Markov Models al rilevamento del regime è complicato poiché si tratta in realtà di una forma di <strong>apprendimento non supervisionato</strong>. Cioè, non esiste una &#8220;verità di base&#8221; o dati etichettati su cui &#8220;addestrare&#8221; il modello. In particolare non è chiaro quanti stati di regime esistano a priori. Esistono due, tre, quattro o più &#8220;veri&#8221; regimi di mercato nascosti?</p><p>Le risposte a queste domande dipendono fortemente dalla classe di asset che si vuole analizzare, dalla scelta del periodo di tempo e dalla natura dei dati utilizzati. Ad esempio, i dati sui rendimenti giornalieri nei mercati azionari mostrano spesso periodi di calma e volatilità più bassa, anche per un certo numero di anni, con periodi eccezionali di elevata volatilità in momenti di &#8220;panico&#8221; o di &#8220;correzione&#8221;. È quindi sufficiente modellare gli indici azionari con due stati? Potrebbe esserci un terzo stato intermedio che rappresenta più volatilità del solito ma non è un vero e proprio panico?</p><p>L&#8217;utilizzo di Hidden Markov Models come componente di un gestore del rischio che può interferire con gli ordini generati dalla strategia richiede un&#8217;analisi di ricerca attenta e una solida comprensione delle classi di asset modellati. Nei prossimi articoli le performance di varie strategie di trading sono analizzate per diversi gestori del rischio basati sul modello Hidden Markov.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-b4b7857 elementor-widget elementor-widget-text-editor" data-id="b4b7857" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Dati simulati</h2><p>In questa sezione vediamo come generare dati simulati dei rendimenti a partire da distribuzioni gaussiane separate, ciascuna delle quali rappresenta il regime di mercato &#8220;rialzista&#8221; o &#8220;ribassista&#8221;. I rendimenti rialzisti attingono da una distribuzione guassiana con media positiva e varianza bassa, mentre i rendimenti ribassisti attingono da una distribuzione gaussiana con media leggermente negativa ma varianza più elevata.</p><p>Cinque periodi di regime di mercato separati sono simulati e &#8220;cuciti&#8221; insieme con Python. Il flusso di rendimenti complessivo è quindi usato in un modello di Markov nascosto per dedurre le probabilità a posteriori degli stati di regime, a partire la sequenza di osservazioni.</p><p>Il primo compito è installare le librerie <code>yfinance</code> e <code>hmmlearn</code> e quindi importarle in Python. Impostiamo anche il seed casuale in modo da consentire una replica coerente dei risultati:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-32e21b4 elementor-widget elementor-widget-code-highlight" data-id="32e21b4" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance
import hmmlearn

np.random.seed(1)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-a25e41a elementor-widget elementor-widget-text-editor" data-id="a25e41a" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>In questo esempio simuliamo un mercato con due regimi, assumendo che i rendimenti di mercato siano normalmente distribuiti.  Simuliamo regimi separati, ciascuno composto da \(N_k\) giorni di  rendimenti. Ognuno di questi <em>k</em> regimi può essere rialzista o ribassista. L&#8217;obiettivo del modello Hidden Markov è identificare quando il regime passa da rialzista a ribassista e viceversa.</p><p>Consideriamo \(k=5\) e \(N_k \in [50, 150]\). Il mercato rialzista è distribuito come \(\mathcal{N}(0.1, 0.1)\) mentre il mercato ribassista è distribuito come \(\mathcal{N}(-0.05, 0.2)\). I parametri sono impostati tramite il seguente codice:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2dacb75 elementor-widget elementor-widget-code-highlight" data-id="2dacb75" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Create the parameters for the bull and bear market returns distributions
Nk_lower = 50
Nk_upper = 150
bull_mean = 0.1
bull_var = 0.1
bear_mean = -0.05
bear_var = 0.2</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-1fe14ce elementor-widget elementor-widget-text-editor" data-id="1fe14ce" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				I valori \(N_k\) sono scelti casualmente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-13b6b5c elementor-widget elementor-widget-code-highlight" data-id="13b6b5c" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Create the list of durations (in days) for each regime
days = np.random.randint(Nk_lower, Nk_upper, 5)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-1c473c4 elementor-widget elementor-widget-text-editor" data-id="1c473c4" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				I rendimenti per ciascuno <em>k</em>-esimo periodo sono estratti casualmente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-e6abc9b elementor-widget elementor-widget-code-highlight" data-id="e6abc9b" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Create the various bull and bear markets returns
market_bull_1 = np.random.normal(loc=bull_mean, scale=bull_var, size=days[1])
market_bear_2 = np.random.normal(loc=bear_mean, scale=bear_var, size=days[2])
market_bull_3 = np.random.normal(loc=bull_mean, scale=bull_var, size=days[3])
market_bear_4 = np.random.normal(loc=bear_mean, scale=bear_var, size=days[4])
market_bull_5 = np.random.normal(loc=bull_mean, scale=bull_var, size=days[5])</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-5fe6bf3 elementor-widget elementor-widget-text-editor" data-id="5fe6bf3" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Il codice Python per creare gli stati del regime (1 per rialzista o 2 per ribassista) e l&#8217;elenco finale dei rendimenti è il seguente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-aa9ba51 elementor-widget elementor-widget-code-highlight" data-id="aa9ba51" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp># Create the list of true regime states and full returns list
true_regimes=np.r_[np.repeat(1, days[0]), np.repeat(2,days[1]), np.repeat(1, days[2]), np.repeat(2,days[3]), np.repeat(1,days[4])]

returns = np.r_[market_bull_1, market_bear_2, market_bull_3, market_bear_4, market_bull_5]</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-def558f elementor-widget elementor-widget-text-editor" data-id="def558f" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Il grafico dei rendimenti mostra chiaramente le variazioni nella media e nella varianza tra i cambi di regime:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-729c882 elementor-widget elementor-widget-image" data-id="729c882" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="384" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-returns.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-hmm-returns" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-returns.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-returns-300x165.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-returns-160x88.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-e5909a2 elementor-widget elementor-widget-text-editor" data-id="e5909a2" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				In questa fase è possibile specificare e adattare il modello di Markov nascosto utilizzando l&#8217;<a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">algoritmo di massimizzazione delle aspettative</a>, i cui dettagli esulano dallo scopo di questo articolo. A tale scopo usiamo la funzione <code>GaussianHMM</code> della libreria <code>hmmlearn</code>, impostando il numero di stati pari a 2:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-f58824e elementor-widget elementor-widget-code-highlight" data-id="f58824e" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from hmmlearn.hmm import GaussianHMM
rets = np.column_stack([returns])
# Create the Gaussian Hidden markov Model and fit it
# to the returns data, outputting a score
hmm_model = GaussianHMM(
    n_components=2, covariance_type="full", n_iter=1000
).fit(rets)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-260d127 elementor-widget elementor-widget-text-editor" data-id="260d127" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Dopo il fitting del modello è possibile tracciare il grafico delle probabilità a posteriori di trovarsi in un particolare stato di regime. La variabile <code>post_probs</code> contiene le probabilità a posteriori. Queste probabilità sono confrontati con i veri stati sottostanti. Da notare che il modello Hidden Markov fa un buon lavoro nell&#8217;identificare correttamente i regimi, anche se con un certo ritardo:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-190b662 elementor-widget elementor-widget-code-highlight" data-id="190b662" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>fig, axs = plt.subplots(2) # Definezione di due grafici
axs[0].plot(hidden_states)
axs[0].set(ylabel='Regimes')
axs[1].plot(post_prob, label=['Bull','Bear'])
axs[1].set(ylabel='Probability')
axs[1].legend()
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-51f76e1 elementor-widget elementor-widget-image" data-id="51f76e1" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="348" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-simulate.png" class="attachment-large size-large" alt="trading-quantitativo-hmm-simulate" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-simulate.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-simulate-300x149.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-simulate-160x80.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a1561eb elementor-widget elementor-widget-text-editor" data-id="a1561eb" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Vediamo ora come applicare il  modello Hidden Markov a dati finanziari storici del mondo reale.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6a88cc6 elementor-widget elementor-widget-text-editor" data-id="6a88cc6" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Dati finanziari</h2>
Nella sezione precedente è stato semplice nascosto determinare i regimi con il modello di Markov dato che gli stati erano simulati da un insieme pre-specificato di gaussiane. Come affermato in precedenza, il problema del Regime Detection è in realtà una sfida di apprendimento non supervisionato poiché il numero di stati non è noto a priori, cioè non esiste nessuna &#8220;verità di base&#8221; con cui &#8220;addestrare&#8221; l&#8217;HMM.

In questa sezione effettuiamo due attività di modellazione separate. La prima esegue il fitting dell&#8217;HMM con due stati di regime ai rendimenti dell&#8217;S&#038;P500, mentre la seconda utilizza tre stati. Quindi confrontiamo i risultati tra i due modelli.

In questo esempio, il processo di applicazione del modello Hidden Markov è simile a quello eseguito per i dati simulati. Invece di generare il flusso dei rendimenti da due distribuzioni gaussiane, scarichiamo i dati storici reali utilizzando la libreria <code>yfinance</code>:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-042ac81 elementor-widget elementor-widget-code-highlight" data-id="042ac81" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import matplotlib.pyplot as plt
import yfinance

# Obtain S&P500 data from 2004 and
# create the returns stream from this
data = yfinance.download("^GSPC", start="2004-01-01", end="2017-11-01")
data = data.asfreq('b').fillna(method='ffill')

Return = data['Adj Close'].pct_change()
LogRet = np.log(data['Adj Close']).diff().dropna()

plt.plot(LogRet)
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-e5a27a4 elementor-widget elementor-widget-image" data-id="e5a27a4" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="374" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-gspcrets.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-hmm-gspcrets" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-gspcrets.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-gspcrets-300x160.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-gspcrets-160x85.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-903c2c4 elementor-widget elementor-widget-text-editor" data-id="903c2c4" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Ora applichiamo un modello di Markov nascosto a due stati utilizzando l&#8217;algoritmo EM. Visualizziamo quindi i grafici dei rendimenti e le probabilità a posteriori di ciascun regime:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-4896712 elementor-widget elementor-widget-code-highlight" data-id="4896712" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from hmmlearn.hmm import GaussianHMM
rets = np.column_stack([LogRet])
# Create the Gaussian Hidden markov Model and fit it to the returns data
hmm_model = GaussianHMM(
    n_components=2, covariance_type="full", n_iter=1000
).fit(rets)
post_prob = hmm_model.predict_proba(rets)

fig, axs = plt.subplots(2) # Definezione di due grafici
axs[0].plot(rets)
axs[0].set(ylabel='Returns')
axs[1].plot(post_prob, label=['Regime 1','Regime 2'])
axs[1].set(ylabel='Probability')
axs[1].legend()
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-f0fdc67 elementor-widget elementor-widget-image" data-id="f0fdc67" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="376" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-two-state.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-hmm-market-regime-two-state" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-two-state.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-two-state-300x161.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-two-state-160x86.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8019706 elementor-widget elementor-widget-text-editor" data-id="8019706" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Possiamo vedere che dal 2004 al 2007 i mercati erano più calmi e quindi il modello Hidden Markov ha dato un&#8217;elevata probabilità a posteriori di regime n. 1 per questo periodo. Tuttavia, tra il 2007 e il 2009 i mercati sono stati incredibilmente volatili a causa della crisi dei <a href="https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%9308">mutui subprime</a>. Questo causa inizialmente un rapido cambio delle probabilità a posteriori tra i due stati, ma resta comunque abbastanza coerentemente nel regime n. 2 durante tutto il 2008.</p><p>I mercati sono diventati più calmi nel 2010, ma nel 2011 si è verificata un&#8217;ulteriore volatilità, portando ancora una volta l&#8217;HMM a dare un&#8217;elevata probabilità a posteriori per il regime 2. Dopo il 2011 i mercati sono tornati a calmarsi e l&#8217;HMM restituisce costantemente alte probabilità per il regime 1. Nel 2015 i mercati sono diventati ancora una volta più instabili e ciò si riflette nell&#8217;HMM con una maggiore frequenza di passaggi da un regime all&#8217;altro.</p><p>Lo stesso processo verrà ora eseguito per un HMM a tre stati. ci sono poche modifiche da fare nel codice, ad eccezione della modifica del parametro n_components=3 e della della legenda del grafico:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-bdd7e96 elementor-widget elementor-widget-code-highlight" data-id="bdd7e96" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>hmm_model = GaussianHMM(
    n_components=3, covariance_type="full", n_iter=1000
).fit(rets)
post_prob = hmm_model.predict_proba(rets, None)

fig, axs = plt.subplots(2) # Definezione di due grafici
axs[0].plot(rets)
axs[0].set(ylabel='Returns')
axs[1].plot(post_prob, '--', label=['Regime 1','Regime 2', 'Regime 3'])
axs[1].set(ylabel='Probability')
axs[1].legend()
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-33cf35a elementor-widget elementor-widget-image" data-id="33cf35a" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="768" height="381" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-three-state-768x381.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-hmm-market-regime-three-state" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-three-state-768x381.png 768w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-three-state-300x149.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-three-state-160x79.png 160w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-market-regime-three-state.png 850w" sizes="(max-width: 768px) 100vw, 768px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-bffdf44 elementor-widget elementor-widget-text-editor" data-id="bffdf44" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>La lunghezza dei dati rende il grafico delle probabilità a posteriori un po&#8217; più complicato da interpretare. Dato che il modello è costretto a considerare tre regimi separati, otteniamo a un comportamento di frequente passaggio tra il Regime #1 e il Regime #2 nel periodo più calmo dal 2004 al 2007. Tuttavia, nei periodi volatili del 2008, 2010 e 2011, il regime n. 3 domina la probabilità a posteriori indicando uno stato altamente volatile. Dopo il 2011 il modello torna al passaggio tra il Regime #1 e il Regime #2.</p><p>È chiaro che la scelta del numero iniziale di stati da applicare a un flusso di rendimenti reali è un elemento critico. Dipende dalla tipologia di asset utilizzata, da come viene eseguita la negoziazione di questo asset e dal periodo temporale considerato.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-0559b92 elementor-widget elementor-widget-text-editor" data-id="0559b92" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Prossimi passi</h2><p>Negli articoli successivi descriviamo come utilizzare il modello di Markov nascosto sarà utilizzato in una sottoclasse di <code>RiskManager</code> nel motore di backtesting e trading live di <a href="https://github.com/datatrading-info/DataTrader">DataTrader</a>. In particolare vediamo come determinare quando applicare una strategia trend following nel tentativo di migliorare la redditività in caso di assenza di gestione del rischio.</p>					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/modelli-di-markov-nascosti-per-determinare-il-regime-di-mercato/">Modelli di Markov nascosti per determinare il Regime di Mercato</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://datatrading.info/modelli-di-markov-nascosti-per-determinare-il-regime-di-mercato/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Introduzione ai Modelli di Markov Nascosti</title>
		<link>https://datatrading.info/introduzione-ai-modelli-di-markov-nascosti/</link>
					<comments>https://datatrading.info/introduzione-ai-modelli-di-markov-nascosti/#respond</comments>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Sat, 26 Aug 2017 13:35:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Time series analysis]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4772</guid>

					<description><![CDATA[<p>Una sfida costante per i trader quantitativi è la frequente modifica del comportamento dei mercati finanziari, spesso bruscamente, a causa dei periodi mutevoli delle politiche governative, dell&#8217;ambiente normativo e di altri effetti macroeconomici. Tali periodi sono conosciuti colloquialmente come &#8220;regimi di mercato&#8221; e rilevare tali cambiamenti è un processo comune, sebbene difficile, intrapreso dai partecipanti &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/introduzione-ai-modelli-di-markov-nascosti/"> <span class="screen-reader-text">Introduzione ai Modelli di Markov Nascosti</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/introduzione-ai-modelli-di-markov-nascosti/">Introduzione ai Modelli di Markov Nascosti</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4772" class="elementor elementor-4772">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-4275b3c elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="4275b3c" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-4a74184" data-id="4a74184" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-01d8ee1 elementor-widget elementor-widget-text-editor" data-id="01d8ee1" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Una sfida costante per i trader quantitativi è la frequente modifica del comportamento dei mercati finanziari, spesso bruscamente, a causa dei periodi mutevoli delle politiche governative, dell&#8217;ambiente normativo e di altri effetti macroeconomici. Tali periodi sono conosciuti colloquialmente come &#8220;regimi di mercato&#8221; e rilevare tali cambiamenti è un processo comune, sebbene difficile, intrapreso dai partecipanti al mercato quantitativo.</p><p>Questi vari regimi portano ad aggiustamenti dei rendimenti degli asset tramite spostamenti nelle loro medie, varianze/volatilità, correlazioni seriali e covarianze, che influiscono sull&#8217;efficacia dei metodi delle serie temporali che si basano sulla stazionarietà. In particolare può portare a una correlazione dinamicamente variabile, curtosi in eccesso (&#8220;code grasse&#8221;), eteroschedasticità (raggruppamento di correlazione seriale) e rendimenti distorti.</p><p>Per questi motivi è necessario rilevare e classificare efficacemente  i regimi di mercato in modo da selezionare le migliori implementazioni di strategie di trading quantitative e ottimizzare i parametri al loro interno. L&#8217;attività di modellazione diventa quindi un tentativo di identificare quando si verifica un cambio di regime e adeguare di conseguenza i criteri di implementazione della strategia, la gestione del rischio e il dimensionamento della posizione.</p><p>Uno dei metodi metodo più usati per rilevare il regime consiste nell&#8217;utilizzare una tecnica statistica di serie temporali nota come Hidden Markov Model [2] . Questi modelli sono adatti al compito perché implicano l&#8217;inferenza su processi generativi &#8220;nascosti&#8221; tramite osservazioni indirette &#8220;rumorose&#8221; correlate a questi processi. In questo caso il processo nascosto o latente è lo stato del regime sottostante, mentre i rendimenti dell&#8217;asset sono le osservazioni rumorose indirette, che sono influenzate da questi stati.</p><p>Questo articolo descrive la teoria matematica alla base dei modelli di Markov nascosti (HMM) e come possono essere applicati per determinare il regime ai fini del trading quantitativo.</p><p>Iniziamo introducendo il concetto di modello di Markov [1] e la relativa categorizzazione, che dipende dal livello di autonomia del sistema e dalla quantità di informazioni osservate sul sistema. Successivamente descriviamo nello specifico l&#8217;architettura di HMM come processo autonomo, con informazioni parzialmente osservabili.</p><p>Come per i precedenti articoli sui <a href="https://datatrading.info/modelli-dello-spazio-statale-e-filtro-di-kalman/">modelli di spazio degli stati e il filtro di Kalman</a>, introduciamo i concetti inferenziali di filtraggio, smoothing e previsione. Non descriviamo gli algoritmi specifici come Forward Algorithm [6] e Viterbi Algorithm [7] che svolgono questi compiti, poiché il fulcro dell&#8217;articolo sono le applicazioni dell&#8217;HMM alla finanza quantistica, piuttosto che nella derivazione dell&#8217;algoritmo.</p><p>L&#8217;articolo successivo descrive come applicare l&#8217;HMM  a vari asset per  determinare il regime. Questi dati sono quindi aggiunti a una serie di strategie di trading quantitative tramite un &#8220;gestore del rischio&#8221;. Questo è usato per valutare come varia la performance del trading algoritmico con e senza la determinazione del regime.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6f7e0df elementor-widget elementor-widget-text-editor" data-id="6f7e0df" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Modelli di Markov</h2><p>Prima della idescrivere i i modelli di Markov nascosti è necessario introdurre il concetto base del modello di Markov. Un modello di Markov è un modello stocastico dello spazio degli stati che prevede transizioni casuali tra gli stati, dove la probabilità del salto dipende solo dallo stato corrente, piuttosto che da uno qualsiasi degli stati precedenti. Si dice che il modello possieda la <a href="https://datatrading.info/le-proprieta-di-markov-e-martingale/">proprietà Markov</a> ed è &#8220;senza memoria&#8221;. I modelli <a href="https://datatrading.info/white-noise-e-random-walks-nellanalisi-delle-serie-temporali/">Random Walk</a> sono un altro classico esempio del modello di Markov.</p><p>I modelli di Markov possono essere classificati in quattro grandi classi di modelli a seconda dell&#8217;autonomia del sistema e nel caso dove tutte o parte delle informazioni sul sistema possono essere osservate in ogni stato. La pagina del modello Markov su Wikipedia [1] fornisce un&#8217;utile matrice che delinea queste differenze, che riportiamo di seguito:</p><table class="table"><tbody><tr><th> </th><th>Completamente osservabile</th><th>Parzialmente osservabile</th></tr><tr><td><strong>Autonomo</strong></td><td>Catena di Markov  [3]</td><td>Modello Markov nascosto [2]</td></tr><tr><td><strong>Controllato</strong></td><td>Processo decisionale di Markov [3]</td><td>Processo decisionale di Markov<br />parzialmente osservabile [4]</td></tr></tbody></table><p>Il modello più semplice, la catena di Markov, è autonomo e completamente osservabile. Non può essere modificato dalle azioni di un &#8220;agente&#8221;, come nei processi controllati, e tutte le informazioni sono disponibili dal modello per ogni stato. Un buon esempio di catena di Markov è l&#8217; algoritmo <a href="https://datatrading.info/markov-chain-monte-carlo-per-linferenza-bayesiana-lalgoritmo-metropolis/">Markov Chain Monte Carlo (MCMC)</a> ampiamente utilizzato nell&#8217;inferenza bayesiana computazionale.</p><p>Se il modello è completamente autonomo ma solo parzialmente osservabile, allora è noto come Hidden Markov Model. In questo modello ci sono stati latenti sottostanti (e probabili transizioni tra di loro) che non sono direttamente osservabili ma influenzano le &#8220;osservazioni&#8221;. Da sottolineare che gli stati latenti possiedono la proprietà di Markov, ma non è necessario che la possiedano anche gli stati osservabili. L&#8217;uso più comune di HMM al di fuori della finanza quantitativa è nel campo del riconoscimento vocale.</p><p>Se il sistema può essere &#8220;controllato&#8221; da uno o più agenti, tali processi rientrano nella denominazione di Reinforcement Learning (RL), spesso considerato il terzo &#8220;pilastro&#8221; del machine learning automatico insieme all&#8217;apprendimento supervisionato e all&#8217;apprendimento non supervisionato. Se il sistema è completamente osservabile, ma controllato, il modello è chiamato Markov Decision Process (MDP). Una tecnica correlata è nota come Q-Learning [11] , che viene utilizzata per ottimizzare la politica di selezione del comportamento di un agente all&#8217;interno di un modello di processo decisionale di Markov. Nel 2015 Google DeepMind ha aperto la strada all&#8217;uso di Deep Reinforcement Networks, o Deep Q Networks, per creare un agente in grado di giocare ai videogiochi Atari 2600 esclusivamente dal buffer dello schermo [12] .</p><p>Se il sistema è controllato  ma solo parzialmente osservabile, i modelli di Reinforcement Learning sono chiamati Processi decisionali di Markov parzialmente osservabili (POMDP). Le tecniche per risolvere i POMDP ad alta dimensione sono attualmente oggetto di molte ricerche accademiche. Il team senza scopo di lucro di OpenAI dedica molto tempo a esaminare tali problemi e ha rilasciato un toolkit open source, o &#8220;palestra&#8221;, per consentire il test diretto di nuovi agenti RL noti come OpenAI Gym [13].</p><p>Sfortunatamente il Reinforcement Learning, insieme al MDP e al POMDP, non rientrano nello scopo di questo articolo. Tuttavia, saranno oggetto di articoli successivi, in particolare in una serie di articoli dedicata al <a href="https://datatrading.info/tutorial/modellazione-statistica-machine-learning/">Deep Learning</a>.</p><p><em>Si noti che in questo articolo non sono considerati i processi di Markov a tempo continuo. Nel trading quantitativo l&#8217;unità di tempo viene spesso fornita tramite tick o barre di dati storici sugli asset. Tuttavia, se l&#8217;obiettivo è quello di prezzare i contratti derivati, si dovrebbe usare il meccanismo a tempo continuo del calcolo stocastico.</em></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-1fe7799 elementor-widget elementor-widget-text-editor" data-id="1fe7799" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h4>Specifiche matematiche del modello Markov</h4><p><em>Questa sezione, così come quella sulla matematica del modello di Markov nascosto, segue da vicino la notazione e la specifica del modello di Murphy (2012) [8].</em></p><p>Nella finanza quantitativa l&#8217;analisi di una serie storica è spesso di primario interesse. Tale serie temporale è generalmente costituita da una sequenza \(T\) di osservazioni discrete \(X_1, \ldots, X_T\). I modelli di Markov Chain prevedono che, in qualsiasi momento \(t\), l&#8217;osservazione \(X_t\) acquisisce tutte le informazioni necessarie per fare previsioni sugli stati futuri. Questa ipotesi sarà utilizzata nella seguente specifica.</p><p>La formulazione della catena di Markov in un quadro probabilistico consente di scrivere la funzione di densità congiunta della probabilità di vedere le osservazioni come segue:</p><p style="text-align: center;">\(\begin{eqnarray}p(X_{1:T}) &amp;=&amp; p(X_1)p(X_2 \mid X_1)p(X_3 \mid X_2)\ldots \\<br />&amp;=&amp; p(X_1) \prod^{T}_{t=2} p(X_t \mid X_{t-1})\end{eqnarray}\)</p><p>La formula afferma che la probabilità di vedere sequenze di osservazioni è data dalla probabilità dell&#8217;osservazione iniziale moltiplicata \(T-1\) volte dalla probabilità condizionata di vedere l&#8217;osservazione successiva, conoscendo l&#8217;osservazione precedente. In questo articolo si assume che il termine \(p(X_t \mid X_{t-1})\), noto come funzione di transizione, sia indipendente dal tempo.</p><p>Inoltre, poiché i modelli del regime di mercato descritti in questi articoli consistono in un numero piccolo e discreto \(K\) di regimi (o &#8220;stati&#8221;), il tipo di modello in esame è noto come catena di Markov a stato discreto (DSMC).</p><p>Quindi se ci sono K possibili stati, o regimi, separati dove il modello può trovarsi in qualsiasi momento t allora la funzione di transizione può essere scritta come una <em>matrice di transizione</em>. Questa matrice descrive la probabilità di passare dallo stato <em>j</em> allo stato <em>i</em> in qualsiasi momento. Matematicamente, gli elementi della matrice di transizione <strong><em>A</em></strong> sono dati da:</p><p style="text-align: center;">\(\begin{eqnarray}A_{ij} = p(X_t = j \mid X_{t-1} = i)\end{eqnarray}\)</p><p><span class="goog-text-highlight">A titolo di esempio è possibile considerare un semplice modello di catena di Markov a due stati. </span>Il seguente diagramma rappresenta gli stati numerati come cerchi mentre gli archi rappresentano la probabilità di saltare da uno stato all&#8217;altro:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-7ceabce elementor-widget elementor-widget-image" data-id="7ceabce" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="300" height="228" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-two-state-300x228.png" class="attachment-medium size-medium" alt="trading-quantitativo-hmm-two-state" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-two-state-300x228.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-two-state-160x121.png 160w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-two-state.png 700w" sizes="(max-width: 300px) 100vw, 300px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-7c7f3aa elementor-widget elementor-widget-text-editor" data-id="7c7f3aa" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Si noti che le probabilità si sommano all&#8217;unità per ogni stato, ad es \(\alpha + (1 &#8211; \alpha) = 1\). La matrice di transizione A per questo sistema è una matrice 2&#215;2 data da:</p><p style="text-align: center;">\(\begin{eqnarray}A = \left( \begin{array}{cc}<br />1-\alpha &amp; \alpha \\<br />\beta &amp; 1-\beta \end{array} \right)\end{eqnarray}\)</p><p> Per simulare i <em>n</em> passi di un modello DSMC generico è possibile definire n-esimo passo della matrice di transizione come:</p><p style="text-align: center;">\(\begin{eqnarray}A_{ij}(n) := p(X_{t+n} = j \mid X_t = i)\end{eqnarray}\)</p><p>Si può facilmente dimostrare che \(A(m+n)=A(m)A(n)\) e quindi che \(A(n)=A(1)^n\). Ciò significa che gli <em>n</em> passaggi di un modello DSMC possono essere simulati semplicemente moltiplicando ripetutamente la matrice di transizione con se stessa.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c8e8358 elementor-widget elementor-widget-text-editor" data-id="c8e8358" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Modelli di Markov Nascosti</h2><p>I modelli di Markov nascosti sono modelli di Markov in cui gli stati sono &#8220;nascosti&#8221; alla vista, invece di essere direttamente osservabili. Ci sono una serie di <em>osservazioni</em> di output, relative agli stati, che sono direttamente visibili. Per avere un&#8217;analogia concreta con la finanza quantitativa, è possibile pensare agli stati come a &#8220;regimi&#8221; nascosti in base ai quali si comporta un mercato mentre le osservazioni sono i rendimenti degli asset che sono direttamente visibili.</p><p>In un modello di Markov è necessario solamente creare una funzione di densità congiunta per le osservazioni. È stata specificata una matrice di transizione invariante nel tempo che consente la simulazione completa del modello. Per i Hidden Markov Models è necessario creare un insieme di stati discreti \(z_t \in \{1,\ldots, K \}\) (sebbene ai fini della determinazione del regime sia spesso necessario avere solo \(K \leq 3\)) e modellare le osservazioni con un modello probabilistico aggiuntivo, \(p({\bf x}_t \mid z_t)\). Cioè la probabilità condizionata di vedere una particolare osservazione (rendimento patrimoniale) dato che lo stato (regime di mercato) è attualmente pari a \(z_t\).</p><p>A seconda dello stato specificato e delle probabilità di transizione dell&#8217;osservazione, un modello di Markov nascosto tenderà a rimanere in uno specifico stato e quindi salterà improvvisamente a un nuovo stato e rimarrà in quello stato per qualche tempo. Questo è precisamente il comportamento che si desidera da un tale modello quando si cerca di applicarlo ai regimi di mercato. Non si prevede che i regimi stessi cambino troppo rapidamente (i cambiamenti normativi e altri effetti macroeconomici  sono considerati lenti). Tuttavia, quando cambiano, dovrebbero persistere per qualche tempo.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-1ccd963 elementor-widget elementor-widget-text-editor" data-id="1ccd963" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h4>Specifiche matematiche del modello Markov nascosto</h4><p><span class="">La corrispondente funzione di densità congiunta per l&#8217;HMM è data da (usando di nuovo la notazione di Murphy &#8211; 2012 [8]):</span></p><p style="text-align: center;">\(\begin{eqnarray}p({\bf z}_{1:T} \mid {\bf x}_{1:T}) &amp;=&amp; p({\bf z}_{1:T}) p ({\bf x}_{1:T} \mid {\bf z}_{1:T}) \\<br />&amp;=&amp; \left[ p(z_1) \prod_{t=2}^{T} p(z_t \mid z_{t-1}) \right] \left[ \prod_{t=1}^T p({\bf x}_t \mid z_t) \right]\end{eqnarray}\)</p><p>Nella prima riga si afferma che la probabilità congiunta di vedere l&#8217;insieme completo degli stati nascosti e delle osservazioni è uguale alla probabilità di vedere solo gli stati nascosti moltiplicata per la probabilità di vedere le osservazioni, subordinate agli stati. Questo ha senso in quanto le osservazioni non possono influenzare gli stati, ma gli stati nascosti influenzano indirettamente le osservazioni.</p><p>La seconda riga divide queste due distribuzioni in funzioni di transizione. La funzione di transizione per gli stati è data da \(p(z_t \mid z_{t-1})\) mentre quella per le osservazioni (che dipendono dagli stati) è data da \(p({\bf x}_t \mid z_t)\).</p><p>Come per la descrizione del modello di Markov, ai fini di questo articolo si assume che la funzione di transizione di stato e quella di osservazione siano invarianti nel tempo. Questo significa che è possibile utilizzare la  \(K \times K\) matrice di transizione <em>A</em> di stato in modo analogo al modello di Markov per quella componente del modello.</p><p>Tuttavia, per l&#8217;applicazione qui considerata, ovvero le osservazioni dei rendimenti degli asset, i valori sono continui. Questo significa che la scelta del modello per la funzione di transizione dell&#8217;osservazione è più complessa. La scelta comune è quella di utilizzare una distribuzione gaussiana multivariata condizionale con media \({\bf \mu}_k\) e covarianza \({\bf \sigma}_k\). Questo è formalizzato come segue:</p><p style="text-align: center;">\(\begin{eqnarray}p({\bf x}_t \mid z_t = k, {\bf \theta}) = \mathcal{N}({\bf x}_t \mid {\bf \mu}_k, {\bf \sigma}_k)\end{eqnarray}\)</p><p>Cioè, se lo stato \(z_t\) è attualmente pari a <em>k</em>, allora la probabilità di vedere l&#8217;osservazione \({\bf x}_t\), dati i parametri del modello \(\theta\), è una distribuzione guassianoa multivariata.</p><p>Al fine di rendere più chiari questi passaggi, il seguente diagramma mostra l&#8217;evoluzione degli stati \(z_t\) e come conducano indirettamente all&#8217;evoluzione delle osservazioni,\({\bf x}_t\):</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-b9bfe71 elementor-widget elementor-widget-image" data-id="b9bfe71" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="300" height="114" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-state-model-300x114.png" class="attachment-medium size-medium" alt="trading-quantitativo-hmm-state-model" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-state-model-300x114.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-state-model-160x61.png 160w, https://datatrading.info/wp-content/uploads/trading-quantitativo-hmm-state-model.png 700w" sizes="(max-width: 300px) 100vw, 300px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-d38815f elementor-widget elementor-widget-text-editor" data-id="d38815f" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h4>Filtraggio dei modelli Markov nascosti</h4><p>Dopo aver specificato la funzione di densità congiunta, dobbiamo ora considerare come usare il modello. Nella modellazione generale dello spazio degli stati ci sono spesso tre principali compiti di interesse: filtraggio, smoothing e previsione. Il precedente articolo sui <a href="https://datatrading.info/hedge-ratio-dinamico-tra-coppie-di-etf-utilizzando-il-filtro-di-kalman/">modelli dello spazio degli stati e sul filtro di Kalman</a> li descrive brevemente. Ripetiamoli di seguito per completezza:</p><ul><li><strong>Previsione</strong> &#8211; Previsione dei valori successivi dello stato</li><li><strong>Filtraggio</strong> &#8211; Stima dei valori attuali dello stato da osservazioni passate e attuali</li><li><strong>Smoothing</strong> &#8211; Stima dei valori passati dello stato in base alle osservazioni</li></ul><p>Il filtraggio e lo smoothing sono simili, ma non identici. Lo smoothing ha l&#8217;obbiettivo di capire cosa è successo agli stati in passato  a partire dalle conoscenze attuali, mentre il filtraggio riguarda ciò che sta accadendo allo stato in <em>questo momento</em>.</p><p>Descrivere in dettaglio gli algoritmi sviluppati per il filtraggio, lo smoothing e la previsione va oltre lo scopo di questo articolo. L&#8217;obiettivo principale di questi articoli è applicare i modelli di Markov nascosti per determinare il regime del mercato. Quindi il compito da svolgere diventa determinare quale sia l&#8217;attuale &#8220;stato di regime di mercato&#8221; usando i rendimenti delle attività disponibili fino ad oggi. E&#8217; questa una criticità del filtraggio.</p><p>Matematicamente siamo interessati alla probabilità condizionata dello stato nel tempo <em>t</em> è data la sequenza delle osservazioni fino al momento <em>t</em>, cioè dobbiamo determinare \(p(z_t \mid {\bf x}_{1:T})\). Analogamente al <a href="https://datatrading.info/modelli-dello-spazio-statale-e-filtro-di-kalman/">filtro di Kalman</a>, è possibile applicare ricorsivamente la regola di Bayes per ottenere il filtraggio su un HMM.</p><p> </p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-381332b elementor-widget elementor-widget-text-editor" data-id="381332b" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Prossimi passi</h2><p>Nel secondo articolo della serie per la determinazione del regime degli asset finanziarie descriviamo in modo più approfondito i modelli di markov nascosti. Inoltre, usiamo le librerie del linguaggio Python applicate ai rendimenti storici degli asset al fine di produrre uno strumento per determinare il regime. Quest&#8217;ultimo è usato come strumento di gestione del rischio per il trading quantitativo.</p><h2>Nota bibliografica</h2><p>Una panoramica dei modelli di Markov (così come le loro varie categorizzazioni), inclusi i modelli di Markov nascosti (e gli algoritmi per risolverli), può essere trovata negli articoli introduttivi su Wikipedia [1], [2], [3], [4], [5], [6], [7].</p><p>Una panoramica matematica altamente dettagliata dei modelli di Markov nascosti, con applicazioni a problemi di riconoscimento vocale e l&#8217;algoritmo di Google PageRank, può essere trovata in Murphy (2012) [8]. Bishop (2007) [8] copre un approccio simile a Murphy (2012), inclusa la derivazione della Maximum Likelihood Estimate (MLE) per l&#8217;HMM, nonché gli algoritmi Forward-Backward e Viterbi. La discussione si conclude con Sistemi dinamici lineari e filtri antiparticolato.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-bb6a925 elementor-widget elementor-widget-text-editor" data-id="bb6a925" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Riferimenti</h2>
<ul>
 	<li><a href="https://en.wikipedia.org/wiki/Markov_model" name="ref-wiki-markov-model">[1] (2016). <em>Markov Model</em>, Wikipedia</a></li>
 	<li><a href="https://en.wikipedia.org/wiki/Hidden_Markov_model" name="ref-wiki-hidden-markov-model">[2] (2016). <em>Hidden Markov Model</em>, Wikipedia</a></li>
 	<li><a href="https://en.wikipedia.org/wiki/Markov_decision_process" name="ref-wiki-markov-decision-process">[3] (2016). <em>Markov Decision Process</em>, Wikipedia</a></li>
 	<li><a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process" name="ref-wiki-partially-observable-markov-decision-process">[4] (2016). <em>Partially observable Markov decision process</em>, Wikipedia</a></li>
 	<li><a href="https://en.wikipedia.org/wiki/Markov_chain" name="ref-wiki-markov-chain">[5] (2016). <em>Markov Chain</em>, Wikipedia</a></li>
 	<li><a href="https://en.wikipedia.org/wiki/Forward_algorithm" name="ref-wiki-forward-algo">[6] (2016). <em>Forward Algorithm</em>, Wikipedia</a></li>
 	<li><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" name="ref-wiki-viterbi-algo">[7] (2016). <em>Viterbi Algorithm</em>, Wikipedia</a></li>
 	<li><a href="https://www.cs.ubc.ca/~murphyk/MLbook/" name="ref-murphy">[8] Murphy, K.P. (2012) <em>Machine Learning &#8211; A Probabilistic Perspective</em>, MIT Press</a></li>
 	<li><a href="http://www.cs.ucl.ac.uk/staff/d.barber/brml/" name="ref-barber">[9] Barber, D. (2012) <em>Bayesian Reasoning and Machine Learning</em>, Cambridge University Press</a></li>
 	<li><a href="http://appliedpredictivemodeling.com/" name="ref-kuhn2013">[10] Kuhn, M., Johnson, K. (2013) <em>Applied Predictive Modeling</em>, Springer</a></li>
 	<li><a href="https://en.wikipedia.org/wiki/Q-learning" name="ref-wiki-q-learning">[11] (2016). <em>Q-Learning</em>, Wikipedia</a></li>
 	<li><a href="http://www.nature.com/nature/journal/v518/n7540/pdf/nature14236.pdf" name="ref-deepmindatari2015">[12] Mnih, V. et al (2015) &#8220;Human-level control through deep reinforcement learning&#8221;, <em>Nature</em> <strong>518</strong>: 529–533</a></li>
 	<li><a href="https://arxiv.org/pdf/1606.01540.pdf" name="ref-openaigym2016">[13] Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J., Zaremba, W. (2016) &#8220;OpenAI Gym, <em>arXiv:1606.01540v1 [cs.LG]</em>&#8220;</a></li>
 	<li><a href="http://amzn.to/2crPMo9" name="ref-bishop2007">[14] Bishop, C. (2007) <em>Pattern Recognition and Machine Learning</em>, Springer</a></li>
</ul>					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/introduzione-ai-modelli-di-markov-nascosti/">Introduzione ai Modelli di Markov Nascosti</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
					<wfw:commentRss>https://datatrading.info/introduzione-ai-modelli-di-markov-nascosti/feed/</wfw:commentRss>
			<slash:comments>0</slash:comments>
		
		
			</item>
		<item>
		<title>Test di Johansen per l&#8217;Analisi di Serie Temporali Cointegrate</title>
		<link>https://datatrading.info/test-di-johansen-per-lanalisi-di-serie-temporali-cointegrate/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Tue, 22 Aug 2017 15:37:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Time series analysis]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4719</guid>

					<description><![CDATA[<p>Nel precedente articolo relativo al test CADF (Cointegrated Augmented Dickey Fuller) abbiamo sottolineato che uno dei maggiori inconvenienti del test era la sua applicazione limitata a solo a due serie temporali separate. Tuttavia, possiamo chiaramente immaginare un insieme di tre o più attività finanziarie che potrebbero condividere una sottostante relazione cointegrata. Un semplice esempio potrebbe &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/test-di-johansen-per-lanalisi-di-serie-temporali-cointegrate/"> <span class="screen-reader-text">Test di Johansen per l&#8217;Analisi di Serie Temporali Cointegrate</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/test-di-johansen-per-lanalisi-di-serie-temporali-cointegrate/">Test di Johansen per l&#8217;Analisi di Serie Temporali Cointegrate</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4719" class="elementor elementor-4719">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-827c905 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="827c905" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-8468c64" data-id="8468c64" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-5c4ea2b elementor-widget elementor-widget-text-editor" data-id="5c4ea2b" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Nel <a href="https://datatrading.info/test-di-dickey-fuller-aumentato-e-cointegrato-per-la-valutazione-del-pairs-trading/">precedente articolo relativo al test CADF (Cointegrated Augmented Dickey Fuller)</a> abbiamo sottolineato che uno dei maggiori inconvenienti del test era la sua applicazione limitata a solo a due serie temporali separate. Tuttavia, possiamo chiaramente immaginare un insieme di tre o più attività finanziarie che potrebbero condividere una sottostante relazione cointegrata.</p><p>Un semplice esempio potrebbe essere tre classi di azioni separate sullo stesso asset, mentre un esempio più interessante consiste nel considerare tre ETF separati che replicano determinate aree delle azioni delle materie prime e dei prezzi spot delle materie prime sottostanti.</p><p>In questo articolo descriviamo un test introdotto da Johansen [2] che permette di determinare se tre o più serie temporali sono cointegrate. Determiniamo quindi una serie stazionaria a partire da una combinazione lineare delle serie sottostanti. Tale procedura sarà utilizzata negli articoli successivi per formare un portafoglio di asset mean-reverting a fini di trading.</p><p>Iniziamo descrivendo la teoria alla base del test di Johansen e quindi eseguiamo la procedura di esecuzione del test su dati simulati con proprietà di cointegrazione note a priori. Successivamente applichiamo il test ai dati finanziari storici e vediamo se riusciamo a trovare un portafoglio di asset cointegrati.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8ce43e5 elementor-widget elementor-widget-text-editor" data-id="8ce43e5" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Test di Johannsen</h2><p>In questa sezione introduciamo i concetti matematici alla base dek test di Johansen, che permette di analizzare se due o più serie temporali possono formare una relazione di cointegrazione. Nel trading quantitativo ciò consente di formare un portafoglio di due o più titoli in una strategia di trading mean-reverting.</p><p>I dettagli teorici del test di Johansen richiedono un po&#8217; di esperienza con le serie temporali multivariate. In particolare occorre considerare i <a href="https://en.wikipedia.org/wiki/Vector_autoregression" target="_blank" rel="noopener">Modelli Autoregressivi Vettoriali (VAR)</a> &#8211; da non confondere con il Value at Risk (VaR) &#8211; che sono un&#8217;estensione multidimensionale dei <a href="https://datatrading.info/modelli-a-media-mobile-autoregressiva-armap-q-parte-1/">Modelli Autoregressivi</a> descritti in precedenza.</p><p>Un modello autoregressivo vettoriale generale è simile al modello AR(p) tranne per il fatto che ogni elemento è rappresentato da un vettore  di variabili e i coefficienti sono rappresentati con matrici. La forma generale del modello VAR(p) è data da:</p><p style="text-align: center;">\(\begin{eqnarray}{\bf x_t} = {\bf \mu} + A_1 {\bf x_{t-1}} + \ldots + A_p {\bf x_{t-p}} + {\bf w_t}\end{eqnarray}\)</p><p>Dove \({\bf \mu}\) è la funzione vettoriale delle intercette delle serie, \(A_i\) sono le matrici dei coefficienti per ogni lag e \({\bf w_t}\) è un termine di rumore gaussiano multivariato con media zero.<br />A questo punto possiamo formare un <a href="https://en.wikipedia.org/wiki/Error_correction_model" target="_blank" rel="noopener">modello di correzione degli errori dei vettori (VECM)</a> differenziando le serie:</p><p style="text-align: center;">\(\begin{eqnarray}\Delta {\bf x_t} = {\bf \mu} + A {\bf x_{t-1}} + \Gamma_1 \Delta {\bf x_{t-1}} + \ldots + \Gamma_p \Delta {\bf x_{t-p}} + {\bf w_t}\end{eqnarray}\)</p><p>dove \(\Delta {\bf x_t} := {\bf x_t} &#8211; {\bf x_{t-1}}\) è l&#8217;operatore differenziale, \(A\) è la matrice dei coefficienti per il primo  lag e \(\Gamma_i\) sono le matrici per ogni lag differenziato.</p><p>Il test verifica la presenza di non cointegrazione, che avviene quando la matrice A=0.</p><p>Il test di Johansen è più flessibile della procedura CADF descritta nell&#8217;<a href="https://datatrading.info/test-di-dickey-fuller-aumentato-e-cointegrato-per-la-valutazione-del-pairs-trading/">articolo precedente</a> e può verificare la presenza di più combinazioni lineari di serie temporali per la costruzione di portafogli stazionari. Per raggiungere questo scopo è necessario effettuare una autodecomposizione di A, cioè la scomposizioni in autovalori. Il range della matrice A è dato da <strong>r </strong>e il test di Johansen verifica in sequenza se questo range è uguale a zero, uguale a uno, fino a \(r=n-1\), dove n è il numero di serie temporali in esame.<br />L&#8217;ipotesi nulla di \(r=0\) significa che non c&#8217;è alcuna cointegrazione. Un grado \(r \gt 0\) implica una relazione di cointegrazione tra due o forse più serie temporali.</p><p>La scomposizione degli autovalori produce un insieme di autovettori. Le componenti dell&#8217;autovettore più grande hanno le proprietà per essere i candidati a formare i coefficienti di una combinazione lineare di serie temporali per produrre un portafoglio stazionario. Si noti come questo differisca dal test CADF (spesso noto come procedura di Engle-Granger) in cui è necessario accertare la combinazione lineare a priori tramite  la regressione lineare e minimi quadrati ordinari (OLS).</p><p>Nel test di Johansen i valori della combinazione lineare sono stimati <em>come parte del test</em>, ciò implica che il test ha una minore potenza statistica rispetto al CADF. È possibile imbattersi in situazioni in cui non ci sono prove sufficienti per rifiutare l&#8217;ipotesi nulla di mancata cointegrazione nonostante la CADF suggerisca diversamente. Vedremo di seguito questi casi.</p><p>Forse il modo migliore per comprendere il test di Johansen è applicarlo a dati finanziari sia simulati che storici.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-bea46cc elementor-widget elementor-widget-text-editor" data-id="bea46cc" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Test di Johansen su dati simulati</h2>
Ora che abbiamo descritto le basi teoriche del test, lo applichiamo utilizzando Python. In particolare utilizziamo la libreria <code>statsmodels</code>, che implementa il test di Johansen con la funzione <code>coint_johansen</code>.

Il primo compito è importare le librerie necessarie. Come nei precedenti articoli impostiamo il seed in modo che i risultati del generatore di numeri casuali possano essere replicati, quindi creiamo la passeggiata casuale sottostante \(z_t\). Infine creiamo le tre serie temporali che condividono la stessa passeggiata casuale sottostante. Sono indicati rispettivamente come \(p_t\), \(q_t\) e \(r_t\):					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-447684a elementor-widget elementor-widget-code-highlight" data-id="447684a" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import numpy as np
import pandas as pd
import statsmodels.tsa.vector_ar.vecm as vecm

np.random.seed(123)
n = 1000
z = np.zeros(n)
for i in range(1, n):
    z[i] = z[i-1] + np.random.standard_normal(1)

p = 0.3 * z + np.random.standard_normal(1000)
q = 0.6 * z + np.random.standard_normal(1000)
r = 0.2 * z + np.random.standard_normal(1000)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-78eaae9 elementor-widget elementor-widget-text-editor" data-id="78eaae9" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Chiamiamo quindi la funzione <code>coint_johansen</code> applicata a un frame di dati di tutte e tre le serie temporali.</p><p>Questa funzione prevede ulteriori pamametri:</p><ul><li>det_order &#8211; indica se utilizzare un termine costante o un trend lineare</li><li>k &#8211; indica il numero di ritardi da utilizzare nel modello </li></ul><p>Infine, stampiamo il riepilogo dell&#8217;output:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-4b07ff3 elementor-widget elementor-widget-code-highlight" data-id="4b07ff3" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>df = pd.DataFrame({'p':p, 'q':q, 'r':r})
res = vecm.coint_johansen(df, det_order=0, k_ar_diff=1)

output = pd.DataFrame([res.lr1],  index=["trace_stat"], columns=['r=0', 'r<=1', 'r<=2'])
print(output.T, '\n')

cvt = pd.DataFrame(res.cvt, index=["r=0", "r<=1", "r<=2"], columns=["90%", "95%", "99%"])
print("Critical values(90%, 95%, 99%) of trace_stat\n", cvt, '\n')

print("Eigenvalues (lambda):\n")
print(res.eig, "\n")

print("Eigenvectors:")
evec = pd.DataFrame(res.evec).T
print(evec)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-2162d59 elementor-widget elementor-widget-code-highlight" data-id="2162d59" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>      trace_stat
r=0   631.340645
r<=1  278.465131
r<=2    0.887798 

Critical values(90%, 95%, 99%) of trace_stat
           90%      95%      99%
r=0   27.0669  29.7961  35.4628
r<=1  13.4294  15.4943  19.9349
r<=2   2.7055   3.8415   6.6349 

Eigenvalues (lambda):

[0.29808104 0.24301555 0.00089007] 

Eigenvectors
          0         1         2
0  1.613432 -0.835372  0.141379
1 -0.327201 -0.366202  1.600709
2  0.073222  0.097455  0.031615</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-512ba11 elementor-widget elementor-widget-text-editor" data-id="512ba11" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Proviamo ad interpretare tutte queste informazioni! La prima sezione mostra la statistica del test &#8216;trace&#8217; per le tre ipotesi \(r=0\), \(r \leq 1\) e \(r \leq 2\). Per ognuno di questi tre test abbiamo non solo la statistica stessa ma anche i valori critici a determinati livelli di confidenza: rispettivamente 10%, 5% e 1%.</p><p>La prima ipotesi,\(r = 0\), verifica la presenza di cointegrazione. Poiché la statistica del test supera significativamente il livello dell&#8217;1% (631.34&gt;35.46) abbiamo forti prove per rifiutare l&#8217;ipotesi nulla di mancata cointegrazione. La seconda prova per \(r \leq 1\) contro l&#8217;ipotesi alternativa \(r \gt 1\) fornisce anch&#8217;essa prove chiare per rifiutare \(r \leq 1\) poiché la statistica del test supera significativamente il livello dell&#8217;1%. Anche la prova finale per \(r \leq 2\) contro \(r \gt 2\) fornisce prove sufficienti per rifiutare l&#8217;ipotesi nulla che \(r \leq 2\)  e quindi si può concludere che il rango della matrice è maggiore di 2.</p><p>Da quanto sopra la migliore stima del rango della matrice è \(r=3\), che implica la necessità di considerare una combinazione lineare di tre serie temporali per formare una serie stazionaria. Questo è prevedibile, per definizione della serie, poiché la passeggiata casuale sottostante, utilizzata per tutte e tre le serie, non è stazionaria.</p><p>Come facciamo a formare una combinazione lineare di questo tipo? La risposta è utilizzare le componenti autovettoriali dell&#8217;autovettore associato all&#8217;autovalore più grande.</p><p>La successiva sezione del nostro output mostra gli autovalori e gli autovettori normalizzati restituiti dal test. Notiamo che l&#8217;autovalore più grande è circa 0,298. Il corrispondente vettore  è indicato sotto la colonna 0 ed è approssimativamente uguale a (1.61343, -0.83533, 0.14138). Se formiamo una combinazione lineare di serie utilizzando questi componenti, ricaviamo una serie stazionaria:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-7889c9a elementor-widget elementor-widget-code-highlight" data-id="7889c9a" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>s = 1.61343164*p - 0.83537214*q + 0.14137947*r

import matplotlib.pyplot as plt
plt.plot(s)
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-7f8b2ac elementor-widget elementor-widget-image" data-id="7f8b2ac" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="442" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-johansen-plots.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-johansen-plots" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-johansen-plots.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-johansen-plots-300x189.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-johansen-plots-160x101.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a138ca6 elementor-widget elementor-widget-text-editor" data-id="a138ca6" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Visivamente sembra molto simile a una serie stazionaria. Possiamo effettuare il test Augmented Dickey-Fuller (ADF) come controllo aggiuntivo:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-77d5b26 elementor-widget elementor-widget-code-highlight" data-id="77d5b26" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from arch.unitroot import *

adf = ADF(s)
print(adf.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-650d3b2 elementor-widget elementor-widget-code-highlight" data-id="650d3b2" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>   Augmented Dickey-Fuller Results   
=====================================
Test Statistic                -20.546
P-value                         0.000
Lags                                2
-------------------------------------

Trend: Constant
Critical Values: -3.44 (1%), -2.86 (5%), -2.57 (10%)
Null Hypothesis: The process contains a unit root.
Alternative Hypothesis: The process is weakly stationary.
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-5693197 elementor-widget elementor-widget-text-editor" data-id="5693197" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>La statistica del test di Dickey-Fuller è molto bassa, dato che abbiamo un p-value basso. Quindi possiamo rifiutare l&#8217;ipotesi nulla di una radice unitaria e abbiamo la prova di una serie stazionaria formata da una combinazione lineare.</p><p>Questo non dovrebbe sorprenderci in quanto, per costruzione, l&#8217;insieme di serie è stato progettato per formare una combinazione lineare stazionaria. Tuttavia, è istruttivo seguire i test su dati simulati poiché ci aiuta nell&#8217;analisi dei dati finanziari reali, come nel prossimo capitolo.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-ff7f0fa elementor-widget elementor-widget-text-editor" data-id="ff7f0fa" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Test di Johansen sui dati finanziari</h2><p>In questa sezione esamineremo due serie separate di panieri di ETF: EWA, EWC e IGE, nonché SPY, IVV e VOO.</p><h4>EWA, EWC e IGE</h4><p>Nell&#8217;<a href="https://datatrading.info/test-di-dickey-fuller-aumentato-e-cointegrato-per-la-valutazione-del-pairs-trading/">articolo precedente</a> abbiamo esaminato il lavoro di Ernest Chan [1] sulla cointegrazione tra i due ETF di EWA ed EWC, che rappresentano panieri di azioni rispettivamente per le economie australiana e canadese.</p><p>Chan descrive anche il test di Johansen come mezzo per aggiungere un terzo ETF al mix, ovvero l&#8217;IGE, che contiene un paniere di stock di risorse naturali. L&#8217;idea di base è che questi ETF dovrebbero subire l&#8217;influenza delle tendenze stocastiche delle merci e quindi possono formare una relazione di cointegrazione.</p><p>Nel suo lavoro, Chan ha eseguito la procedura di Johansen utilizzando MatLab e ha potuto respingere l&#8217;ipotesi \(r \leq 2\) al livello del 5%. Questo implica che ha trovato prove a sostegno dell&#8217;esistenza di una combinazione lineare stazionaria di EWA, CAE e IGE.</p><p>Sarebbe utile vedere se possiamo replicare i risultati usando Python. Iniziamo utilizzando la libreria <code>yfinance</code> per scaricare i dati storici delle serie finanziare e quindi effettuare il test di Johansen con la libreria <code>statsmodels.</code></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-4d090e1 elementor-widget elementor-widget-code-highlight" data-id="4d090e1" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import pandas as pd
import yfinance as yf

EWA = yf.download('EWA', start="2006-04-26", end="2012-04-09")
EWC = yf.download('EWC', start="2006-04-26", end="2012-04-09")
IGE = yf.download('IGE', start="2006-04-26", end="2012-04-09")

df = pd.DataFrame({'EWA': EWA['Adj Close'], 'EWC': EWC['Adj Close'], 'IGE': IGE['Adj Close']})

res = vecm.coint_johansen(df, 0, 1) 
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-b4e4c73 elementor-widget elementor-widget-text-editor" data-id="b4e4c73" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				I risultati del test sono i seguenti:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a06f567 elementor-widget elementor-widget-code-highlight" data-id="a06f567" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>      trace_stat
r=0    34.781746
r<=1   16.940157
r<=2    4.417218 

Critical values(90%, 95%, 99%) of trace_stat
           90%      95%      99%
r=0   27.0669  29.7961  35.4628
r<=1  13.4294  15.4943  19.9349
r<=2   2.7055   3.8415   6.6349 

Eigenvalues (lambda):

[0.01183963 0.00832493 0.0029444 ] 

Eigenvectors
        EWA       EWC       IGE
0  1.487722 -1.417733  0.321943
1 -0.227340 -0.691618  0.687187
2  0.108149  0.389697 -0.143024</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-b322c49 elementor-widget elementor-widget-text-editor" data-id="b322c49" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Forse la prima cosa che si nota è la differenza tra i valori della statistica Trace del nostro test e quelli forniti nel pacchetto MatLab utilizzato da Chan.
La statistica Trace del nostro test è sostanzialmente simile per \(r \leq 2\) a 4,417 contro 4,471 dei risultati di Chan, mentre i valori critici sono molto simili. Fondamentalmente, la statistica Trace del test è maggiore rispetto al livello del 5%, quindi possiamo concludere che dovremmo
avere tre relazioni di cointegrazione con una certezza del 95%.

Da sottolineare che non c&#8217;è differenza tra i due insiemi di serie temporali utilizzati per ciascuna analisi! L&#8217;unica differenza risiede nell&#8217;implementazioni del test di Johansen, che sono diverse tra MatLab e Python. Ciò significa che dobbiamo essere estremamente attenti quando valutiamo i risultati dei test statistici, in particolare tra diverse implementazioni e linguaggi di programmazione.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8718ad6 elementor-widget elementor-widget-text-editor" data-id="8718ad6" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h4>SPY, IVV e VOO</h4><p>Un altro approccio consiste nel considerare un paniere di ETF che replicano un indice azionario. Ad esempio, ci sono una moltitudine di ETF che replicano l&#8217;indice del mercato azionario statunitense S&amp;P500 come Standard &amp; Poor&#8217;s Depository Receipts SPY, iShares IVV e VOO di Vanguard. Dato che replicano tutti lo stesso asset sottostante, è probabile che questi tre ETF abbiano una forte relazione di cointegrazione.</p><p>Otteniamo i prezzi di chiusura giornalieri rettificati per ciascuno di questi ETF nell&#8217;ultimo anno:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-1b32511 elementor-widget elementor-widget-code-highlight" data-id="1b32511" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import pandas as pd
import yfinance as yf

SPY = yf.download('SPY', start="2016-01-01", end="2016-12-31")
IVV = yf.download('IVV', start="2016-01-01", end="2016-12-31")
VOO = yf.download('VOO', start="2016-01-01", end="2016-12-31")

df = pd.DataFrame({'SPY': SPY['Adj Close'], 'IVV': IVV['Adj Close'], 'VOO': VOO['Adj Close']})

res = vecm.coint_johansen(df, 0, 1)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-33febeb elementor-widget elementor-widget-code-highlight" data-id="33febeb" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>      trace_stat
r=0   100.207059
r<=1    9.629058
r<=2    0.715191 

Critical values(90%, 95%, 99%) of trace_stat
           90%      95%      99%
r=0   27.0669  29.7961  35.4628
r<=1  13.4294  15.4943  19.9349
r<=2   2.7055   3.8415   6.6349 

Eigenvalues (lambda):

[0.30292937 0.03489021 0.00284531] 

Eigenvectors
         EWA       EWC        IGE
0  35.138330 -0.602150 -37.360558
1  -3.685629  4.317242  -0.574631
2   1.040785 -0.255254  -0.964393
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-5951ebf elementor-widget elementor-widget-text-editor" data-id="5951ebf" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Eseguiamo in sequenza i test di ipotesi iniziando con l&#8217;ipotesi nulla di \(r = 0\) contro l&#8217;ipotesi alternativa di \(r \gt 0\). Ci sono prove evidenti per rifiutare l&#8217;ipotesi nulla al livello dell&#8217;1% e possiamo probabilmente concludere che \(r \gt 0\).</p><p>Allo stesso modo verifichiamo l&#8217;ipotesi nulla \(r \leq 1\) contro l&#8217;ipotesi alternativa \(r \gt 1\). Abbiamo prove sufficienti per rifiutare l&#8217;ipotesi nulla al livello dell&#8217;1% e possiamo concludere che \(r \gt 1\).</p><p>Infine, possiamo rifiuturare l&#8217;ipotesi nulla \(r \leq 2\) contro l&#8217;ipotesi alternativa \(r \gt 2\) e quindi possiamo concludere che \(r=3\). Ciò significa che possiamo formare una combinazione lineare di tutte e 3 gli asset per un portafoglio di cointegrazione.</p><p>Inoltre, dovremmo essere estremamente cauti nell&#8217;interpretare questi risultati poiché abbiamo utilizzato solo un anno di dati, ovvero circa 250 giorni di negoziazione. È improbabile che un campione così piccolo fornisca una rappresentazione fedele delle relazioni sottostanti. Quindi bisogna stare sempre attenti nell&#8217;interpretare i test statistici!</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-5049082 elementor-widget elementor-widget-text-editor" data-id="5049082" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Prossimi passi</h2><p>Negli ultimi articoli abbiamo trattato una moltitudine di test statistici per rilevare la stazionarietà tra combinazioni di serie temporali come precursori per creare strategie di trading mean-reverting.</p><p>In particolare abbiamo esaminato il test Augmented Dickey-Fuller, Phillips-Perron, Phillips-Ouliaris, Cointegrated Augmented Dickey-Fuller e il test di Johansen.</p><p>Siamo ora in grado di applicare questi test a strategie di ritorno verso la media. Per fare ciò effettuiamo un backtest realistico usando <a href="https://github.com/datatrading-info/DataTrader" target="_blank" rel="noopener">DataTrader</a> in Python per queste strategie. Descriviamo questi test negli articoli successivi.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a20afba elementor-widget elementor-widget-text-editor" data-id="a20afba" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Riferimenti</h2><ul><li><a href="https://amzn.to/3MDxKPX" name="ref-chan">[1] Chan, E. P. (2013) <em>Algorithmic Trading: Winning Strategies and their Rationale</em>, Wiley</a></li><li><a href="https://www.jstor.org/stable/2938278" name="ref-johansen">[3] Johansen, S. (1991) <em>Estimation and Hypothesis Testing of Cointegration Vectors in Gaussian Vector Autoregressive Models</em>, Econometrica <strong>59</strong> (6): 1551-1580</a></li></ul>					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/test-di-johansen-per-lanalisi-di-serie-temporali-cointegrate/">Test di Johansen per l&#8217;Analisi di Serie Temporali Cointegrate</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Test di Dickey Fuller Aumentato e Cointegrato per la Valutazione del Pairs Trading</title>
		<link>https://datatrading.info/test-di-dickey-fuller-aumentato-e-cointegrato-per-la-valutazione-del-pairs-trading/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Fri, 18 Aug 2017 10:06:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Time series analysis]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4672</guid>

					<description><![CDATA[<p>Nel precedente articolo sulla cointegrazione abbiamo simulato due serie temporali non stazionarie che formavano una coppia cointegrata per una specifica combinazione lineare. Abbiamo utilizzato i test statistici Augmented Dickey-Fuller, Phillips-Perron e Phillips-Ouliaris per verificare la presenza di radici unitarie e di cointegrazione. Purtroppo il test ADF non ci fornisce il parametro di regressione &#8211; l&#8217;hedge &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/test-di-dickey-fuller-aumentato-e-cointegrato-per-la-valutazione-del-pairs-trading/"> <span class="screen-reader-text">Test di Dickey Fuller Aumentato e Cointegrato per la Valutazione del Pairs Trading</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/test-di-dickey-fuller-aumentato-e-cointegrato-per-la-valutazione-del-pairs-trading/">Test di Dickey Fuller Aumentato e Cointegrato per la Valutazione del Pairs Trading</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4672" class="elementor elementor-4672">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-abde932 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="abde932" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-d1c5dac" data-id="d1c5dac" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-be0ef78 elementor-widget elementor-widget-text-editor" data-id="be0ef78" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Nel<a href="https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/"> precedente articolo sulla cointegrazione</a> abbiamo simulato due serie temporali non stazionarie che formavano una coppia cointegrata per una specifica combinazione lineare. Abbiamo utilizzato i test statistici Augmented Dickey-Fuller, Phillips-Perron e Phillips-Ouliaris per verificare la presenza di radici unitarie e di cointegrazione.</p><p>Purtroppo il test ADF non ci fornisce il parametro di regressione \(\beta\) &#8211; l&#8217;hedge ratio &#8211; necessario per ottenere la combinazione lineare delle due serie temporali. In questo articolo descriviamo la procedura Cointegrated Augmented Dickey-Fuller (CADF), che tenta di risolvere questo problema. Abbiamo già introdotto il CADF nell&#8217;ambito della <a href="https://datatrading.info/testing-statistico-della-mean-reversion-parte-ii/">modellazione statistica</a>.</p><p>Il CADF ci aiuta a identificare il coefficiente di regressione \(\beta\) per le nostre due serie ma non può identificare quale delle due serie è la variabile dipendente o indipendente per la regressione. Cioè, il valore della &#8220;risposta&#8221; \(Y\) a partire dalla &#8220;caratteristica&#8221; \(X\), nell&#8217;ambito del machine learning statistico. Descriviamo quindi come evitare questo problema calcolando la statistica del test ADF e usandola per determinare quale delle due regressioni produce una serie stazionaria.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8a4bd59 elementor-widget elementor-widget-text-editor" data-id="8a4bd59" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Test di Dickey Fuller aumentato e cointegrato</h2>
Lo scopo principale del test CADF è determinare un rapporto di copertura (hedge-ratio) ottimale da utilizzare tra due serie nel trading di ritorno alla media, che era un problema identificato nell&#8217;analisi  descritta nel <a href="https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/">precedente articolo</a>. In particole, ci aiuta a determinare il rapporto long-short di ciascuna coppia quando si effettua un pairs trading

Il CADF è una procedura relativamente semplice. Prendiamo i dati storici dei due asset e eseguiamo una regressione lineare tra di loro, si ottiene i coefficienti di regressione \(\alpha\) e \(\beta\), che rappresentano rispettivamente l&#8217;intercetta e la pendenza. In particolare, il valore della pendenza ci aiuta a determinare la quantità relativa da tradare per ciascuna coppia.

Dopo aver ottenuto il coefficiente di pendenza &#8211; il rapporto di copertura &#8211; possiamo eseguire un test ADF (come nell&#8217;<a href="https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/">articolo precedente</a>) sui residui della regressione lineare in modo da determinare l&#8217;evidenza di stazionarietà e quindi di cointegrazione.

Usiamo Python per eseguire la procedura CADF, utilizzando le librerie <code>arch</code>, <code>stasmodels</code> e <code>yfinance</code> rispettivamente per il test ADF e l&#8217;acquisizione dei dati storici.

Iniziamo costruendo un set di dati sintetici, con note proprietà di cointegrazione, per vedere se la procedura CADF può recuperare la stazionarietà e il rapporto di copertura. Applichiamo quindi la stessa analisi ad alcuni dati storici reali, come anticipazione dell&#8217;implementazione di alcune strategie di trading di mean-reverting.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-f48ec6a elementor-widget elementor-widget-text-editor" data-id="f48ec6a" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>CADF sui dati simulati</h2><p>Descriviamo l&#8217;approccio CADF sui dati simulati. Usiamo le stesse serie temporali simulate dell&#8217;<a href="https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/">articolo precedente</a>.</p><p>Ricordiamo che abbiamo creato artificialmente due serie temporali non stazionarie che formano una serie residua stazionaria per una specifica combinazione lineare.</p><p>Possiamo utilizzare la funzione <code>OLS</code> della libreria <code>stasmodels</code> per eseguire una regressione lineare tra le due serie. Otteniamo una stima dei coefficienti di regressione e quindi il rapporto di copertura ottimale tra le due serie.</p><p>Iniziamo importando le librerie necessarie per il test ADF:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c639f19 elementor-widget elementor-widget-code-highlight" data-id="c639f19" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import numpy as np

import statsmodels.api as sm
from statsmodels.tsa.stattools import adfuller
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-97e35ca elementor-widget elementor-widget-text-editor" data-id="97e35ca" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Dato che vogliamo usare le stesse serie stocastiche con un trend sottostante dell&#8217;articolo precedente, impostiamo il seed per il generatore di numeri casuali e creiamo una serie storica stocastica a partire da una random walk sottostante, \(z_t\). Quindi creiamo due serie che rinominiamo \(p_t\) e \(q_t\) in modo da non confonderci i nomi originali \(x_t\) e \(y_t\) con i nomi convenzionali per le regressioni delle risposte e dei predittori:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a4c05e0 elementor-widget elementor-widget-code-highlight" data-id="a4c05e0" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>np.random.seed(123)
n = 1000

z = np.zeros(n)
for i in range(1, n):
    z[i] = z[i-1] + np.random.standard_normal(1)

p = 0.3 * z + np.random.standard_normal(1000)
q = 0.6 * z + np.random.standard_normal(1000)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-594ba1f elementor-widget elementor-widget-text-editor" data-id="594ba1f" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				A questo punto possiamo utilizzare la funzione <code>OLS</code>, che calcola una regressione lineare tra due vettori. In questo caso impostiamo \(q_t\) come variabile indipendente e \(p_t\) come variabile dipendente:</code>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-159a823 elementor-widget elementor-widget-code-highlight" data-id="159a823" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>q = sm.add_constant(q)

comb = sm.OLS(p, q)

results = comb.fit()
print(results.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-0deb07e elementor-widget elementor-widget-text-editor" data-id="0deb07e" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Se diamo un&#8217;occhiata al modello di regressione lineare, possiamo vedere che la stima per il coefficiente di regressione \(\beta\) è di circa 0,5. Questo valore è accettabile dato che \(q_t\) dipende due volte da \(z_t\) rispetto a \(\)p_t[/latex (0,6 rispetto a 0,3):</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-244e489 elementor-widget elementor-widget-code-highlight" data-id="244e489" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R-squared:                       0.909
Model:                            OLS   Adj. R-squared:                  0.909
Method:                 Least Squares   F-statistic:                     9924.
Date:                     28 Oct 2017   Prob (F-statistic):               0.00
Time:                        18:04:56   Log-Likelihood:                -1489.2
No. Observations:                1000   AIC:                             2982.
Df Residuals:                     998   BIC:                             2992.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const         -0.1331      0.046     -2.871      0.004      -0.224      -0.042
x1             0.4795      0.005     99.620      0.000       0.470       0.489
==============================================================================
Omnibus:                        1.077   Durbin-Watson:                   2.003
Prob(Omnibus):                  0.584   Jarque-Bera (JB):                0.952
Skew:                          -0.029   Prob(JB):                        0.621
Kurtosis:                       3.140   Cond. No.                         13.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-37e502e elementor-widget elementor-widget-text-editor" data-id="37e502e" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Infine, applichiamo il test ADF ai residui del modello lineare per verificare la stazionarietà:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8216d7d elementor-widget elementor-widget-code-highlight" data-id="8216d7d" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from arch.unitroot import *

resid = results.resid
adf = ADF(resid)
print(adf.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-7907ce1 elementor-widget elementor-widget-text-editor" data-id="7907ce1" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>La statistica del test Dickey-Fuller è molto bassa, fornendoci un P-value molto piccolo. </span><span class="goog-text-highlight">Possiamo probabilmente rifiutare l&#8217;ipotesi nulla della presenza di una radice unitaria e concludere che abbiamo una serie stazionaria e quindi una coppia cointegrata. </span><span>Questo chiaramente conferma la bontà dei dati che abbiamo simulato per avere queste proprietà.</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-05220ca elementor-widget elementor-widget-code-highlight" data-id="05220ca" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>   Augmented Dickey-Fuller Results   
=====================================
Test Statistic                -20.356
P-value                         0.000
Lags                                2
-------------------------------------

Trend: Constant
Critical Values: -3.44 (1%), -2.86 (5%), -2.57 (10%)
Null Hypothesis: The process contains a unit root.
Alternative Hypothesis: The process is weakly stationary.

Process finished with exit code 0

</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-e41054c elementor-widget elementor-widget-text-editor" data-id="e41054c" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Vediamo ora come applicare la procedura CADF a diversi set di dati finanziari storici.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-b98cf75 elementor-widget elementor-widget-text-editor" data-id="b98cf75" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>CADF sui dati finanziari</h2><p>Esistono molti modi per formare un set di asset cointegrati. Una approccio comune è utilizzare ETF che presentano caratteristiche simili. Un buon esempio è un ETF che rappresenta un paniere di società di estrazione dell&#8217;oro abbinato a un ETF che segue il prezzo spot dell&#8217;oro. Allo stesso modo si può usare il petrolio greggio o qualsiasi altra  commodity.</p><p>Un&#8217;alternativa consiste nel formare coppie di cointegrazione più strette considerando classi di azioni separate sullo stesso titolo, come la Royal Dutch Shell nell&#8217;esempio seguente. Un altro esempio è la famosa Berkshire Hathaway holding, gestita da Warren Buffet e Charlie Munger, che si divide in azioni A e B. Tuttavia, in questo caso dobbiamo prestare attenzione perché dobbiamo verificare se siamo in grado di formare una strategia di mean-reverting redditizia su una tale coppia, a seconda di quanto sia stretta la cointegrazione.</p><h4>EWA e EWC</h4><p>Nella comunità quantistica un famoso esempio del test CADF applicato ai dati azionari è fornito da Ernie Chan[1]. Si considera una coppia cointegrata da due ETF, con i simboli ticker EWA ed EWC, che rappresentano rispettivamente panieri di titoli azionari australiani e canadesi. Dato che entrambi questi paesi sono fortemente basati sulle materie prime è probabile che questi ETF avranno una simile tendenza stocastica di fondo.</p><p>Ernie fa uso di MatLab per il suo lavoro, ma questo è un articolo su Python. Quindi ho pensato che sarebbe stato istruttivo utilizzare le stesse date di inizio e fine della sua analisi storica per vedere come si confrontano i risultati.</p><p>Il primo compito è importare la libreria <code>yfinance</code>, che ci sarà utile per scaricare i dati finanziari da Yahoo Finance, quindi otteniamo i prezzi di chiusura aggiustati per EWA ed EWC per il periodo esatto utilizzato nel lavoro di Ernie &#8211; dal 26 aprile 2006 al 9 aprile 2012:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6ef948c elementor-widget elementor-widget-code-highlight" data-id="6ef948c" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import yfinance as yf

EWA = yf.download('EWA', start="2006-04-26", end="2012-04-09")
EWC = yf.download('EWC', start="2006-04-26", end="2012-04-09")

EWA_adjcls = EWA['Adj Close']
EWC_adjcls = EWC['Adj Close']</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-fc7c508 elementor-widget elementor-widget-text-editor" data-id="fc7c508" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Per completezza replichiamo i grafici del lavoro di Ernie, in modo che da vedere lo stesso codice nell&#8217;ambiente Python. In primo luogo, tracciamo i prezzi degli stessi ETF.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a5cdf84 elementor-widget elementor-widget-code-highlight" data-id="a5cdf84" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import matplotlib.pyplot as plt

plt.plot(EWA_adjcls, 'b')
plt.plot(EWC_adjcls, 'r')
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-72944c5 elementor-widget elementor-widget-image" data-id="72944c5" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="347" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-prices.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-cadf-plot-ewa-ewc-prices" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-prices.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-prices-300x149.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-prices-160x79.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
				<section class="elementor-section elementor-top-section elementor-element elementor-element-2406714 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="2406714" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-ab2a81d" data-id="ab2a81d" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-d6afb2b elementor-widget elementor-widget-text-editor" data-id="d6afb2b" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Possiamo notare che differisce leggermente dal grafico riportato nel lavoro di Ernie dato che in stiamo usando i prezzi rettificati, invece dei prezzi di chiusura non rettificati.

Possiamo anche creare un grafico a dispersione dei prezzi:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-7acad30 elementor-widget elementor-widget-code-highlight" data-id="7acad30" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>plt.scatter(EWA_adjcls.values, EWC_adjcls.values)
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-d42bb24 elementor-widget elementor-widget-image" data-id="d42bb24" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="350" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-scatter.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-cadf-plot-ewa-ewc-scatter" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-scatter.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-scatter-300x150.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-scatter-160x80.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-98ed001 elementor-widget elementor-widget-text-editor" data-id="98ed001" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>A questo punto è necessario ottenere le regressioni lineari tra le due serie dei prezzi. Tuttavia, come detto in precedenza, non è chiaro quale serie sia la variabile dipendente e quale sia la variabile indipendente per la regressione. Quindi proviamo entrambi le ipotesi e facciamo una scelta in base alla negatività della statistica del test ADF.</p><p>Usiamo la funzione del modello lineare OLS per la valutare la regressione. In questo modo otteniamo l&#8217;intercetta e il coefficiente di regressione per queste coppie.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c17d8fb elementor-widget elementor-widget-code-highlight" data-id="c17d8fb" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import statsmodels.api as sm

comb1 = sm.OLS(EWC_adjcls, sm.add_constant(EWA_adjcls)).fit()
comb2 = sm.OLS(EWA_adjcls, sm.add_constant(EWC_adjcls)).fit()

print(comb1.summary())
print(comb2.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-3867e14 elementor-widget elementor-widget-text-editor" data-id="3867e14" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Possiamo tracciare i residui e valutare visivamente la stazionarietà della serie:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c5bd0b0 elementor-widget elementor-widget-code-highlight" data-id="c5bd0b0" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>resid1 = comb1.resid
plt.plot(resid1)
plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-550711d elementor-widget elementor-widget-image" data-id="550711d" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="347" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-resids1.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-cadf-plot-ewa-ewc-resids1" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-resids1.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-resids1-300x149.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cadf-plot-ewa-ewc-resids1-160x79.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-f3c81cb elementor-widget elementor-widget-text-editor" data-id="f3c81cb" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Di seguito vediamo il report della regressione lineare con EWA come variabile indipendente:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-0c09861 elementor-widget elementor-widget-code-highlight" data-id="0c09861" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>OLS Regression Results                            
==============================================================================
Dep. Variable:          EWC Adj Close   R-squared:                       0.921
Model:                            OLS   Adj. R-squared:                  0.921
Method:                 Least Squares   F-statistic:                 1.744e+04
Date:                     28 Oct 2017   Prob (F-statistic):               0.00
Time:                        19:35:22   Log-Likelihood:                -2135.7
No. Observations:                1500   AIC:                             4275.
Df Residuals:                    1498   BIC:                             4286.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const             3.3329      0.132     25.180      0.000       3.073       3.593
EWA Adj Close     1.3922      0.011    132.047      0.000       1.372       1.413
==============================================================================
Omnibus:                       59.177   Durbin-Watson:                   0.049
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               65.369
Skew:                           0.508   Prob(JB):                     6.39e-15
Kurtosis:                       3.116   Cond. No.                         64.4
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-d38873c elementor-widget elementor-widget-text-editor" data-id="d38873c" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Vediamo che il modello restituisce un&#8217;intercetta \(\alpha=3.3329\) e un coefficiente \(\beta=1.3922\).</p><p>Allo stesso modo, vediamo di seguito il report per EWC come variabile indipendente:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a7e80cb elementor-widget elementor-widget-code-highlight" data-id="a7e80cb" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>                            OLS Regression Results                            
==============================================================================
Dep. Variable:          EWA Adj Close   R-squared:                       0.921
Model:                            OLS   Adj. R-squared:                  0.921
Method:                 Least Squares   F-statistic:                 1.744e+04
Date:                     28 Oct 2017   Prob (F-statistic):               0.00
Time:                        19:41:27   Log-Likelihood:                -1577.6
No. Observations:                1500   AIC:                             3159.
Df Residuals:                    1498   BIC:                             3170.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P>|t|      [0.025      0.975]
---------------------------------------------------------------------------------
const            -1.2306      0.104    -11.822      0.000      -1.435      -1.026
EWC Adj Close     0.6615      0.005    132.047      0.000       0.652       0.671
==============================================================================
Omnibus:                       24.830   Durbin-Watson:                   0.051
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               23.805
Skew:                          -0.273   Prob(JB):                     6.77e-06
Kurtosis:                       2.713   Cond. No.                         121.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-eade8ba elementor-widget elementor-widget-text-editor" data-id="eade8ba" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>In questo caso il modello restituisce un&#8217;intercetta\(\alpha=-1.2306\) e un coefficiente \(\beta=0.6615\).</p><p>La criticità fondamentale è la notevole differenza dei valori dei coefficienti di regressione tra i due modelli. Dobbiamo quindi utilizzare la statistica del test ADF per determinare il rapporto di copertura (hedge-ratio) ottimale.</p><p>Per EWA come variabile indipendente otteniamo:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c3ba79d elementor-widget elementor-widget-code-highlight" data-id="c3ba79d" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>adf1 = ADF(comb1.resid, lags=1)
print(adf1.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-faf23d1 elementor-widget elementor-widget-code-highlight" data-id="faf23d1" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>   Augmented Dickey-Fuller Results   
=====================================
Test Statistic                 -3.650
P-value                         0.005
Lags                                1
-------------------------------------

Trend: Constant
Critical Values: -3.43 (1%), -2.86 (5%), -2.57 (10%)
Null Hypothesis: The process contains a unit root.
Alternative Hypothesis: The process is weakly stationary.</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-616abbf elementor-widget elementor-widget-text-editor" data-id="616abbf" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>La statistica del test restituisce un p-value inferiore a 0,05 fornendo la prova che possiamo rifiutare l&#8217;ipotesi nulla di una radice unitaria al livello del 5%.</p><p>Allo stesso modo per EWC come variabile indipendente ottienamo:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c54d964 elementor-widget elementor-widget-code-highlight" data-id="c54d964" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>adf2 = ADF(comb2.resid, lags=1)
print(adf2r.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-58010c2 elementor-widget elementor-widget-code-highlight" data-id="58010c2" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>  Augmented Dickey-Fuller Results   
=====================================
Test Statistic                 -3.655
P-value                         0.005
Lags                                1
-------------------------------------

Trend: Constant
Critical Values: -3.43 (1%), -2.86 (5%), -2.57 (10%)
Null Hypothesis: The process contains a unit root.
Alternative Hypothesis: The process is weakly stationary.</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-928587c elementor-widget elementor-widget-text-editor" data-id="928587c" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Anche in questo abbiamo prove per rifiutare l&#8217;ipotesi nulla della presenza di una radice unitaria, portando all&#8217;evidenza di una serie stazionaria (e coppia cointegrata) al livello del 5%.

La statistica del test ADF per EWC come variabile indipendente è  leggermente più piccola (più negativa) di quella per EWA come variabile indipendente e quindi la consideriamo come la nostra combinazione lineare per qualsiasi futura applicazione di trading.					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8d477bf elementor-widget elementor-widget-text-editor" data-id="8d477bf" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Prossimi passi</h2><p>Abbiamo utilizzato il CADF per ottenere il rapporto di copertura ottimale per due serie temporali cointegrate. Negli articoli successivi prenderemo in considerazione il test di Johansen, che ci consentirà di formare serie temporali di cointegrazione per più di due asset, fornendo un universo di trading molto più ampio da cui scegliere strategie.</p><p>Inoltre, considereremo il fatto che il rapporto di copertura stesso non è stazionario e come tale utilizzeremo tecniche per aggiornare il nostro rapporto di copertura quando arrivano nuove informazioni. Possiamo utilizzare l&#8217;approccio bayesiano del filtro di Kalman per questo.</p><p>Dopo aver esaminato questi test, li applicheremo a una serie di strategie di trading integrate in <a href="https://github.com/datatrading-info/DataTrader" target="_blank" rel="noopener">DataTrader</a> e vedremo come si comportano con costi di transazione realistici.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-c651ffd elementor-widget elementor-widget-text-editor" data-id="c651ffd" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Riferimenti</h2><ul><li><a href="https://amzn.to/3MDxKPX" name="ref-chan">[1] Chan, E. P. (2013) <em>Algorithmic Trading: Winning Strategies and their Rationale</em>, Wiley</a></li></ul>					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/test-di-dickey-fuller-aumentato-e-cointegrato-per-la-valutazione-del-pairs-trading/">Test di Dickey Fuller Aumentato e Cointegrato per la Valutazione del Pairs Trading</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Analisi delle Serie Temporali Cointegrate per il Trading Mean-Reverting</title>
		<link>https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Sun, 13 Aug 2017 13:58:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Time series analysis]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4618</guid>

					<description><![CDATA[<p>In un precedente articolo abbiamo descritto una strategia di trading basata sull&#8217;applicazione dei modelli ARIMA e GARCH delle serie temporali ai dati giornalieri dell&#8217;S&#38;P500 e abbiamo preannunciato, così come negli altri articoli della serie sull&#8217;analisi delle serie temporali, l&#8217;uso di questi modelli nelle strategie di trading mean-reverting e come costruirli. In questo articolo introduciamo un &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/"> <span class="screen-reader-text">Analisi delle Serie Temporali Cointegrate per il Trading Mean-Reverting</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/">Analisi delle Serie Temporali Cointegrate per il Trading Mean-Reverting</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4618" class="elementor elementor-4618">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-fe147a8 elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="fe147a8" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-7b1989a" data-id="7b1989a" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-953fec2 elementor-widget elementor-widget-text-editor" data-id="953fec2" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>In un precedente articolo abbiamo descritto una strategia di trading basata sull&#8217;applicazione dei <a href="https://datatrading.info/strategia-di-trading-arimagarch-sullindice-sp500/" class="broken_link">modelli ARIMA e GARCH delle serie temporali ai dati giornalieri dell&#8217;S&amp;P500</a> e abbiamo preannunciato, così come negli altri <a href="https://datatrading.info/tutorial/data-science/">articoli della serie sull&#8217;analisi delle serie temporali</a>, l&#8217;uso di questi modelli nelle strategie di trading mean-reverting e come costruirli.</p><p>In questo articolo introduciamo un tema chiamato <a href="https://en.wikipedia.org/wiki/Cointegration" target="_blank" rel="noopener">cointegrazione</a>, un concetto delle serie temporale che consente di determinare se possiamo formare una coppia di asset  mean-reverting, cioè di ritorno medio. Descriviamo la teoria delle serie temporali relativa alla cointegrazione e successivamente vediamo come applicarla a reali strategie di trading utilizzando il nostro framework di backtesting open source: <a href="https://github.com/datatrading-info/DataTrader">DataTrader</a>.</p><p>Procediamo descrivendo il ritorno alla media nel tradizionale quadro del &#8220;pairs-trading&#8221;.  Inoltre introduciamo il concetto di <em>stazionarietà</em> di una combinazione lineare di asset, arrivando infine a definire la <em>cointegrazione</em> e ai <em>test di radice unitaria</em>. Dopo aver delineato questi test, simuliamo varie serie temporali con Python e applichiamo i test per valutare la cointegrazione.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-e8b1176 elementor-widget elementor-widget-text-editor" data-id="e8b1176" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Strategie di trading di reversione media</h2><p>L&#8217;idea base di un mean-reverting &#8220;pairs-trading&#8221; è quella di aprire contemporaneamente posizioni long e short su due diversi asset. Questi due asset devono condividere fattori sottostanti che influenzano i loro movimenti. Un esempio dal mondo azionario potrebbe essere long su McDonald&#8217;s (NYSE:MCD) e short su Burger King (NYSE:BKW &#8211; prima della fusione con Tim Horton&#8217;s).</p><p>La logica di questo approccio consiste nell&#8217;elevata probabilità che i prezzi dei due asset tornano all&#8217;equilibrio nel lungo periodo a causa degli ampi fattori di mercato che influenzano la produzione e il consumo di hamburger. Un disequilibrio a breve termine di un asset della coppia, come un&#8217;interruzione della catena di approvvigionamento che colpisce esclusivamente McDonald&#8217;s, porterebbe a una temporanea dislocazione dei loro prezzi relativi. Un&#8217;operazione long-short effettuata in questo punto di disequilibrio potrebbe diventare redditizia poiché i due asset tornano al loro valore di equilibrio una volta risolta l&#8217;interruzione. Questa è l&#8217;essenza del classico &#8220;pairs-trading&#8221;.</p><p>Come quants, siamo interessati a fare trading di mean-reverting non solo su una <em>coppia</em> di azioni, ma anche su <em>panieri</em> di azioni che sono separatamente interconnesse.</p><p>Per raggiungere questo obiettivo abbiamo bisogno di un solido quadro matematico per identificare coppie o panieri di azioni che rientrano nel comportamento sopra descritto. È qui che nasce il concetto di serie temporali cointegrate.</p><p>L&#8217;idea è quella di considerare una coppia di serie temporali non stazionarie, come le attività simili alla camminata casuale di MCD e BKW, e formare una <em>combinazione lineare</em> di ciascuna serie per produrre una serie stazionaria, che abbia una media e una varianza fisse.</p><p>Questa serie stazionaria può avere interruzioni a breve termine in cui il valore si allontana dalla media, ma a causa della sua stazionarietà questo valore alla fine tornerà alla media. Le strategie di trading possono farne uso allungando/cortocircuitando la coppia nel punto di interruzione appropriato e scommettendo su un ritorno a lungo termine della serie alla sua media.</p><p>Le strategie di mean-reverting consentono di creare serie temporali stazionarie &#8220;sintetiche&#8221; per<span style="font-size: 15px; font-style: normal; font-weight: 400;"> un&#8217;ampia gamma di strumenti</span><span style="font-size: 15px;">. Non siamo certamente limitati alle azioni &#8220;vanilla&#8221;. Ad esempio, possiamo utilizzare gli Exchange Traded Fund (ETF) che tracciano i prezzi delle materie prime, come il petrolio greggio, e i panieri delle società produttrici di petrolio. Ci sono molte opportuna per per identificare tali sistemi di mean-reverting.</span></p><p>Prima di approfondire gli effettivi meccanismi delle strategie di trading, che sono oggetto di articoli successivi, dobbiamo capire come identificare statisticamente le serie <em>cointegrate</em>. A tale scopo usiamo le tecniche dell&#8217;analisi delle <a href="https://datatrading.info/tutorial/data-science/">serie temporali</a>, continuando l&#8217;uso del linguaggio Python come nei <a href="https://datatrading.info/tutorial/data-science/">precedenti articoli sull&#8217;argomento</a>.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-d98ab99 elementor-widget elementor-widget-text-editor" data-id="d98ab99" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Cointegrazione</h2><p>Dopo aver motivato la necessità di un framework quantitativo per effettuare il mean reversion trading, possiamo definire il concetto di cointegrazione. Consideriamo una coppia di serie temporali, entrambe non stazionarie, allora esiste una particolare combinazione lineare di queste due serie che può portare ad una serie stazionaria. Tale coppia di serie è definita <em>cointegrata</em> .</p><p>La definizione matematica è data da:</p><blockquote><p><strong>Cointegrazione</strong><br />Date  due serie temporali non stazionarie, con \(a, b \in \mathbb<span style="font-size: 16px; font-style: normal; font-weight: 400;">\(\{x_t\}\) e \(\{y_t\}\)</span><span style="font-size: 16px;">{R}\) costanti. Se la serie combinata \(</span><span style="font-size: 16px;">a x_t + b y_t\) è stazionaria, allora \(\{x_t\}\) e \(\{y_t\}\) sono cointegrati.</span></p></blockquote><p> <br />Sebbene la definizione sia utile, non ci fornisce direttamente un meccanismo per determinare i valori di a e b, e verificare se la combinazione sia statisticamente stazionaria. Per quest&#8217;ultima verifica dobbiamo utilizzare i test per <em>le radici unitarie</em>.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-fe036a5 elementor-widget elementor-widget-text-editor" data-id="fe036a5" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Test di radice unitaria</h2><p>Nel precedente articolo sui<a href="https://datatrading.info/modelli-a-media-mobile-autoregressiva-armap-q-parte-1/"> modelli autoregressivi AR(p)</a> abbiamo descritto il ruolo <em>dell&#8217;equazione caratteristica</em>. Abbiamo notato che si trattava semplicemente di un modello autoregressivo impostato a zero, scritto in forma di spostamento all&#8217;indietro. Risolvere questa equazione fornisce un insieme di <em>radici</em> .</p><p>Affinché il modello sia considerato stazionario, tutte le radici dell&#8217;equazione devono superare l&#8217;unità. Un modello AR(p) con una radice uguale ad una unità &#8211; una <em>radice unitaria</em> &#8211; non è stazionario. Le passeggiate casuali sono processi AR(1) con radici unitarie e quindi sono non stazionarie.</p><p>Pertanto, al fine di rilevare se una serie temporale è stazionaria o meno, possiamo costruire un test di ipotesi statistica per la presenza di una radice unitaria in un campione di serie temporali.</p><p>Consideriamo tre test separati per le radici unitarie: Augmented Dickey-Fuller (AFD), Phillips-Perron e Phillips-Ouliaris. Vediamo che si basano su ipotesi diverse, ma tutti e tre effettuato verifiche per lo stesso problema, cioè la stazionarietà del campione di serie temporali.</p><p>Descriviamo brevemente gli approcci di questi metodi.</p><h4>Test di Dickey-Fuller aumentato</h4><p>Dickey e Fuller [2] hanno introdotto il seguente test per la presenza di una radice unitaria. Il test originale considera una serie temporale \(z_t = \alpha z_{t-1} + w_t\), dove \(w_t\) è un rumore bianco discreto. L&#8217;ipotesi nulla è \(\alpha = 1\), mentre l&#8217;ipotesi alternativa è \(\alpha &lt; 1\).</p><p>Said e Dickey [6] hanno migliorato il test originale Dickey-Fuller arrivando alla definizione del test Augmented Dickey-Fuller (ADF), in cui la serie \(z_t\) viene modificato in un modello AR(p) a partire da un modello AR(1). abbiamo descritto questo test in un <a href="https://datatrading.info/testing-statistico-della-mean-reversion/">articolo</a> dove abbiamo usato Python per calcolare il test ADF. In questo articolo effettuiamo lo stesso test.</p><h4>Test di Phillips-Perron</h4><p>Il test ADF presuppone un modello AR(p) come approssimazione per il campione di serie temporali e lo utilizza per considerare le autocorrelazioni di ordine superiore. Il test di Phillips-Perron [5] non prevede un&#8217;approssimazione del modello AR(p). In questo caso un metodo non parametrico di smoothing del kernel viene applicato sul processo stazionario \(w_t\), che gli consente di tenere conto dell&#8217;autocorrelazione e dell&#8217;eteroscedasticità non specificate.</p><h4>Phillips-Ouliaris Test</h4><p>The Phillips-Ouliaris test[4] is different from the previous two tests in that it is testing for evidence of cointegration among the residuals between two time series. The main idea here is that tests such as ADF, when applied to the estimated cointegrating residuals, do not have the Dickey-Fuller distributions under the null hypothesis where cointegration isn&#8217;t present. Instead, these distributions are known as Phillips-Ouliaris distributions and hence this test is more appropriate.</p><h4>Difficoltà con gli Unit Root Test</h4><p>Nonostante il test ADF e Phillips-Perron sono asintoticamente equivalenti, possono produrre risposte molto diverse su campioni finiti [7] . Questo si verifica perché gestiscono l&#8217;autocorrelazione e l&#8217;eteroscedasticità in modo diverso. È necessario avere molto chiari quali ipotesi si vogliono verificate quando si applicano questi test e non applicarli semplicemente alla cieca su serie arbitrarie.</p><p>Inoltre, i test di radice unitaria non sono adattati a distinguere processi stazionari altamente persistenti da processi non stazionari. Bisogna stare molto attenti quando si usano questi test su determinate forme di serie temporali finanziarie. Ciò può essere particolarmente problematico quando la relazione sottostante che è stata modellata (ad es, il ritorno alla media di due coppie simili) si interrompe naturalmente a causa di un cambio di regime o di altri cambiamenti strutturali nei mercati finanziari.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-5089186 elementor-widget elementor-widget-text-editor" data-id="5089186" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Simulazione di serie temporali cointegrate con Python</h2><p>Applichiamo ora i precedenti unit root test ad alcuni dati simulati che sappiamo essere cointegrati. Possiamo utilizzare la definizione di cointegrazione per creare artificialmente due serie temporali non stazionarie che condividono una tendenza stocastica sottostante, ma con una combinazione lineare stazionaria.<br />Il nostro primo compito è definire una passeggiata casuale \(z_t = z_{t-1} + w_t\), dove \(w_t\) è un rumore bianco discreto.<br /><em>Per rispolverare questi concetti si può dare un&#8217;occhiata al <a href="https://datatrading.info/white-noise-e-random-walks-nellanalisi-delle-serie-temporali/">precedente articolo sul rumore bianco e le passeggiate casuali</a></em></p><p>Con la passeggiata casuale \(z_t\) creiamo due nuove serie temporali \(x_t\) e \(y_t\) che condividono lo stesso sottostante trend stocastico di \(z_t\), anche se in quantità diverse:</p><p style="text-align: center;">\(\begin{eqnarray}x_t &amp;=&amp; p z_t + w_{x,t} \\<br />y_t &amp;=&amp; q z_t + w_{y,t}\end{eqnarray}\)</p><p>Se consideriamo una combinazione lineare \(a x_t + b y_t\), otteniamo:</p><p style="text-align: center;">\(\begin{eqnarray}a x_t + b y_t &amp;=&amp; a (p z_t + w_{x,t}) + b (q z_t + w_{y,t}) \\<br />&amp;=&amp; (ap + bq) z_t + a w_{x,t} + b w_{y,t}\end{eqnarray}\)</p><p>Da notare che otteniamo una serie stazionaria (che è una combinazione di termini di rumore bianco) nel caso \(ap + bq = 0\). Possiamo valorizzare numericamente le variabili per renderlo più concreto, ad esempio supponiamo \(p=0.3\) e \(q=0.6\). Dopo qualche semplice operazione algebrica otteniamo \(ap + bq = 0\) per \(a=2\) e \(b=-1\), portando a una combinazione di serie stazionaria. Quindi \(x_t\) e \(y_t\) sono cointegrati quando \(a=2\) e \(b=-1\).</p><p>Vediamo ora come simulare queste serie temporali con Python e visualizzare la combinazione stazionaria. In primo luogo, desideriamo creare e tracciare la serie di passeggiate casuali sottostante, \(z_t\):</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-56dd7a2 elementor-widget elementor-widget-code-highlight" data-id="56dd7a2" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import numpy as np

np.random.seed(1)
steps = np.random.standard_normal(1000)
steps[0] = 0
random_walk = np.cumsum(steps)

import matplotlib.pyplot as plt
plt.figure(figsize=[10, 7.5]); # Dimensioni del grafico
plt.plot(random_walk)
plt.title("Simulated Random Walk")
plt.show()
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-9c0f524 elementor-widget elementor-widget-image" data-id="9c0f524" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="348" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-cointegrated-time-series-random-walks" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks-300x149.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks-160x80.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2496db1 elementor-widget elementor-widget-text-editor" data-id="2496db1" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Se tracciamo il correlogramma della serie e quello delle sue differenze possiamo vedere poche evidenze di autocorrelazione:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-f0e71c7 elementor-widget elementor-widget-code-highlight" data-id="f0e71c7" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from statsmodels.graphics.tsaplots import plot_acf

plot_acf(random_walk, alpha=0.05, lags=30)
plot_acf(np.diff(random_walk), alpha=0.05, lags=30)
plt.show()
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-a0b79f9 elementor-widget elementor-widget-image" data-id="a0b79f9" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="642" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks-acf.png" class="attachment-large size-large" alt="trading-quantitativo-cointegrated-time-series-random-walks-acf" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks-acf.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks-acf-300x275.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-random-walks-acf-160x147.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2fec4a4 elementor-widget elementor-widget-text-editor" data-id="2fec4a4" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				Da quanto sopra questa realizzazione di \(z_t\) sembra chiaramente una passeggiata casuale. Il prossimo passo è creare \(x_t\) e \(y_t\) a partire da \(z_t\), usando \(p=0.3\) e \(q=0.6\), quindi tracciare i grafici di entrambe le serie:					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a163043 elementor-widget elementor-widget-code-highlight" data-id="a163043" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>p=0.3
q=0.6

x_t = p*random_walk + np.random.standard_normal(1000)
y_t = q*random_walk + np.random.standard_normal(1000)

fig, axs = plt.subplots(2) # Definezione di due grafici
axs[0].plot(x_t)
axs[0].set(ylabel='x_t')
axs[1].plot(y_t)
axs[1].set(ylabel='y_t')

plt.show()
</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-7de28d8 elementor-widget elementor-widget-image" data-id="7de28d8" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="348" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-x_t_y_t.png" class="attachment-large size-large" alt="trading-quantitativo-cointegrated-time-series-x_t_y_t" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-x_t_y_t.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-x_t_y_t-300x149.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-x_t_y_t-160x80.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-d2a001d elementor-widget elementor-widget-text-editor" data-id="d2a001d" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Possiamo notare come grafici sono molto simili. Ovviamente lo sono per definizione: condividono la stessa struttura sottostrante di camminata casuale di \(z_t\).<br />Calcoliamo ora la combinazione lineare usando \(p=2\) e \(q=-1\) ed esaminiamo la struttura di autocorrelazione:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-93c3216 elementor-widget elementor-widget-code-highlight" data-id="93c3216" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>
comb = 2*x_t - y_t

fig, axs = plt.subplots(2) # Definezione di due grafici
axs[0].plot(comb)
axs[0].set(ylabel='comb')
plot_acf(comb, alpha=0.05, lags=30, ax=axs[1])
axs[1].set(ylabel='acf')

plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-5df7ce5 elementor-widget elementor-widget-image" data-id="5df7ce5" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="339" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-comb-lin-x-y.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-cointegrated-time-series-comb-lin-x-y" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-comb-lin-x-y.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-comb-lin-x-y-300x145.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-cointegrated-time-series-comb-lin-x-y-160x77.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-a490923 elementor-widget elementor-widget-text-editor" data-id="a490923" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>È evidente che la serie combinata </span><code>comb</code> <span>assomiglia molto a una serie stazionaria. Questo era prevedibile dalla sua definizione.</span></p><p><span>Proviamo ad applicare i tre test della radice unitaria alla serie di combinazioni lineari. In primo luogo, il test Augmented Dickey-Fuller:</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-f861faf elementor-widget elementor-widget-code-highlight" data-id="f861faf" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from arch.unitroot import ADF

adf = ADF(comb)
print(adf.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-8675063 elementor-widget elementor-widget-code-highlight" data-id="8675063" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>Augmented Dickey-Fuller Results   
=====================================
Test Statistic                -31.547
P-value                         0.000
Lags                                0
-------------------------------------

Trend: Constant
Critical Values: -3.44 (1%), -2.86 (5%), -2.57 (10%)
Null Hypothesis: The process contains a unit root.
Alternative Hypothesis: The process is weakly stationary.</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-390e531 elementor-widget elementor-widget-text-editor" data-id="390e531" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Il P-value è piccolo e quindi abbiamo prove per rifiutare l&#8217;ipotesi nulla che la serie possieda una radice unitaria. </p><p>Proviamo il test di Phillips-Perron:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6d97ba3 elementor-widget elementor-widget-code-highlight" data-id="6d97ba3" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from arch.unitroot import PhillipsPerron

php = PhillipsPerron(comb)
print(php.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-dac4c11 elementor-widget elementor-widget-code-highlight" data-id="dac4c11" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>Phillips-Perron Test (Z-tau)    
=====================================
Test Statistic                -31.584
P-value                         0.000
Lags                               22
-------------------------------------

Trend: Constant
Critical Values: -3.97 (1%), -3.41 (5%), -3.13 (10%)
Null Hypothesis: The process contains a unit root.
Alternative Hypothesis: The process is weakly stationary.</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-77a3749 elementor-widget elementor-widget-text-editor" data-id="77a3749" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>Anche in questo caso  abbiamo un piccolo P-value quindi abbiamo prove per rifiutare l&#8217;ipotesi nulla di una radice unitaria. </span></p><p><span>Infine, proviamo il test di Phillips-Ouliaris (questo test richiede in input la matrice dei costituenti della serie sottostante):</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-79944d2 elementor-widget elementor-widget-code-highlight" data-id="79944d2" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>from arch.unitroot.cointegration import phillips_ouliaris

po = phillips_ouliaris(2*x_t, -1*y_t)
print(po.summary())</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-230b9da elementor-widget elementor-widget-code-highlight" data-id="230b9da" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-default copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-markup ">
				<code readonly="true" class="language-markup">
					<xmp>Phillips-Ouliaris Zt Cointegration Test
=====================================
Test Statistic                -31.129
P-value                         0.000
Kernel                       Bartlett
Bandwidth                       8.186
-------------------------------------

Trend: Constant
Critical Values: -3.51 (10%), -3.79 (5%), -4.35 (1%)
Null Hypothesis: No Cointegration
Alternative Hypothesis: Cointegration
Distribution Order: 4</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-d755570 elementor-widget elementor-widget-text-editor" data-id="d755570" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Ancora una volta vediamo un P-vaule piccolo e quindi possiamo rifiutare l&#8217;ipotesi nulla. E&#8217; sufficiente per dimostrare che abbiamo una coppia di serie cointegrate.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-690fee9 elementor-widget elementor-widget-text-editor" data-id="690fee9" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2><span>Prossimi passi</span></h2><p><span>In questo articolo abbiamo esaminato alcuni test di radice unitaria per valutare se una combinazione lineare di serie temporali sia stazionaria, cioè se le due serie siano cointegrate.</span></p><p><span>Negli articoli futuri prenderemo in considerazione l&#8217;implementazione completa delle strategie di trading di ritorno alla media per i dati giornalieri di azioni ed ETF utilizzando DataTrader sulla base di questi test di cointegrazione.</span></p><p><span>Inoltre estenderemo la nostra analisi alla cointegrazione a più di due asset, ottenendo strategie di trading che sfruttano portafogli cointegrati.</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-3713aa5 elementor-widget elementor-widget-text-editor" data-id="3713aa5" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2><span>Riferimenti</span></h2><ul><li><a href="http://amzn.to/1RQaOYF" name="ref-itsr">[1] Cowpertwait, P.S.P., Metcalfe, A.V. (2009) <em>Introductory Time Series with R</em>, Springer</a></li><li><a href="https://www.jstor.org/stable/2286348" name="ref-dickeyfuller">[2] Dickey, D.A., Fuller, W.A. (1979) &#8220;Distribution of the Estimators for Autoregressive Time Series with a Unit Root&#8221;, <em>Journal of the American Statistical Association</em> <strong>74</strong> (366): 427-431</a></li><li><a href="https://www.amazon.co.uk/Analysis-Integrated-Cointegrated-Time-Use/dp/0387759662/" name="ref-aictsr">[3] Pfaff, B. (2010) <em>Analysis of Integrated and Cointegrated Time Series with R, 2nd Ed.</em>, Springer</a></li><li><a href="https://www.jstor.org/stable/2938339" name="ref-phillipsouliaris">[4] Phillips, P.C.B., Ouliaris, S. (1990) &#8220;Asymptotic Properties of Residual Based Tests for Cointegration&#8221;, <em>Econometrica</em> <strong>58</strong> (1): 165-193</a></li><li><a href="http://biomet.oxfordjournals.org/content/75/2/335.abstract" name="ref-phillipsperron">[5] Phillips, P.C.B., Perron, P. (1988) &#8220;Testing for a Unit Root in Time Series Regression&#8221;, <em>Biometrika</em> <strong>75</strong> (2): 335–346</a></li><li><a href="http://biomet.oxfordjournals.org/content/71/3/599.abstract" name="ref-saiddickey">[6] Said, S.E., Dickey, D.A. (1984) &#8220;Testing for Unit Roots in Autoregressive-Moving Average Models of Unknown Order&#8221;, <em>Biometrika</em> <strong>71</strong> (3): 599-607</a></li><li><a href="http://faculty.washington.edu/ezivot/econ584/econ584.htm" name="ref-zivot">[7] Zivot, E. (2006) Economics 584: Time Series Economics, <em>Course Notes</em></a></li></ul>					</div>
						</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/analisi-delle-serie-temporali-cointegrate-per-il-trading-mean-reverting/">Analisi delle Serie Temporali Cointegrate per il Trading Mean-Reverting</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
		<item>
		<title>Hedge Ratio Dinamico tra coppie di ETF utilizzando il Filtro di Kalman</title>
		<link>https://datatrading.info/hedge-ratio-dinamico-tra-coppie-di-etf-utilizzando-il-filtro-di-kalman/</link>
		
		<dc:creator><![CDATA[Gianluca]]></dc:creator>
		<pubDate>Tue, 08 Aug 2017 19:12:00 +0000</pubDate>
				<category><![CDATA[Tutorial Data Science]]></category>
		<category><![CDATA[Tutorial Time series analysis]]></category>
		<guid isPermaLink="false">https://datatrading.info/?p=4572</guid>

					<description><![CDATA[<p>Una tecnica di quant trading prevede di considerare due asset che hanno una relazione di cointegrazione e l&#8217;utilizzo di un approccio mean-reverting per costruire una strategia di trading. Questo può essere effettuato eseguendo una regressione lineare tra i due asset (come una coppia di ETF) e utilizzare la regressione per determinare la quantità di ciascuna &#8230;</p>
<p class="read-more"> <a class="" href="https://datatrading.info/hedge-ratio-dinamico-tra-coppie-di-etf-utilizzando-il-filtro-di-kalman/"> <span class="screen-reader-text">Hedge Ratio Dinamico tra coppie di ETF utilizzando il Filtro di Kalman</span> Leggi tutto »</a></p>
<p>L'articolo <a rel="nofollow" href="https://datatrading.info/hedge-ratio-dinamico-tra-coppie-di-etf-utilizzando-il-filtro-di-kalman/">Hedge Ratio Dinamico tra coppie di ETF utilizzando il Filtro di Kalman</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></description>
										<content:encoded><![CDATA[		<div data-elementor-type="wp-post" data-elementor-id="4572" class="elementor elementor-4572">
						<div class="elementor-inner">
				<div class="elementor-section-wrap">
									<section class="elementor-section elementor-top-section elementor-element elementor-element-3db9bbc elementor-section-boxed elementor-section-height-default elementor-section-height-default" data-id="3db9bbc" data-element_type="section">
						<div class="elementor-container elementor-column-gap-default">
							<div class="elementor-row">
					<div class="elementor-column elementor-col-100 elementor-top-column elementor-element elementor-element-2e9159a" data-id="2e9159a" data-element_type="column">
			<div class="elementor-column-wrap elementor-element-populated">
							<div class="elementor-widget-wrap">
						<div class="elementor-element elementor-element-7485aac elementor-widget elementor-widget-text-editor" data-id="7485aac" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Una tecnica di quant trading prevede di considerare due asset che hanno una relazione di cointegrazione e l&#8217;utilizzo di un approccio mean-reverting per costruire una strategia di trading. Questo può essere effettuato eseguendo una regressione lineare tra i due asset (come una coppia di ETF) e utilizzare la regressione per determinare la quantità di ciascuna asset per andare long e short con determinate soglie.</p><p>Una delle principali criticità di tale strategia è la variazione nel tempo di qualsiasi parametro introdotto attraverso questa relazione strutturale, come il rapporto di copertura tra i due asset. Cioè, i parametri non sono fissi durante tutto il periodo di attività della strategia. Al fine di migliorare la redditività  è utile poter disporre di un meccanismo di aggiustamento del rapporto di copertura nel tempo.</p><p>Per risolvere questo problema si può utilizzare una regressione lineare mobile in una finestra di ricerca (o periodo temporale), cioè è necessario aggiornare la regressione lineare ad ogni nuova barra in modo che i termini di pendenza e di intercetta &#8220;seguano&#8221; l&#8217;ultimo valore osservato della relazione di cointegrazione. Tuttavia, in questo modo si introduce  nella strategia un altro parametro variabile, ovvero la lunghezza della finestra di ricerca. Questo parametro deve essere ottimizzato, ad esempio tramite <a href="https://datatrading.info/la-cross-validation-per-ottimizzare-il-machine-learning/">la convalida incrociata</a> .</p><p>Un approccio più sofisticato consiste nell&#8217;utilizzare un <a href="https://datatrading.info/modelli-dello-spazio-statale-e-filtro-di-kalman/">modello dello spazio degli stati</a>, dove si considera il &#8220;vero&#8221; rapporto di copertura come una variabile nascosta non osservata e tenta di stimarlo con osservazioni &#8220;rumorose&#8221;, cioè i dati dei prezzi di ogni asset.</p><p>Il filtro Kalman esegue esattamente questo compito. In un <a href="https://www.quantstart.com/articles/State-Space-Models-and-the-Kalman-Filter">articolo precedente</a> abbiamo descritto in modo approfondito il filtro di Kalman e la sua applicazione come un processo di aggiornamento bayesiano.</p><p>In questo articolo usiamo il filtro Kalman, tramite la libreria <a href="https://pykalman.github.io/" target="_blank" rel="noopener">pykalman</a> di Python, per aiutarci a stimare dinamicamente la pendenza e l&#8217;intercettazione (e quindi il rapporto di copertura) tra una coppia di ETF.</p><p>Questa tecnica è infine testata con il <a href="https://github.com/datatrading-info/DataTrader">sistema di trading DataTrader</a>, che ci consente di verificare le prestazioni di una questa strategia negli ultimi anni.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-79d673f elementor-widget elementor-widget-text-editor" data-id="79d673f" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Breve riepilogo del filtro di Kalman</h2><p>Per capire le basi matematiche si può leggere l&#8217;<a href="https://datatrading.info/modelli-dello-spazio-statale-e-filtro-di-kalman/">articolo precedente</a>, dove  le formule del Filtro di Kalman sono descritte in modo approfondito. Di seguito riassumiamo brevemente i punti chiave qui.</p><p>In questo caso usiamo un modello dello spazio degli stati composto da due equazioni matriciali. La prima è nota come equazione di <em>stato</em> o di <em>transizione</em> e descrive come un insieme di variabili di stato,\(\theta_t\), vengono modificati da un periodo di tempo all&#8217;altro. La dipendenza lineare dallo stato precedente è rappresentata da matrice di transizione \(G_t\) e dal <em>rumore di sistema normalmente distribuito </em>\(w_t\). Da notare che \(G=G_t\) , cioè la matrice di transizione è essa stessa dipendente dal tempo:</p><p style="text-align: center;">\(\begin{eqnarray}\theta_t = G_t \theta_{t-1} + w_t\end{eqnarray}\)</p><p>Tuttavia, questi stati sono spesso non osservabili ed ogni intervallo di tempo potremmo avere accesso solo alle <em>osservazioni</em>, descritte da \(y_t\).  Le osservazioni sono associate ad un&#8217;equazione di osservazione che include una componente lineare tramite la matrice di osservazione \(F_t\), oltre ad un <em>rumore di misura</em> normalmente distribuito dato da \(v_t\).</p><p style="text-align: center;">\(\begin{eqnarray}y_t = F_t \theta_t + v_t \end{eqnarray}\)</p><p>Per maggiori dettagli sul modello dello spazio degli stati e sul filtro di Kalman, fare riferimento al <a href="https://datatrading.info/modelli-dello-spazio-statale-e-filtro-di-kalman/">precedente articolo</a> .</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-01d3f6b elementor-widget elementor-widget-text-editor" data-id="01d3f6b" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Incorporare la regressione lineare in un filtro di Kalman</h2><p>La domanda principale in questa fase è come utilizziamo questo modello dello spazio degli stati per incorporare le informazioni in una regressione lineare?</p><p>Come descritto nell&#8217;articolo sul <a href="https://datatrading.info/metodo-della-massima-verosimiglianza-per-la-regressione-lineare/">MLE per la regressione lineare</a>, una regressione lineare multipla prevede che il valore in uscita \(y\)  è una funzione lineare delle componenti in input \(x\):</p><p style="text-align: center;">\(\begin{eqnarray}y({\bf x}) = \beta^T {\bf x} + \epsilon\end{eqnarray}\)</p><p>dove \(\beta^T = (\beta_0, \beta_1, \ldots, \beta_p)\) rappresenta il vettore di trasposizione dell&#8217;intercetta \(\beta_0\) e  pendenze \(\beta_i\), insieme a \(\epsilon \sim \mathcal{N}(\mu, \sigma^2)\) che rappresenta il termine di errore.</p><p>Poiché siamo in un ambiente unidimensionale, possiamo semplicemente scrivere \(\beta^T = (\beta_0, \beta_1)\) e \({\bf x} = \begin{pmatrix} 1 \\ x \end{pmatrix}\).</p><p>Consideriamo gli stati (nascosti) del sistema in modo che siano rappresentati dal vettore \(\beta^T\), ovvero l&#8217;intercetta e la pendenza della regressione lineare. Il passaggio successivo consiste nell&#8217;assumere che l&#8217;intercetta e la pendenza di domani siano uguali all&#8217;intercetta e alla pendenza di oggi con l&#8217;aggiunta di un rumore di sistema casuale. Questo gli conferisce la natura di una passeggiata casuale, il cui comportamento è descritto in dettaglio nell&#8217;articolo  sul <a href="https://datatrading.info/white-noise-e-random-walks-nellanalisi-delle-serie-temporali/">rumore bianco e le passeggiate casuali</a>:</p><p style="text-align: center;">\(\begin{eqnarray}\beta_{t+1} ={\bf I} \beta_{t} + w_t\end{eqnarray}\)</p><p>Dove la matrice di transizione è impostata sulla matrice di identificazione bidimensionale, \(G_t = {\bf I}\), che rappresenta la metà del modello dello spazio degli stati. Il passaggio successivo consiste nell&#8217;utilizzare effettivamente uno degli ETF della coppia come &#8220;osservazioni&#8221;.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-8a27784 elementor-widget elementor-widget-text-editor" data-id="8a27784" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h2>Applicazione del filtro di Kalman a una coppia di ETF</h2><p>Per formare l&#8217;equazione di osservazione è necessario scegliere una delle serie di prezzi dell&#8217;ETF come variabile &#8220;osservata&#8221;, \(y_t\), e la serie dell&#8217;altro ETF come formulazione di regressione lineare \(x_t\) descritta in precedenza:</p><p style="text-align: center;">\(\begin{eqnarray}y_t &amp;=&amp; F_t {\bf x}_t + v_t \\<br />&amp;=&amp; (\beta_0, \beta_1 ) \begin{pmatrix} 1 \\ x_t \end{pmatrix} + v_t\end{eqnarray}\)</p><p style="text-align: left;">Quindi abbiamo la regressione lineare riformulata come un modello dello spazio degli stati, che ci consente di stimare l&#8217;intercetta e la pendenza quando arrivano nuovi dati di prezzo tramite il filtro di Kalman.</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-1c743c1 elementor-widget elementor-widget-text-editor" data-id="1c743c1" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h3>TLT ed ETF</h3><p>Prenderemo in considerazione due ETF a reddito fisso, ovvero <a href="https://www.ishares.com/us/products/239454/ishares-20-year-treasury-bond-etf" target="_blank" rel="noopener">iShares 20+ Year Treasury Bond ETF (TLT)</a> e <a href="https://www.ishares.com/us/products/239455/ishares-37-year-treasury-bond-etf" target="_blank" rel="noopener">iShares 3-7 Year Treasury Bond ETF (IEI)</a> . Entrambi questi ETF replicano la performance di obbligazioni del Tesoro statunitensi di durata variabile e, in quanto tali, sono entrambi esposti a fattori di mercato simili. Analizziamo il loro comportamento di regressione negli ultimi cinque anni circa.</p><h4>Grafico a dispersione dei prezzi degli ETF</h4><p>Usiamo una varietà di librerie Python, tra cui numpy, matplotlib, pandas e pykalman per analizzare il comportamento di una regressione lineare dinamica tra questi due  titoli. Come per tutti i programmi Python, il primo compito è importare le librerie necessarie:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-18069eb elementor-widget elementor-widget-code-highlight" data-id="18069eb" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yfinance as yf
from pykalman import KalmanFilter</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-ef4a599 elementor-widget elementor-widget-text-editor" data-id="ef4a599" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><em>Nota: probabilmente dovrai eseguire <code>pip install pykalman</code> per installare la libreria PyKalman.</em></p><p>Dobbiamo ora scrivere la funzione <code>draw_date_coloured_scatterplot</code> per produrre un grafico a dispersione dei prezzi di chiusura rettificati dell&#8217;asset. Il grafico a dispersione verrà colorato utilizzando la mappa dei colori matplotlib, in particolare &#8220;Yellow To Red&#8221;, dove il giallo rappresenta le coppie di prezzi più vecchie, mentre il rosso rappresenta le coppie di prezzi più recenti:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-015fc2d elementor-widget elementor-widget-code-highlight" data-id="015fc2d" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>def draw_date_coloured_scatterplot(etfs, prices):
"""
    Creare un grafico scatterplot dei prezzi di due ETF, che è
    colorato dalle date dei prezzi per indicare il cambiamento
    della relazione tra le due serie di prezzi
    """
    # Creare a una mappa di colore da giallo a rosso dove il giallo
    # indica le date più vecchie e il rosso indica le date recenti
    # early dates and red indicates later dates
    plen = len(prices)
    colour_map = plt.cm.get_cmap('YlOrRd')
    colours = np.linspace(0.1, 1, plen)

    # Creare l'oggetto scatterplot
    scatterplot = plt.scatter(
        prices[etfs[0]], prices[etfs[1]],
        s=30, c=colours, cmap=colour_map,
        edgecolor='k', alpha=0.8
    )

    # Aggiungere una barra di colori per la colorazione dei dati ed
    # impostare le etichette dell'asse corrispondente
    colourbar = plt.colorbar(scatterplot)
    colourbar.ax.set_yticklabels(
        [str(p.date()) for p in prices[::plen // 9].index]
    )
    plt.xlabel(prices.columns[0])
    plt.ylabel(prices.columns[1])
    plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-7f2c668 elementor-widget elementor-widget-text-editor" data-id="7f2c668" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>Abbiamo commentato il codice, quindi dovrebbe essere abbastanza semplice  capire il significato di tutti i comandi. Il lavoro principale viene svolto all&#8217;interno delle variabili </span><code>colour_map</code><span>, </span><code>colours</code> <span>e </span><code>scatterplot</code>. Si ottiene il seguente grafico<span>:</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6b79cfc elementor-widget elementor-widget-image" data-id="6b79cfc" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="600" height="431" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-scatterplot-etf.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-kalman-scatterplot-etf" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-scatterplot-etf.png 600w, https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-scatterplot-etf-300x216.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-scatterplot-etf-160x115.png 160w" sizes="(max-width: 600px) 100vw, 600px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-06a5507 elementor-widget elementor-widget-text-editor" data-id="06a5507" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<h3><span>Pendenza e intercetta variabili nel tempo</span></h3><p><span>Il passaggio successivo consiste nell&#8217;utilizzare effettivamente pykalman per regolare dinamicamente l&#8217;intercetta e la pendenza tra TFT e IEI. Questa funzione è più complessa e richiede alcune spiegazioni.</span></p><p>Per prima cosa definiamo una variabile chiamata <code>delta</code>, che viene utilizzata per controllare la covarianza di transizione per il rumore del sistema. Nell&#8217;articolo sulla <a href="https://datatrading.info/modelli-dello-spazio-statale-e-filtro-di-kalman/">teoria del filtro Kalman</a> questo era indicato con \(W_t\). <span>Moltiplichiamo semplicemente tale valore per la matrice dell&#8217;identità bidimensionale.</span></p><p><span>Il passaggio successivo consiste nel creare la matrice di osservazione. Come abbiamo descritto in precedenza, questa matrice è un vettore composto dai prezzi di TFT e una sequenza di valori unitari. Per costruirlo utilizziamo il metodo</span> <code>vstack</code> di<span> numpy per impilare verticalmente queste due serie di prezzi in un vettore a colonna singola, che poi trasponiamo.</span></p><p><span>A questo punto utilizziamo la classe </span><code>KalmanFilter</code><span> di pykalman per creare l&#8217;istanza del filtro Kalman. Forniamo la dimensione delle osservazioni (unitaria in questo caso), la dimensione degli stati (due in questo caso poiché stiamo osservando l&#8217;intercetta e la pendenza della regressione lineare).</span></p><p><span>Dobbiamo anche fornire la media e la covarianza dello stato iniziale. In questo caso impostiamo la media dello stato iniziale a zero sia per l&#8217;intercetta che per la pendenza, mentre prendiamo la matrice di identità bidimensionale per la covarianza dello stato iniziale. Le matrici di transizione sono date anche dalla matrice identità bidimensionale.</span></p><p><span>Gli ultimi termini da specificare sono le matrici di osservazione in </span><code>obs_mat</code><span>, con la sua covarianza uguale all&#8217;unità. Infine la matrice di covarianza di transizione (controllata da </span><code>delta</code><span>) è data da </span><code>trans_cov</code>.</p><p><span>Ora che abbiamo l&#8217;istanza </span><code>kf</code><span> del filtro Kalman, possiamo utilizzarla per filtrare in base ai prezzi rettificati da IEI. Questo ci fornisce la media degli stati dell&#8217;intercetta e della pendenza, cioè quello che stiamo cercando. Inoltre ricaviamo anche le covarianze degli stati.</span></p><p><span>Tutto questo è raccolto nella  funzione </span><code>calc_slope_intercept_kalman</code><span>:</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-6638035 elementor-widget elementor-widget-code-highlight" data-id="6638035" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>
def calc_slope_intercept_kalman(etfs, prices):
    """
     Utilizzo del filtro Kalman dal pacchetto pyKalman
     per calcolare la pendenza e l'intercetta della 
     regressione lineare dei prezzi degli ETF.
     """
    delta = 1e-5
    trans_cov = delta / (1 - delta) * np.eye(2)
    obs_mat = np.vstack(
        [prices[etfs[0]], np.ones(prices[etfs[0]].shape)]
    ).T[:, np.newaxis]

    kf = KalmanFilter(
        n_dim_obs=1,
        n_dim_state=2,
        initial_state_mean=np.zeros(2),
        initial_state_covariance=np.ones((2, 2)),
        transition_matrices=np.eye(2),
        observation_matrices=obs_mat,
        observation_covariance=1.0,
        transition_covariance=trans_cov
    )

    state_means, state_covs = kf.filter(prices[etfs[1]].values)
    return state_means, state_covs</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-6cde9b6 elementor-widget elementor-widget-text-editor" data-id="6cde9b6" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p><span>Infine tracciamo il grafico dei valori restituiti dalla funzione precedente. </span><span>Per raggiungere questo obiettivo, creiamo un DataFrame pandas delle  pendenze e intercette per gli intervalli temporali <em>t</em>,  utilizzando l&#8217;indice del DataFrame <code>prices</code>, e tracciamo ogni colonna come un grafico:</span></p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-69a9bc4 elementor-widget elementor-widget-code-highlight" data-id="69a9bc4" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>
def draw_slope_intercept_changes(prices, state_means):
    """
    Tracciare la variazione di pendenza e intercetta  
    dai valori calcolati dal Filtro di Kalman.
    """
    pd.DataFrame(
        dict(
            slope=state_means[:, 0],
            intercept=state_means[:, 1]
        ), index=prices.index
    ).plot(subplots=True)
    plt.show()</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
				<div class="elementor-element elementor-element-603ea9c elementor-widget elementor-widget-text-editor" data-id="603ea9c" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Si ottiene il seguente grafico:</p>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-9dd6e87 elementor-widget elementor-widget-image" data-id="9dd6e87" data-element_type="widget" data-widget_type="image.default">
				<div class="elementor-widget-container">
								<div class="elementor-image">
												<img width="700" height="507" src="https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-dynamic-lin-reg.png" class="attachment-medium_large size-medium_large" alt="trading-quantitativo-kalman-dynamic-lin-reg" loading="lazy" srcset="https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-dynamic-lin-reg.png 700w, https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-dynamic-lin-reg-300x217.png 300w, https://datatrading.info/wp-content/uploads/trading-quantitativo-kalman-dynamic-lin-reg-160x116.png 160w" sizes="(max-width: 700px) 100vw, 700px" />														</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-2062ca4 elementor-widget elementor-widget-text-editor" data-id="2062ca4" data-element_type="widget" data-widget_type="text-editor.default">
				<div class="elementor-widget-container">
								<div class="elementor-text-editor elementor-clearfix">
				<p>Chiaramente la pendenza variabile nel tempo cambia drasticamente nel periodo, scendendo da circa 1,25 nel 2014 a circa 0,9 nel 2016. Non è difficile vedere che l&#8217;utilizzo di un hedge ratio fisso in una strategia di  pairs trading sarebbe troppo rigido.</p><p>Inoltre la stima della pendenza subisce l&#8217;effetto del rumore. La stima può essere controllata dalla  variabile <code>delta</code>, presente nel codice precedente, che ha anche l&#8217;effetto di ridurre la reattività del filtro alle variazioni del &#8220;vero&#8221; rapporto di copertura non osservato tra i due ETF.</p><p>Quando dobbiamo sviluppare una strategia di trading, è necessario ottimizzare questo parametro <code>delta</code> su panieri di coppie di ETF, utilizzando ancora una volta la convalida incrociata.</p><h2>Prossimi passi</h2><p>Ora che siamo stati in grado di costruire un rapporto di copertura dinamico tra i due ETF, abbiamo bisogno di un modo per realizzare effettivamente una strategia di trading basata su queste informazioni. A tale scopo si può utilizzare <a href="https://github.com/datatrading-info/DataTrader">DataTrader</a> per eseguire un backtest su varie coppie al fine di vedere come cambiano le prestazioni al variare dei parametri e degli intervalli temporali.</p><h2>Nota bibliografica</h2><p>L&#8217;utilizzo del filtro di Kalman per la &#8220;regressione lineare online&#8221; è stato eseguito da molti quant trader. Ernie Chan utilizza la tecnica nel suo libro [1] per stimare i coefficienti di regressione lineare dinamica tra i due ETF: EWA ed EWC.</p><p>Aidan O&#8217;Mahony ha utilizzato matplotlib e pykalman anche per stimare i coefficienti di regressione nel suo post [2].</p><p>Jonathan Kinlay discute l&#8217;applicazione del filtro di Kalman ai dati finanziari simulati [3] e suggerisce che potrebbe essere consigliabile utilizzare il KF per sopprimere i segnali  di trading generati in periodi di forte rumore, o aumentare le allocazioni alle coppie in cui il rumore è basso.</p><p>Una discussione introduttiva sul filtro Kalman, utilizzando il linguaggio di programmazione R, può essere trovata in Cowpertwait e Metcalfe [4].</p><h2>Riferimenti</h2><ul><li><a href="https://amzn.to/3NqBLHt" name="ref-chan">[1] Chan, E.P. (2013). <em>Algorithmic Trading: Winning Strategies and Their Rationale.</em></a></li><li><a href="http://www.thealgoengineer.com/2014/online_linear_regression_kalman_filter/" name="ref-omahony">[2] O&#8217;Mahony, A. (2014). <em>Online Linear Regression using a Kalman Filter</em></a></li><li><a href="http://jonathankinlay.com/2015/02/statistical-arbitrage-using-kalman-filter/" name="ref-kinlay">[3] Kinlay, J. (2015). <em>Statistical Arbitrage Using the Kalman Filter</em></a></li><li><a href="https://amzn.to/3wytUSC" name="ref-cowpertwait">[4] Cowpertwait, P.S.P. and Metcalfe, A.V. (2009). <em>Introductory Time Series with R.</em></a></li><li><a href="https://amzn.to/3wGqxrE" name="ref-pole">[5] Pole, A., West, M., and Harrison, J. (1994). <em>Applied Bayesian Forecasting.</em></a></li></ul><h2>Codice completo</h2>					</div>
						</div>
				</div>
				<div class="elementor-element elementor-element-526b1f2 elementor-widget elementor-widget-code-highlight" data-id="526b1f2" data-element_type="widget" data-widget_type="code-highlight.default">
				<div class="elementor-widget-container">
					<div class="prismjs-okaidia copy-to-clipboard word-wrap">
			<pre data-line="" class="highlight-height language-python ">
				<code readonly="true" class="language-python">
					<xmp>import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import yfinance as yf
from pykalman import KalmanFilter


def draw_date_coloured_scatterplot(etfs, prices):
    """
    Creare un grafico scatterplot dei prezzi di due ETF, che è
    colorato dalle date dei prezzi per indicare il cambiamento
    della relazione tra le due serie di prezzi
    """
    # Creare a una mappa di colore da giallo a rosso dove il giallo
    # indica le date più vecchie e il rosso indica le date recenti
    # early dates and red indicates later dates
    plen = len(prices)
    colour_map = plt.cm.get_cmap('YlOrRd')
    colours = np.linspace(0.1, 1, plen)

    # Creare l'oggetto scatterplot
    scatterplot = plt.scatter(
        prices[etfs[0]], prices[etfs[1]],
        s=30, c=colours, cmap=colour_map,
        edgecolor='k', alpha=0.8
    )

    # Aggiungere una barra di colori per la colorazione dei dati ed
    # impostare le etichette dell'asse corrispondente
    colourbar = plt.colorbar(scatterplot)
    colourbar.ax.set_yticklabels(
        [str(p.date()) for p in prices[::plen // 9].index]
    )
    plt.xlabel(prices.columns[0])
    plt.ylabel(prices.columns[1])
    plt.show()


def calc_slope_intercept_kalman(etfs, prices):
    """
     Utilizzo del filtro Kalman dal pacchetto pyKalman
     per calcolare la pendenza e l'intercetta della
     regressione lineare dei prezzi degli ETF.
     """
    delta = 1e-5
    trans_cov = delta / (1 - delta) * np.eye(2)
    obs_mat = np.vstack(
        [prices[etfs[0]], np.ones(prices[etfs[0]].shape)]
    ).T[:, np.newaxis]

    kf = KalmanFilter(
        n_dim_obs=1,
        n_dim_state=2,
        initial_state_mean=np.zeros(2),
        initial_state_covariance=np.ones((2, 2)),
        transition_matrices=np.eye(2),
        observation_matrices=obs_mat,
        observation_covariance=1.0,
        transition_covariance=trans_cov
    )

    state_means, state_covs = kf.filter(prices[etfs[1]].values)
    return state_means, state_covs

def draw_slope_intercept_changes(prices, state_means):
    """
    Tracciare la variazione di pendenza e intercetta
    dai valori calcolati dal Filtro di Kalman.
    """
    pd.DataFrame(
        dict(
            slope=state_means[:, 0],
            intercept=state_means[:, 1]
        ), index=prices.index
    ).plot(subplots=True)
    plt.show()

if __name__ == "__main__":
    # Scegliere i simboli ETF symbols e il periodo temporale 
    # dei prezzi storici
    etfs = ['TLT', 'IEI']
    start_date = "2012-10-01"
    end_date = "2017-10-01"

    # Download dei prezzi di chiusura da Yahoo finance
    prices = yf.download(etfs, start=start_date, end=end_date)['Adj Close']

    draw_date_coloured_scatterplot(etfs, prices)
    state_means, state_covs = calc_slope_intercept_kalman(etfs, prices)
    draw_slope_intercept_changes(prices, state_means)</xmp>
				</code>
			</pre>
		</div>
				</div>
				</div>
						</div>
					</div>
		</div>
								</div>
					</div>
		</section>
									</div>
			</div>
					</div>
		<p>L'articolo <a rel="nofollow" href="https://datatrading.info/hedge-ratio-dinamico-tra-coppie-di-etf-utilizzando-il-filtro-di-kalman/">Hedge Ratio Dinamico tra coppie di ETF utilizzando il Filtro di Kalman</a> proviene da <a rel="nofollow" href="https://datatrading.info">Data Trading</a>.</p>
]]></content:encoded>
					
		
		
			</item>
	</channel>
</rss>
